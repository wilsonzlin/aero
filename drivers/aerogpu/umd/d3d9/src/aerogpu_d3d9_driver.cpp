#include "../include/aerogpu_d3d9_umd.h"
#include "aerogpu_d3d9_wdk_abi_asserts.h"

#include <array>
#include <algorithm>
#include <bitset>
#include <chrono>
#include <cmath>
#include <cctype>
#include <cmath>
#include <cstddef>
#include <cstring>
#include <cstdlib>
#include <cwchar>
#include <initializer_list>
#include <limits>
#include <memory>
#include <mutex>
#include <new>
#include <thread>
#include <type_traits>
#include <unordered_map>
#include <utility>

#if defined(_WIN32)
  #include <d3d9types.h>
#endif

#ifndef D3DVS_VERSION
  #define D3DVS_VERSION(major, minor) (0xFFFE0000u | ((major) << 8) | (minor))
#endif

#ifndef D3DPS_VERSION
  #define D3DPS_VERSION(major, minor) (0xFFFF0000u | ((major) << 8) | (minor))
#endif

#include "aerogpu_d3d9_caps.h"
#include "aerogpu_d3d9_blit.h"
#include "aerogpu_d3d9_fixedfunc_shaders.h"
#include "aerogpu_d3d9_objects.h"
#include "aerogpu_d3d9_submit.h"
#include "aerogpu_d3d9_dma_priv.h"
#include "aerogpu_wddm_submit_buffer_utils.h"
#include "aerogpu_win7_abi.h"
#include "aerogpu_log.h"
#include "aerogpu_alloc.h"
#include "aerogpu_trace.h"
#include "aerogpu_wddm_alloc.h"
#include "../../../protocol/aerogpu_dbgctl_escape.h"
#include "../../common/aerogpu_win32_security.h"

namespace {

// Avoid exceptions escaping UMD code under memory pressure: prefer nothrow
// allocation and catch any unexpected constructor exceptions.
template <typename T, typename... Args>
std::unique_ptr<T> make_unique_nothrow(Args&&... args) {
  try {
    return std::unique_ptr<T>(new (std::nothrow) T(std::forward<Args>(args)...));
  } catch (...) {
    return nullptr;
  }
}

template <typename T, typename = void>
struct has_interface_version_member : std::false_type {};

template <typename T>
struct has_interface_version_member<T, std::void_t<decltype(std::declval<T>().InterfaceVersion)>> : std::true_type {};

template <typename T>
UINT get_interface_version(const T* open) {
  if (!open) {
    return 0;
  }
  if constexpr (has_interface_version_member<T>::value) {
    return open->InterfaceVersion;
  }
  return open->Interface;
}

template <typename T, typename = void>
struct has_adapter_callbacks2_member : std::false_type {};

template <typename T>
struct has_adapter_callbacks2_member<T, std::void_t<decltype(std::declval<T>().pAdapterCallbacks2)>> : std::true_type {};

template <typename T>
D3DDDI_ADAPTERCALLBACKS2* get_adapter_callbacks2(T* open) {
  if (!open) {
    return nullptr;
  }
  if constexpr (has_adapter_callbacks2_member<T>::value) {
    return open->pAdapterCallbacks2;
  }
  return nullptr;
}

template <typename T, typename = void>
struct has_vid_pn_source_id_member : std::false_type {};

template <typename T>
struct has_vid_pn_source_id_member<T, std::void_t<decltype(std::declval<T>().VidPnSourceId)>> : std::true_type {};

template <typename T>
void set_vid_pn_source_id(T* open, UINT vid_pn_source_id) {
  if (!open) {
    return;
  }
  if constexpr (has_vid_pn_source_id_member<T>::value) {
    open->VidPnSourceId = vid_pn_source_id;
  } else {
    (void)vid_pn_source_id;
  }
}

} // namespace

namespace aerogpu {

// D3D9 StateBlock (BeginStateBlock/EndStateBlock + Create/Capture/Apply).
//
// This is a minimal state capture model that records the subset of device state
// the current AeroGPU D3D9 UMD already understands/emits:
// - render states
// - sampler states
// - fixed-function / legacy state caches (cached; used for deterministic Get*
//   queries + state-block compatibility, and consulted by emulation paths where
//   implemented (fixed-function fallback shaders, instancing expansion, etc)):
//   - texture stage state (D3DTSS_*)
//   - transforms / clip planes / clip status
//   - stream source frequency / software vertex processing / N-patch mode
//   - shader int/bool constants
//   - lighting/material
//   - (WDK) palettes/current palette + gamma ramp
// - texture bindings
// - render target + depth/stencil bindings
// - viewport + scissor
// - VB/IB bindings
// - vertex decl / FVF hint
// - shader bindings + float/int/bool constants
//
// State blocks are runtime-managed objects; the runtime owns their lifetime and
// invokes DeleteStateBlock when released.
struct StateBlock {
  // Render state (D3DRS_*). Only the 0..255 range is cached by the UMD today.
  std::bitset<256> render_state_mask{};
  std::array<uint32_t, 256> render_state_values{};

  // Sampler state (D3DSAMP_*). Cached as [stage][state], with both ranges 0..15.
  std::bitset<16 * 16> sampler_state_mask{}; // stage * 16 + state
  std::array<uint32_t, 16 * 16> sampler_state_values{};

  // Texture stage state (D3DTSS_*). Fixed-function; most of it is cached-only
  // (Get* queries + state blocks), but stages 0..3 are consulted by the UMD's
  // fixed-function fallback path to select/synthesize a `ps_2_0` variant.
  std::bitset<16 * 256> texture_stage_state_mask{}; // stage * 256 + state
  std::array<uint32_t, 16 * 256> texture_stage_state_values{};

  // Transform matrices (D3DTRANSFORMSTATETYPE numeric values).
  std::bitset<Device::kTransformCacheCount> transform_mask{};
  std::array<float, Device::kTransformCacheCount * 16> transform_values{};

  // User clip planes (0..5). Fixed-function and not currently consumed by the
  // AeroGPU command stream, but cached for Get*/state-block compatibility.
  std::bitset<6> clip_plane_mask{};
  std::array<float, 6 * 4> clip_plane_values{};

  // Misc fixed-function cached state.
  bool software_vertex_processing_set = false;
  BOOL software_vertex_processing = FALSE;
  bool n_patch_mode_set = false;
  float n_patch_mode = 0.0f;

  // Fixed-function lighting/material state. Cached-only and not currently
  // consumed by the AeroGPU command stream directly, but tracked for Get*/state-block
  // compatibility with legacy apps and consulted by the minimal fixed-function
  // fallback path for a small lighting subset (see README).
  bool material_set = false;
  bool material_valid = false;
  D3DMATERIAL9 material{};

  std::bitset<Device::kMaxLights> light_mask{};
  std::array<D3DLIGHT9, Device::kMaxLights> lights{};
  std::bitset<Device::kMaxLights> light_valid_bits{};
  std::bitset<Device::kMaxLights> light_enable_mask{};
  std::bitset<Device::kMaxLights> light_enable_bits{};

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  // Misc legacy state (cached only, not currently emitted to the AeroGPU command
  // stream), but must be round-trippable via Get* and state blocks.
  bool gamma_ramp_set = false;
  bool gamma_ramp_valid = false;
  D3DGAMMARAMP gamma_ramp{};

  bool clip_status_set = false;
  bool clip_status_valid = false;
  D3DCLIPSTATUS9 clip_status{};

  std::bitset<Device::kMaxPalettes> palette_mask{};
  std::array<std::array<PALETTEENTRY, 256>, Device::kMaxPalettes> palette_entries{};
  std::bitset<Device::kMaxPalettes> palette_valid_bits{};

  bool current_texture_palette_set = false;
  uint32_t current_texture_palette = 0;
#endif

  // Texture bindings (pixel shader stages only; 0..15).
  std::bitset<16> texture_mask{};
  std::array<Resource*, 16> textures{};

  // Render target bindings (0..3) + depth/stencil.
  std::bitset<4> render_target_mask{};
  std::array<Resource*, 4> render_targets{};
  bool depth_stencil_set = false;
  Resource* depth_stencil = nullptr;

  // Viewport + scissor.
  bool viewport_set = false;
  D3DDDIVIEWPORTINFO viewport = {0, 0, 0, 0, 0.0f, 1.0f};
  bool scissor_set = false;
  RECT scissor_rect = {0, 0, 0, 0};
  // Mirrors `Device::scissor_rect_user_set`. This is not a D3D9-visible bit, but
  // it influences the UMD's "default scissor rect" fixup behavior when scissor
  // is enabled before any SetScissorRect call. Treat it as part of scissor
  // state so state blocks can round-trip the pre-SetScissorRect condition.
  bool scissor_rect_user_set = false;
  BOOL scissor_enabled = FALSE;

  // VB/IB bindings.
  std::bitset<16> stream_mask{};
  std::array<DeviceStateStream, 16> streams{};
  std::bitset<16> stream_source_freq_mask{};
  std::array<uint32_t, 16> stream_source_freq_values{};
  bool index_buffer_set = false;
  Resource* index_buffer = nullptr;
  D3DDDIFORMAT index_format = static_cast<D3DDDIFORMAT>(101); // D3DFMT_INDEX16
  uint32_t index_offset_bytes = 0;

  // Input layout state.
  bool vertex_decl_set = false;
  VertexDecl* vertex_decl = nullptr;
  bool fvf_set = false;
  uint32_t fvf = 0;

  // Shader bindings (D3D9 stages: VS/PS) + constant registers.
  bool user_vs_set = false;
  Shader* user_vs = nullptr;
  bool user_ps_set = false;
  Shader* user_ps = nullptr;

  // Float4 constant registers.
  std::bitset<256> vs_const_mask{};
  std::array<float, 256 * 4> vs_consts{};
  std::bitset<256> ps_const_mask{};
  std::array<float, 256 * 4> ps_consts{};

  // Int4 constant registers.
  std::bitset<256> vs_const_i_mask{};
  std::array<int32_t, 256 * 4> vs_consts_i{};
  std::bitset<256> ps_const_i_mask{};
  std::array<int32_t, 256 * 4> ps_consts_i{};

  // Bool constant registers.
  std::bitset<256> vs_const_b_mask{};
  std::array<uint8_t, 256> vs_consts_b{};
  std::bitset<256> ps_const_b_mask{};
  std::array<uint8_t, 256> ps_consts_b{};
};

namespace {

#define AEROGPU_D3D9_STUB_LOG_ONCE()                 \
  do {                                               \
    static std::once_flag aerogpu_once;              \
    const char* fn = __func__;                       \
    std::call_once(aerogpu_once, [fn] {              \
      aerogpu::logf("aerogpu-d3d9: stub %s\n", fn);  \
    });                                              \
  } while (0)

template <typename FuncTable>
const char* d3d9_vtable_member_name(size_t index);

template <typename FuncTable>
bool d3d9_validate_nonnull_vtable(const FuncTable* table, const char* table_name) {
  if (!table || !table_name) {
    return false;
  }

  static_assert(sizeof(FuncTable) % sizeof(void*) == 0, "D3D9 DDI function tables must be pointer arrays");
  const uint8_t* bytes = reinterpret_cast<const uint8_t*>(table);
  constexpr size_t kPtrBytes = sizeof(void*);
  const std::array<uint8_t, kPtrBytes> zero{};
  const size_t count = sizeof(FuncTable) / kPtrBytes;

  for (size_t i = 0; i < count; ++i) {
    const uint8_t* slot = bytes + i * kPtrBytes;
    if (std::memcmp(slot, zero.data(), kPtrBytes) == 0) {
      const char* member_name = d3d9_vtable_member_name<FuncTable>(i);
      if (member_name) {
        aerogpu::logf("aerogpu-d3d9: %s missing entry index=%llu (bytes=%llu) member=%s\n",
                      table_name,
                      static_cast<unsigned long long>(i),
                      static_cast<unsigned long long>(i * sizeof(void*)),
                      member_name);
      } else {
        aerogpu::logf("aerogpu-d3d9: %s missing entry index=%llu (bytes=%llu)\n",
                      table_name,
                      static_cast<unsigned long long>(i),
                      static_cast<unsigned long long>(i * sizeof(void*)));
      }
      return false;
    }
  }
  return true;
}

constexpr int32_t kMinGpuThreadPriority = -7;
constexpr int32_t kMaxGpuThreadPriority = 7;

// D3DERR_INVALIDCALL (0x8876086C) is returned by the UMD for invalid arguments.
constexpr HRESULT kD3DErrInvalidCall = static_cast<HRESULT>(0x8876086CL);

// D3DERR_DEVICELOST (0x88760868) is returned by the UMD/DDI when the device is
// considered lost/hung (e.g. a WDDM submission callback failed). Keep a local
// definition so the file doesn't depend on d3d9.h.
constexpr HRESULT kD3dErrDeviceLost = static_cast<HRESULT>(0x88760868L);

// In WDK builds, the D3D9 DDI boundary can also surface device removal/hang
// HRESULTs. If the runtime submission callback reports one of these, preserve it
// so callers see a stable and semantically-correct code.
#if defined(D3DDDIERR_DEVICEHUNG)
constexpr HRESULT kD3dDdiErrDeviceHung = D3DDDIERR_DEVICEHUNG;
#endif
#if defined(D3DDDIERR_DEVICEREMOVED)
constexpr HRESULT kD3dDdiErrDeviceRemoved = D3DDDIERR_DEVICEREMOVED;
#endif
#if defined(D3DERR_DEVICEREMOVED)
constexpr HRESULT kD3dErrDeviceRemoved = D3DERR_DEVICEREMOVED;
#endif

// S_PRESENT_OCCLUDED (0x08760868) is returned by CheckDeviceState/PresentEx when
// the target window is occluded/minimized. Prefer the SDK macro when available
// but provide a fallback so repo builds don't need d3d9.h.
#if defined(S_PRESENT_OCCLUDED)
constexpr HRESULT kSPresentOccluded = S_PRESENT_OCCLUDED;
#else
constexpr HRESULT kSPresentOccluded = 0x08760868L;
#endif

// Best-effort occlusion detection shared by Present/PresentEx/CheckDeviceState.
//
// This must stay cheap/non-blocking: only query cached window state (no waits).
// Note that some D3D9Ex clients (including DWM) can present to HWNDs that are not
// straightforward "app windows", so avoid aggressive heuristics that would cause
// false S_PRESENT_OCCLUDED returns.
bool hwnd_occlude_invisible_enabled() {
  static std::once_flag once;
  static bool enabled = false;
  std::call_once(once, [] {
#if defined(_WIN32)
    char buf[32] = {};
    const DWORD n = GetEnvironmentVariableA("AEROGPU_D3D9_OCCLUDE_INVISIBLE", buf, static_cast<DWORD>(sizeof(buf)));
    if (n == 0 || n >= sizeof(buf)) {
      enabled = false;
      return;
    }
    for (char& c : buf) {
      c = static_cast<char>(std::tolower(static_cast<unsigned char>(c)));
    }
    enabled = (std::strcmp(buf, "1") == 0 || std::strcmp(buf, "true") == 0 || std::strcmp(buf, "yes") == 0 ||
               std::strcmp(buf, "on") == 0);
#else
    // Portable builds should never enable HWND-based occlusion heuristics: they
    // do not have real Win32 windows, and host-side tests exercise PresentEx
    // semantics using nullptr HWNDs.
    enabled = false;
#endif
  });
  return enabled;
}

bool hwnd_is_occluded(HWND hwnd) {
#if defined(_WIN32)
  if (!hwnd) {
    return false;
  }
  // If the HWND is stale/invalid, don't guess: treat it as "not occluded" to
  // avoid spurious S_PRESENT_OCCLUDED.
  if (!IsWindow(hwnd)) {
    return false;
  }
  // If clients pass a child HWND, the real present target is the root/top-level
  // window; use that for "minimized" state checks.
  HWND root = GetAncestor(hwnd, GA_ROOT);
  HWND top = root ? root : hwnd;

  if (IsIconic(top)) {
    return true;
  }
  // Optional: treat hidden/invisible windows as occluded. Disabled by default
  // because some D3D9Ex tests intentionally present to hidden windows (they
  // still require PresentEx to succeed and exercise pacing/throttling).
  if (hwnd_occlude_invisible_enabled() && !IsWindowVisible(top)) {
    return true;
  }
  RECT rc{};
  if (GetClientRect(hwnd, &rc)) {
    const LONG w = rc.right - rc.left;
    const LONG h = rc.bottom - rc.top;
    if (w <= 0 || h <= 0) {
      return true;
    }
  }
  if (top != hwnd && GetClientRect(top, &rc)) {
    const LONG w = rc.right - rc.left;
    const LONG h = rc.bottom - rc.top;
    if (w <= 0 || h <= 0) {
      return true;
    }
  }
  return false;
#else
  (void)hwnd;
  return false;
#endif
}

// D3D9 API/UMD query constants (numeric values from d3d9types.h).
constexpr uint32_t kD3DQueryTypeEvent = 8u;
constexpr uint32_t kD3DIssueEnd = 0x1u;
// Some D3D9 runtimes/WDK header vintages appear to use 0x2 to signal END at the
// DDI boundary (even though the public IDirect3DQuery9::Issue API uses 0x2 for
// BEGIN). Be permissive and accept both encodings for EVENT queries.
constexpr uint32_t kD3DIssueEndAlt = 0x2u;
constexpr uint32_t kD3DGetDataFlush = 0x1u;

// D3DRESOURCETYPE / D3DDDIRESTYPE values (numeric values from d3d9types.h).
//
// AeroGPU only supports:
// - buffers (Size!=0, mapped to CREATE_BUFFER)
// - 2D images (Size==0, mapped to CREATE_TEXTURE2D)
//
// For Size==0 resources, we validate Type explicitly so we don't accidentally
// treat volume resources as 2D textures. Cube textures are supported by
// representing them as 2D texture arrays with Depth==6.
constexpr uint32_t kD3dRTypeSurface = 1u; // D3DRTYPE_SURFACE
constexpr uint32_t kD3dRTypeVolume = 2u; // D3DRTYPE_VOLUME
constexpr uint32_t kD3dRTypeTexture = 3u; // D3DRTYPE_TEXTURE
constexpr uint32_t kD3dRTypeVolumeTexture = 4u; // D3DRTYPE_VOLUMETEXTURE
constexpr uint32_t kD3dRTypeCubeTexture = 5u; // D3DRTYPE_CUBETEXTURE

enum class D3d9NonBufferTypeValidation : uint32_t {
  RejectUnknown = 0,
  // OpenResource DDI structs vary across WDK header vintages; some do not
  // expose a Type field. In that case `type` will be 0, and we treat it as a
  // 2D surface description (Depth==1) for compatibility.
  AllowUnknownAsSurface = 1,
};

bool d3d9_validate_nonbuffer_type(uint32_t type, uint32_t depth, D3d9NonBufferTypeValidation policy) {
  switch (type) {
    case kD3dRTypeTexture:
      // For 2D textures, Depth is interpreted as array layers (Depth>=1).
      return depth >= 1;
    case kD3dRTypeCubeTexture:
      // Cube textures are represented as `CREATE_TEXTURE2D` with `array_layers=6`.
      return depth == 6;
    case kD3dRTypeSurface:
      // Surfaces cannot be arrays.
      return depth == 1;
    case kD3dRTypeVolume:
    case kD3dRTypeVolumeTexture:
      return false;
    default:
      if (type == 0 && policy == D3d9NonBufferTypeValidation::AllowUnknownAsSurface) {
        return depth == 1;
      }
      return false;
  }
}

uint32_t f32_bits(float v) {
  uint32_t bits = 0;
  static_assert(sizeof(bits) == sizeof(v), "float must be 32-bit");
  std::memcpy(&bits, &v, sizeof(bits));
  return bits;
}

// D3DPRESENT_* flags (numeric values from d3d9.h). We only need DONOTWAIT for
// max-frame-latency throttling.
constexpr uint32_t kD3dPresentDoNotWait = 0x00000001u; // D3DPRESENT_DONOTWAIT
constexpr uint32_t kD3dPresentIntervalImmediate = 0x80000000u; // D3DPRESENT_INTERVAL_IMMEDIATE

// D3DERR_WASSTILLDRAWING (0x8876021C). Returned by PresentEx when DONOTWAIT is
// specified and the present is throttled.
constexpr HRESULT kD3dErrWasStillDrawing = static_cast<HRESULT>(-2005532132);

constexpr uint32_t kMaxFrameLatencyMin = 1;
constexpr uint32_t kMaxFrameLatencyMax = 16;

// Bounded wait for PresentEx throttling. This must be finite to avoid hangs in
// DWM/PresentEx call sites if the GPU stops making forward progress.
constexpr uint32_t kPresentThrottleMaxWaitMs = 100;

enum class WddmSubmitCallbackKind : uint32_t {
  None = 0,
  SubmitCommandCb = 1,
  RenderCb = 2,
  PresentCb = 3,
};

const char* wddm_submit_callback_kind_name(WddmSubmitCallbackKind kind) {
  switch (kind) {
    case WddmSubmitCallbackKind::SubmitCommandCb:
      return "SubmitCommandCb";
    case WddmSubmitCallbackKind::RenderCb:
      return "RenderCb";
    case WddmSubmitCallbackKind::PresentCb:
      return "PresentCb";
    default:
      return "None";
  }
}

inline bool device_is_lost(const Device* dev) {
  return dev && dev->device_lost.load(std::memory_order_acquire);
}

inline HRESULT device_lost_hresult(const Device* dev) {
  if (!dev) {
    return kD3dErrDeviceLost;
  }
  const HRESULT lost_hr = static_cast<HRESULT>(dev->device_lost_hr.load(std::memory_order_acquire));
  // If the submission callback already returned a DDI/device-removed style code,
  // preserve it so callers don't lose the more specific failure reason.
#if defined(D3DDDIERR_DEVICEREMOVED)
  if (lost_hr == kD3dDdiErrDeviceRemoved) {
    return lost_hr;
  }
#endif
#if defined(D3DDDIERR_DEVICEHUNG)
  if (lost_hr == kD3dDdiErrDeviceHung) {
    return lost_hr;
  }
#endif
#if defined(D3DERR_DEVICEREMOVED)
  if (lost_hr == kD3dErrDeviceRemoved) {
    return lost_hr;
  }
#endif
  // Default: D3D9-level device-lost error.
  return kD3dErrDeviceLost;
}

inline HRESULT device_lost_override(Device* dev, HRESULT hr) {
  return device_is_lost(dev) ? device_lost_hresult(dev) : hr;
}

void mark_device_lost_from_submit(Device* dev, HRESULT hr, bool is_present, WddmSubmitCallbackKind cb_kind) {
  if (!dev) {
    return;
  }

  bool expected = false;
  if (!dev->device_lost.compare_exchange_strong(expected, true, std::memory_order_acq_rel)) {
    return;
  }

  dev->device_lost_hr.store(static_cast<int32_t>(hr), std::memory_order_release);
  dev->device_lost_reason.store(
      static_cast<uint32_t>(is_present ? DeviceLostReason::WddmSubmitPresent : DeviceLostReason::WddmSubmitRender),
      std::memory_order_release);

  const unsigned hr_code = static_cast<unsigned>(hr);
  const char* kind_name = wddm_submit_callback_kind_name(cb_kind);
  const char* submit_name = is_present ? "present" : "render";
  if (!dev->device_lost_logged.exchange(true, std::memory_order_acq_rel)) {
    aerogpu::logf("aerogpu-d3d9: device lost: %s submit via %s failed hr=0x%08x\n",
                  submit_name,
                  kind_name,
                  hr_code);
  }
}

// Some WDDM/D3D9 callback structs may not expose `SubmissionFenceId`/`NewFenceValue`
// depending on the WDK header vintage. When the runtime does not provide a
// per-submission fence value via the callback out-params, we fall back to
// querying the AeroGPU KMD fence counters via D3DKMTEscape so we still return a
// real fence value for the submission.

std::once_flag g_submit_log_once;
bool g_submit_log_enabled = false;
#if defined(_WIN32)
std::once_flag g_dma_priv_invalid_once;
std::once_flag g_dma_priv_size_mismatch_once;
#endif

bool submit_log_enabled() {
  std::call_once(g_submit_log_once, [] {
#if defined(_WIN32)
    char buf[32] = {};
    const DWORD n = GetEnvironmentVariableA("AEROGPU_D3D9_LOG_SUBMITS", buf, static_cast<DWORD>(sizeof(buf)));
    if (n == 0 || n >= sizeof(buf)) {
      g_submit_log_enabled = false;
      return;
    }
    for (char& c : buf) {
      c = static_cast<char>(std::tolower(static_cast<unsigned char>(c)));
    }
    g_submit_log_enabled = (std::strcmp(buf, "1") == 0 || std::strcmp(buf, "true") == 0 || std::strcmp(buf, "yes") == 0 ||
                            std::strcmp(buf, "on") == 0);
#else
    const char* v = std::getenv("AEROGPU_D3D9_LOG_SUBMITS");
    if (!v || !*v) {
      g_submit_log_enabled = false;
      return;
    }
    char buf[32] = {};
    std::strncpy(buf, v, sizeof(buf) - 1);
    buf[sizeof(buf) - 1] = 0;
    for (char& c : buf) {
      c = static_cast<char>(std::tolower(static_cast<unsigned char>(c)));
    }
    g_submit_log_enabled = (std::strcmp(buf, "1") == 0 || std::strcmp(buf, "true") == 0 || std::strcmp(buf, "yes") == 0 ||
                            std::strcmp(buf, "on") == 0);
#endif
  });
  return g_submit_log_enabled;
}

// Some D3D9 UMD DDI members vary across WDK header vintages. Use compile-time
// detection (SFINAE) so the UMD can populate as many entrypoints as possible
// without hard-failing compilation when a member is absent.
//
// This mirrors the approach in `tools/wdk_abi_probe/`.
#define AEROGPU_DEFINE_HAS_MEMBER(member)                                                      \
  template <typename T, typename = void>                                                       \
  struct aerogpu_has_member_##member : std::false_type {};                                      \
  template <typename T>                                                                        \
  struct aerogpu_has_member_##member<T, std::void_t<decltype(&T::member)>> : std::true_type {}

AEROGPU_DEFINE_HAS_MEMBER(pfnOpenResource);
AEROGPU_DEFINE_HAS_MEMBER(pfnOpenResource2);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetFVF);
AEROGPU_DEFINE_HAS_MEMBER(pfnBeginScene);
AEROGPU_DEFINE_HAS_MEMBER(pfnEndScene);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawPrimitive2);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawIndexedPrimitive2);
AEROGPU_DEFINE_HAS_MEMBER(pfnWaitForVBlank);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetGPUThreadPriority);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetGPUThreadPriority);
AEROGPU_DEFINE_HAS_MEMBER(pfnCheckResourceResidency);
AEROGPU_DEFINE_HAS_MEMBER(pfnQueryResourceResidency);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetPriority);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetPriority);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetDisplayModeEx);
AEROGPU_DEFINE_HAS_MEMBER(pfnComposeRects);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetConvolutionMonoKernel);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetAutoGenFilterType);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetAutoGenFilterType);
AEROGPU_DEFINE_HAS_MEMBER(pfnGenerateMipSubLevels);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawPrimitiveUP);

// Fixed function / legacy state paths (commonly hit by DWM + simple D3D9 apps).
AEROGPU_DEFINE_HAS_MEMBER(pfnSetTextureStageState);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetTransform);
AEROGPU_DEFINE_HAS_MEMBER(pfnMultiplyTransform);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetClipPlane);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetShaderConstI);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetShaderConstB);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetMaterial);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetLight);
AEROGPU_DEFINE_HAS_MEMBER(pfnLightEnable);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetNPatchMode);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetStreamSourceFreq);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetGammaRamp);
AEROGPU_DEFINE_HAS_MEMBER(pfnCreateStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnDeleteStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnCaptureStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnApplyStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnValidateDevice);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetSoftwareVertexProcessing);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetCursorProperties);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetCursorPosition);
AEROGPU_DEFINE_HAS_MEMBER(pfnShowCursor);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetPaletteEntries);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetCurrentTexturePalette);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetClipStatus);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetClipStatus);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetGammaRamp);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawRectPatch);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawTriPatch);
AEROGPU_DEFINE_HAS_MEMBER(pfnDeletePatch);
AEROGPU_DEFINE_HAS_MEMBER(pfnProcessVertices);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetRasterStatus);
AEROGPU_DEFINE_HAS_MEMBER(pfnSetDialogBoxMode);
AEROGPU_DEFINE_HAS_MEMBER(pfnDrawIndexedPrimitiveUP);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetSoftwareVertexProcessing);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetTransform);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetClipPlane);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetViewport);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetScissorRect);
AEROGPU_DEFINE_HAS_MEMBER(pfnBeginStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnEndStateBlock);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetMaterial);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetLight);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetLightEnable);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetRenderTarget);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetDepthStencil);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetTexture);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetTextureStageState);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetSamplerState);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetRenderState);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetPaletteEntries);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetCurrentTexturePalette);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetNPatchMode);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetFVF);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetVertexDecl);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetStreamSource);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetStreamSourceFreq);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetIndices);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetShader);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetShaderConstF);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetShaderConstI);
AEROGPU_DEFINE_HAS_MEMBER(pfnGetShaderConstB);

// OpenResource arg fields (vary across WDK versions).
AEROGPU_DEFINE_HAS_MEMBER(hAllocation);
AEROGPU_DEFINE_HAS_MEMBER(hAllocations);
AEROGPU_DEFINE_HAS_MEMBER(phAllocation);
AEROGPU_DEFINE_HAS_MEMBER(pOpenAllocationInfo);
AEROGPU_DEFINE_HAS_MEMBER(NumAllocations);

#undef AEROGPU_DEFINE_HAS_MEMBER

template <typename FuncTable>
const char* d3d9_vtable_member_name(size_t index) {
  constexpr size_t kPtrBytes = sizeof(void*);
  if constexpr (std::is_same_v<FuncTable, D3D9DDI_ADAPTERFUNCS>) {
    if (index == offsetof(FuncTable, pfnCloseAdapter) / kPtrBytes) {
      return "pfnCloseAdapter";
    }
    if (index == offsetof(FuncTable, pfnGetCaps) / kPtrBytes) {
      return "pfnGetCaps";
    }
    if (index == offsetof(FuncTable, pfnCreateDevice) / kPtrBytes) {
      return "pfnCreateDevice";
    }
    if (index == offsetof(FuncTable, pfnQueryAdapterInfo) / kPtrBytes) {
      return "pfnQueryAdapterInfo";
    }
    return nullptr;
  }

  if constexpr (std::is_same_v<FuncTable, D3D9DDI_DEVICEFUNCS>) {
    if (index == offsetof(FuncTable, pfnDestroyDevice) / kPtrBytes) {
      return "pfnDestroyDevice";
    }
    if (index == offsetof(FuncTable, pfnCreateResource) / kPtrBytes) {
      return "pfnCreateResource";
    }
    if constexpr (aerogpu_has_member_pfnOpenResource<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnOpenResource) / kPtrBytes) {
        return "pfnOpenResource";
      }
    }
    if constexpr (aerogpu_has_member_pfnOpenResource2<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnOpenResource2) / kPtrBytes) {
        return "pfnOpenResource2";
      }
    }
    if (index == offsetof(FuncTable, pfnDestroyResource) / kPtrBytes) {
      return "pfnDestroyResource";
    }
    if (index == offsetof(FuncTable, pfnLock) / kPtrBytes) {
      return "pfnLock";
    }
    if (index == offsetof(FuncTable, pfnUnlock) / kPtrBytes) {
      return "pfnUnlock";
    }
    if (index == offsetof(FuncTable, pfnSetRenderTarget) / kPtrBytes) {
      return "pfnSetRenderTarget";
    }
    if (index == offsetof(FuncTable, pfnSetDepthStencil) / kPtrBytes) {
      return "pfnSetDepthStencil";
    }
    if (index == offsetof(FuncTable, pfnSetViewport) / kPtrBytes) {
      return "pfnSetViewport";
    }
    if (index == offsetof(FuncTable, pfnSetScissorRect) / kPtrBytes) {
      return "pfnSetScissorRect";
    }
    if (index == offsetof(FuncTable, pfnSetTexture) / kPtrBytes) {
      return "pfnSetTexture";
    }
    if constexpr (aerogpu_has_member_pfnSetTextureStageState<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetTextureStageState) / kPtrBytes) {
        return "pfnSetTextureStageState";
      }
    }
    if (index == offsetof(FuncTable, pfnSetSamplerState) / kPtrBytes) {
      return "pfnSetSamplerState";
    }
    if (index == offsetof(FuncTable, pfnSetRenderState) / kPtrBytes) {
      return "pfnSetRenderState";
    }
    if constexpr (aerogpu_has_member_pfnSetMaterial<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetMaterial) / kPtrBytes) {
        return "pfnSetMaterial";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetLight<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetLight) / kPtrBytes) {
        return "pfnSetLight";
      }
    }
    if constexpr (aerogpu_has_member_pfnLightEnable<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnLightEnable) / kPtrBytes) {
        return "pfnLightEnable";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetNPatchMode<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetNPatchMode) / kPtrBytes) {
        return "pfnSetNPatchMode";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetGammaRamp<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetGammaRamp) / kPtrBytes) {
        return "pfnSetGammaRamp";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetTransform<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetTransform) / kPtrBytes) {
        return "pfnSetTransform";
      }
    }
    if constexpr (aerogpu_has_member_pfnMultiplyTransform<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnMultiplyTransform) / kPtrBytes) {
        return "pfnMultiplyTransform";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetClipPlane<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetClipPlane) / kPtrBytes) {
        return "pfnSetClipPlane";
      }
    }
    if (index == offsetof(FuncTable, pfnCreateVertexDecl) / kPtrBytes) {
      return "pfnCreateVertexDecl";
    }
    if (index == offsetof(FuncTable, pfnSetVertexDecl) / kPtrBytes) {
      return "pfnSetVertexDecl";
    }
    if (index == offsetof(FuncTable, pfnDestroyVertexDecl) / kPtrBytes) {
      return "pfnDestroyVertexDecl";
    }
    if constexpr (aerogpu_has_member_pfnSetFVF<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetFVF) / kPtrBytes) {
        return "pfnSetFVF";
      }
    }
    if (index == offsetof(FuncTable, pfnCreateShader) / kPtrBytes) {
      return "pfnCreateShader";
    }
    if (index == offsetof(FuncTable, pfnSetShader) / kPtrBytes) {
      return "pfnSetShader";
    }
    if (index == offsetof(FuncTable, pfnDestroyShader) / kPtrBytes) {
      return "pfnDestroyShader";
    }
    if (index == offsetof(FuncTable, pfnSetShaderConstF) / kPtrBytes) {
      return "pfnSetShaderConstF";
    }
    if constexpr (aerogpu_has_member_pfnSetShaderConstI<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetShaderConstI) / kPtrBytes) {
        return "pfnSetShaderConstI";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetShaderConstB<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetShaderConstB) / kPtrBytes) {
        return "pfnSetShaderConstB";
      }
    }
    if constexpr (aerogpu_has_member_pfnCreateStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnCreateStateBlock) / kPtrBytes) {
        return "pfnCreateStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnDeleteStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDeleteStateBlock) / kPtrBytes) {
        return "pfnDeleteStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnCaptureStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnCaptureStateBlock) / kPtrBytes) {
        return "pfnCaptureStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnApplyStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnApplyStateBlock) / kPtrBytes) {
        return "pfnApplyStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnValidateDevice<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnValidateDevice) / kPtrBytes) {
        return "pfnValidateDevice";
      }
    }
    if (index == offsetof(FuncTable, pfnSetStreamSource) / kPtrBytes) {
      return "pfnSetStreamSource";
    }
    if constexpr (aerogpu_has_member_pfnSetStreamSourceFreq<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetStreamSourceFreq) / kPtrBytes) {
        return "pfnSetStreamSourceFreq";
      }
    }
    if (index == offsetof(FuncTable, pfnSetIndices) / kPtrBytes) {
      return "pfnSetIndices";
    }
    if constexpr (aerogpu_has_member_pfnSetSoftwareVertexProcessing<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetSoftwareVertexProcessing) / kPtrBytes) {
        return "pfnSetSoftwareVertexProcessing";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetCursorProperties<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetCursorProperties) / kPtrBytes) {
        return "pfnSetCursorProperties";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetCursorPosition<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetCursorPosition) / kPtrBytes) {
        return "pfnSetCursorPosition";
      }
    }
    if constexpr (aerogpu_has_member_pfnShowCursor<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnShowCursor) / kPtrBytes) {
        return "pfnShowCursor";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetPaletteEntries<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetPaletteEntries) / kPtrBytes) {
        return "pfnSetPaletteEntries";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetCurrentTexturePalette<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetCurrentTexturePalette) / kPtrBytes) {
        return "pfnSetCurrentTexturePalette";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetClipStatus<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetClipStatus) / kPtrBytes) {
        return "pfnSetClipStatus";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetClipStatus<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetClipStatus) / kPtrBytes) {
        return "pfnGetClipStatus";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetGammaRamp<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetGammaRamp) / kPtrBytes) {
        return "pfnGetGammaRamp";
      }
    }
    if constexpr (aerogpu_has_member_pfnBeginScene<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnBeginScene) / kPtrBytes) {
        return "pfnBeginScene";
      }
    }
    if constexpr (aerogpu_has_member_pfnEndScene<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnEndScene) / kPtrBytes) {
        return "pfnEndScene";
      }
    }
    if (index == offsetof(FuncTable, pfnClear) / kPtrBytes) {
      return "pfnClear";
    }
    if (index == offsetof(FuncTable, pfnDrawPrimitive) / kPtrBytes) {
      return "pfnDrawPrimitive";
    }
    if constexpr (aerogpu_has_member_pfnDrawPrimitiveUP<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawPrimitiveUP) / kPtrBytes) {
        return "pfnDrawPrimitiveUP";
      }
    }
    if constexpr (aerogpu_has_member_pfnDrawIndexedPrimitiveUP<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawIndexedPrimitiveUP) / kPtrBytes) {
        return "pfnDrawIndexedPrimitiveUP";
      }
    }
    if (index == offsetof(FuncTable, pfnDrawIndexedPrimitive) / kPtrBytes) {
      return "pfnDrawIndexedPrimitive";
    }
    if constexpr (aerogpu_has_member_pfnDrawRectPatch<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawRectPatch) / kPtrBytes) {
        return "pfnDrawRectPatch";
      }
    }
    if constexpr (aerogpu_has_member_pfnDrawTriPatch<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawTriPatch) / kPtrBytes) {
        return "pfnDrawTriPatch";
      }
    }
    if constexpr (aerogpu_has_member_pfnDeletePatch<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDeletePatch) / kPtrBytes) {
        return "pfnDeletePatch";
      }
    }
    if constexpr (aerogpu_has_member_pfnProcessVertices<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnProcessVertices) / kPtrBytes) {
        return "pfnProcessVertices";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetRasterStatus<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetRasterStatus) / kPtrBytes) {
        return "pfnGetRasterStatus";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetDialogBoxMode<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetDialogBoxMode) / kPtrBytes) {
        return "pfnSetDialogBoxMode";
      }
    }
    if constexpr (aerogpu_has_member_pfnDrawPrimitive2<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawPrimitive2) / kPtrBytes) {
        return "pfnDrawPrimitive2";
      }
    }
    if constexpr (aerogpu_has_member_pfnDrawIndexedPrimitive2<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnDrawIndexedPrimitive2) / kPtrBytes) {
        return "pfnDrawIndexedPrimitive2";
      }
    }
    if (index == offsetof(FuncTable, pfnCreateSwapChain) / kPtrBytes) {
      return "pfnCreateSwapChain";
    }
    if (index == offsetof(FuncTable, pfnDestroySwapChain) / kPtrBytes) {
      return "pfnDestroySwapChain";
    }
    if (index == offsetof(FuncTable, pfnGetSwapChain) / kPtrBytes) {
      return "pfnGetSwapChain";
    }
    if (index == offsetof(FuncTable, pfnSetSwapChain) / kPtrBytes) {
      return "pfnSetSwapChain";
    }
    if (index == offsetof(FuncTable, pfnReset) / kPtrBytes) {
      return "pfnReset";
    }
    if (index == offsetof(FuncTable, pfnResetEx) / kPtrBytes) {
      return "pfnResetEx";
    }
    if (index == offsetof(FuncTable, pfnCheckDeviceState) / kPtrBytes) {
      return "pfnCheckDeviceState";
    }
    if constexpr (aerogpu_has_member_pfnWaitForVBlank<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnWaitForVBlank) / kPtrBytes) {
        return "pfnWaitForVBlank";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetGPUThreadPriority<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetGPUThreadPriority) / kPtrBytes) {
        return "pfnSetGPUThreadPriority";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetGPUThreadPriority<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetGPUThreadPriority) / kPtrBytes) {
        return "pfnGetGPUThreadPriority";
      }
    }
    if constexpr (aerogpu_has_member_pfnCheckResourceResidency<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnCheckResourceResidency) / kPtrBytes) {
        return "pfnCheckResourceResidency";
      }
    }
    if constexpr (aerogpu_has_member_pfnQueryResourceResidency<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnQueryResourceResidency) / kPtrBytes) {
        return "pfnQueryResourceResidency";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetPriority<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetPriority) / kPtrBytes) {
        return "pfnSetPriority";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetPriority<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetPriority) / kPtrBytes) {
        return "pfnGetPriority";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetDisplayModeEx<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetDisplayModeEx) / kPtrBytes) {
        return "pfnGetDisplayModeEx";
      }
    }
    if constexpr (aerogpu_has_member_pfnComposeRects<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnComposeRects) / kPtrBytes) {
        return "pfnComposeRects";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetConvolutionMonoKernel<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetConvolutionMonoKernel) / kPtrBytes) {
        return "pfnSetConvolutionMonoKernel";
      }
    }
    if constexpr (aerogpu_has_member_pfnSetAutoGenFilterType<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnSetAutoGenFilterType) / kPtrBytes) {
        return "pfnSetAutoGenFilterType";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetAutoGenFilterType<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetAutoGenFilterType) / kPtrBytes) {
        return "pfnGetAutoGenFilterType";
      }
    }
    if constexpr (aerogpu_has_member_pfnGenerateMipSubLevels<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGenerateMipSubLevels) / kPtrBytes) {
        return "pfnGenerateMipSubLevels";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetSoftwareVertexProcessing<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetSoftwareVertexProcessing) / kPtrBytes) {
        return "pfnGetSoftwareVertexProcessing";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetTransform<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetTransform) / kPtrBytes) {
        return "pfnGetTransform";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetClipPlane<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetClipPlane) / kPtrBytes) {
        return "pfnGetClipPlane";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetViewport<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetViewport) / kPtrBytes) {
        return "pfnGetViewport";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetScissorRect<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetScissorRect) / kPtrBytes) {
        return "pfnGetScissorRect";
      }
    }
    if constexpr (aerogpu_has_member_pfnBeginStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnBeginStateBlock) / kPtrBytes) {
        return "pfnBeginStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnEndStateBlock<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnEndStateBlock) / kPtrBytes) {
        return "pfnEndStateBlock";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetMaterial<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetMaterial) / kPtrBytes) {
        return "pfnGetMaterial";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetLight<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetLight) / kPtrBytes) {
        return "pfnGetLight";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetLightEnable<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetLightEnable) / kPtrBytes) {
        return "pfnGetLightEnable";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetRenderTarget<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetRenderTarget) / kPtrBytes) {
        return "pfnGetRenderTarget";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetDepthStencil<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetDepthStencil) / kPtrBytes) {
        return "pfnGetDepthStencil";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetTexture<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetTexture) / kPtrBytes) {
        return "pfnGetTexture";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetTextureStageState<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetTextureStageState) / kPtrBytes) {
        return "pfnGetTextureStageState";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetSamplerState<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetSamplerState) / kPtrBytes) {
        return "pfnGetSamplerState";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetRenderState<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetRenderState) / kPtrBytes) {
        return "pfnGetRenderState";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetPaletteEntries<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetPaletteEntries) / kPtrBytes) {
        return "pfnGetPaletteEntries";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetCurrentTexturePalette<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetCurrentTexturePalette) / kPtrBytes) {
        return "pfnGetCurrentTexturePalette";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetNPatchMode<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetNPatchMode) / kPtrBytes) {
        return "pfnGetNPatchMode";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetFVF<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetFVF) / kPtrBytes) {
        return "pfnGetFVF";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetVertexDecl<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetVertexDecl) / kPtrBytes) {
        return "pfnGetVertexDecl";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetStreamSource<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetStreamSource) / kPtrBytes) {
        return "pfnGetStreamSource";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetStreamSourceFreq<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetStreamSourceFreq) / kPtrBytes) {
        return "pfnGetStreamSourceFreq";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetIndices<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetIndices) / kPtrBytes) {
        return "pfnGetIndices";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetShader<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetShader) / kPtrBytes) {
        return "pfnGetShader";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetShaderConstF<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetShaderConstF) / kPtrBytes) {
        return "pfnGetShaderConstF";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetShaderConstI<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetShaderConstI) / kPtrBytes) {
        return "pfnGetShaderConstI";
      }
    }
    if constexpr (aerogpu_has_member_pfnGetShaderConstB<FuncTable>::value) {
      if (index == offsetof(FuncTable, pfnGetShaderConstB) / kPtrBytes) {
        return "pfnGetShaderConstB";
      }
    }
    if (index == offsetof(FuncTable, pfnRotateResourceIdentities) / kPtrBytes) {
      return "pfnRotateResourceIdentities";
    }
    if (index == offsetof(FuncTable, pfnPresent) / kPtrBytes) {
      return "pfnPresent";
    }
    if (index == offsetof(FuncTable, pfnPresentEx) / kPtrBytes) {
      return "pfnPresentEx";
    }
    if (index == offsetof(FuncTable, pfnFlush) / kPtrBytes) {
      return "pfnFlush";
    }
    if (index == offsetof(FuncTable, pfnSetMaximumFrameLatency) / kPtrBytes) {
      return "pfnSetMaximumFrameLatency";
    }
    if (index == offsetof(FuncTable, pfnGetMaximumFrameLatency) / kPtrBytes) {
      return "pfnGetMaximumFrameLatency";
    }
    if (index == offsetof(FuncTable, pfnGetPresentStats) / kPtrBytes) {
      return "pfnGetPresentStats";
    }
    if (index == offsetof(FuncTable, pfnGetLastPresentCount) / kPtrBytes) {
      return "pfnGetLastPresentCount";
    }
    if (index == offsetof(FuncTable, pfnCreateQuery) / kPtrBytes) {
      return "pfnCreateQuery";
    }
    if (index == offsetof(FuncTable, pfnDestroyQuery) / kPtrBytes) {
      return "pfnDestroyQuery";
    }
    if (index == offsetof(FuncTable, pfnIssueQuery) / kPtrBytes) {
      return "pfnIssueQuery";
    }
    if (index == offsetof(FuncTable, pfnGetQueryData) / kPtrBytes) {
      return "pfnGetQueryData";
    }
    if (index == offsetof(FuncTable, pfnGetRenderTargetData) / kPtrBytes) {
      return "pfnGetRenderTargetData";
    }
    if (index == offsetof(FuncTable, pfnCopyRects) / kPtrBytes) {
      return "pfnCopyRects";
    }
    if (index == offsetof(FuncTable, pfnWaitForIdle) / kPtrBytes) {
      return "pfnWaitForIdle";
    }
    if (index == offsetof(FuncTable, pfnBlt) / kPtrBytes) {
      return "pfnBlt";
    }
    if (index == offsetof(FuncTable, pfnColorFill) / kPtrBytes) {
      return "pfnColorFill";
    }
    if (index == offsetof(FuncTable, pfnUpdateSurface) / kPtrBytes) {
      return "pfnUpdateSurface";
    }
    if (index == offsetof(FuncTable, pfnUpdateTexture) / kPtrBytes) {
      return "pfnUpdateTexture";
    }
  }
  return nullptr;
}

#if defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
template <typename T, typename = void>
struct aerogpu_has_member_pDrvPrivate : std::false_type {};

template <typename T>
struct aerogpu_has_member_pDrvPrivate<T, std::void_t<decltype(std::declval<T>().pDrvPrivate)>> : std::true_type {};

template <typename T>
uint64_t d3d9_stub_trace_arg(const T& v) {
  if constexpr (aerogpu_has_member_pDrvPrivate<T>::value) {
    return d3d9_trace_arg_ptr(v.pDrvPrivate);
  } else if constexpr (std::is_pointer_v<T>) {
    return d3d9_trace_arg_ptr(v);
  } else if constexpr (std::is_enum_v<T>) {
    using Under = std::underlying_type_t<T>;
    return static_cast<uint64_t>(static_cast<Under>(v));
  } else if constexpr (std::is_integral_v<T>) {
    return static_cast<uint64_t>(v);
  } else {
    return 0;
  }
}

template <typename... Args>
std::array<uint64_t, 4> d3d9_stub_trace_args(const Args&... args) {
  std::array<uint64_t, 4> out{};
  size_t i = 0;
  (void)std::initializer_list<int>{
      (i < out.size() ? (out[i++] = d3d9_stub_trace_arg(args), 0) : 0)...};
  return out;
}

#define AEROGPU_D3D9_DEFINE_DDI_STUB(member, trace_func, stub_hr)                                \
  template <typename Fn>                                                                         \
  struct aerogpu_d3d9_stub_##member;                                                             \
  template <typename Ret, typename... Args>                                                      \
  struct aerogpu_d3d9_stub_##member<Ret(__stdcall*)(Args...)> {                                  \
    static Ret __stdcall member(Args... args) {                                                   \
      AEROGPU_D3D9_STUB_LOG_ONCE();                                                              \
      const auto packed = d3d9_stub_trace_args(args...);                                         \
      D3d9TraceCall trace(trace_func, packed[0], packed[1], packed[2], packed[3]);               \
      if constexpr (std::is_same_v<Ret, void>) {                                                  \
        (void)trace.ret(stub_hr);                                                                 \
        return;                                                                                   \
      }                                                                                           \
      if constexpr (std::is_same_v<Ret, HRESULT>) {                                               \
        return trace.ret(stub_hr);                                                                \
      }                                                                                           \
      (void)trace.ret(stub_hr);                                                                   \
      return Ret{};                                                                               \
    }                                                                                             \
  };                                                                                              \
  template <typename Ret, typename... Args>                                                      \
  struct aerogpu_d3d9_stub_##member<Ret(*)(Args...)> {                                            \
    static Ret member(Args... args) {                                                             \
      AEROGPU_D3D9_STUB_LOG_ONCE();                                                              \
      const auto packed = d3d9_stub_trace_args(args...);                                         \
      D3d9TraceCall trace(trace_func, packed[0], packed[1], packed[2], packed[3]);               \
      if constexpr (std::is_same_v<Ret, void>) {                                                  \
        (void)trace.ret(stub_hr);                                                                 \
        return;                                                                                   \
      }                                                                                           \
      if constexpr (std::is_same_v<Ret, HRESULT>) {                                               \
        return trace.ret(stub_hr);                                                                \
      }                                                                                           \
      (void)trace.ret(stub_hr);                                                                   \
      return Ret{};                                                                               \
    }                                                                                             \
  }

// Same as AEROGPU_D3D9_DEFINE_DDI_STUB, but without the "stub" log-once and with
// a name that does not include "(stub)" in trace strings. These entrypoints are
// treated as benign bring-up no-ops (returning a stable HRESULT) rather than
// "missing" functionality.
#define AEROGPU_D3D9_DEFINE_DDI_NOOP(member, trace_func, ok_hr)                                   \
  template <typename Fn>                                                                          \
  struct aerogpu_d3d9_noop_##member;                                                              \
  template <typename Ret, typename... Args>                                                       \
  struct aerogpu_d3d9_noop_##member<Ret(__stdcall*)(Args...)> {                                   \
    static Ret __stdcall member(Args... args) {                                                    \
      const auto packed = d3d9_stub_trace_args(args...);                                          \
      D3d9TraceCall trace(trace_func, packed[0], packed[1], packed[2], packed[3]);                \
      if constexpr (std::is_same_v<Ret, void>) {                                                   \
        (void)trace.ret(ok_hr);                                                                    \
        return;                                                                                    \
      }                                                                                            \
      if constexpr (std::is_same_v<Ret, HRESULT>) {                                                \
        return trace.ret(ok_hr);                                                                   \
      }                                                                                            \
      (void)trace.ret(ok_hr);                                                                      \
      return Ret{};                                                                                \
    }                                                                                              \
  };                                                                                               \
  template <typename Ret, typename... Args>                                                       \
  struct aerogpu_d3d9_noop_##member<Ret(*)(Args...)> {                                             \
    static Ret member(Args... args) {                                                              \
      const auto packed = d3d9_stub_trace_args(args...);                                          \
      D3d9TraceCall trace(trace_func, packed[0], packed[1], packed[2], packed[3]);                \
      if constexpr (std::is_same_v<Ret, void>) {                                                   \
        (void)trace.ret(ok_hr);                                                                    \
        return;                                                                                    \
      }                                                                                            \
      if constexpr (std::is_same_v<Ret, HRESULT>) {                                                \
        return trace.ret(ok_hr);                                                                   \
      }                                                                                            \
      (void)trace.ret(ok_hr);                                                                      \
      return Ret{};                                                                                \
    }                                                                                              \
  }

// Stubbed entrypoints: keep these non-NULL so the Win7 runtime can call into the
// UMD without crashing. See `drivers/aerogpu/umd/d3d9/README.md`.
// (Legacy fixed-function Set*/Get* state is cached and implemented elsewhere.)

// Shader constant paths (int/bool) are emitted and cached so Set*/Get*
// round-trips and state blocks are deterministic.

// D3D9Ex image processing API. Treat as a no-op until the fixed-function path is
// fully implemented (DWM should not rely on it).
AEROGPU_D3D9_DEFINE_DDI_NOOP(pfnSetConvolutionMonoKernel, D3d9TraceFunc::DeviceSetConvolutionMonoKernel, S_OK);

// Cursor management is implemented via driver-private KMD escapes when available
// (programs the AeroGPU cursor MMIO block), with a software overlay fallback
// composited at Present time (see device_set_cursor_* and the Present paths).

// Patch rendering is implemented (DrawRectPatch/DrawTriPatch/DeletePatch).
// ProcessVertices is implemented for a small fixed-function subset elsewhere.

// Dialog-box mode impacts present/occlusion semantics; treat as a no-op for bring-up.
AEROGPU_D3D9_DEFINE_DDI_NOOP(pfnSetDialogBoxMode, D3d9TraceFunc::DeviceSetDialogBoxMode, S_OK);

// Legacy user-pointer draw path (indexed). Implemented (see device_draw_indexed_primitive_up).

// Various state "getters" (largely used by legacy apps). These have output
// parameters; return a clean failure so callers don't consume uninitialized
// memory.
// (Shader bool/int constant getters are implemented and cached elsewhere.)

#undef AEROGPU_D3D9_DEFINE_DDI_STUB
#undef AEROGPU_D3D9_DEFINE_DDI_NOOP
#endif  // AEROGPU_D3D9_USE_WDK_DDI

// -----------------------------------------------------------------------------
// Type-safe D3D9 DDI thunks
// -----------------------------------------------------------------------------
// D3D9 DDIs are invoked through function tables filled during OpenAdapter/CreateDevice.
// Use compiler-checked thunks (instead of function-pointer casts) so signature
// mismatches are diagnosed at build time, and to ensure C++ exceptions never
// escape across the UMD ABI boundary.
template <typename Fn, auto Impl>
struct aerogpu_d3d9_ddi_thunk;

template <typename Ret, typename... Args, auto Impl>
struct aerogpu_d3d9_ddi_thunk<Ret(__stdcall*)(Args...), Impl> {
  using impl_return_t = decltype(Impl(std::declval<Args>()...));
  static_assert(std::is_same_v<impl_return_t, Ret>,
                "D3D9 DDI entrypoint return type mismatch (write an explicit adapter instead of casting)");

  static Ret __stdcall thunk(Args... args) noexcept {
    try {
      if constexpr (std::is_same_v<Ret, void>) {
        Impl(args...);
        return;
      } else {
        return Impl(args...);
      }
    } catch (const std::bad_alloc&) {
      if constexpr (std::is_same_v<Ret, HRESULT>) {
        return E_OUTOFMEMORY;
      } else if constexpr (std::is_same_v<Ret, void>) {
        return;
      } else {
        return Ret{};
      }
    } catch (...) {
      if constexpr (std::is_same_v<Ret, HRESULT>) {
        return E_FAIL;
      } else if constexpr (std::is_same_v<Ret, void>) {
        return;
      } else {
        return Ret{};
      }
    }
  }
};

template <typename Ret, typename... Args, auto Impl>
struct aerogpu_d3d9_ddi_thunk<Ret(*)(Args...), Impl> {
  using impl_return_t = decltype(Impl(std::declval<Args>()...));
  static_assert(std::is_same_v<impl_return_t, Ret>,
                "D3D9 DDI entrypoint return type mismatch (write an explicit adapter instead of casting)");

  static Ret thunk(Args... args) noexcept {
    try {
      if constexpr (std::is_same_v<Ret, void>) {
        Impl(args...);
        return;
      } else {
        return Impl(args...);
      }
    } catch (const std::bad_alloc&) {
      if constexpr (std::is_same_v<Ret, HRESULT>) {
        return E_OUTOFMEMORY;
      } else if constexpr (std::is_same_v<Ret, void>) {
        return;
      } else {
        return Ret{};
      }
    } catch (...) {
      if constexpr (std::is_same_v<Ret, HRESULT>) {
        return E_FAIL;
      } else if constexpr (std::is_same_v<Ret, void>) {
        return;
      } else {
        return Ret{};
      }
    }
  }
};

uint64_t monotonic_ms() {
#if defined(_WIN32)
  return static_cast<uint64_t>(GetTickCount64());
#else
  using namespace std::chrono;
  return static_cast<uint64_t>(duration_cast<milliseconds>(steady_clock::now().time_since_epoch()).count());
#endif
}

namespace {
#if defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
template <typename T, typename = void>
struct has_member_hAllocation : std::false_type {};
template <typename T>
struct has_member_hAllocation<T, std::void_t<decltype(std::declval<T&>().hAllocation)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NumAllocations_res : std::false_type {};
template <typename T>
struct has_member_NumAllocations_res<T, std::void_t<decltype(std::declval<T&>().NumAllocations)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_hAllocations : std::false_type {};
template <typename T>
struct has_member_hAllocations<T, std::void_t<decltype(std::declval<T&>().hAllocations)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pAllocations : std::false_type {};
template <typename T>
struct has_member_pAllocations<T, std::void_t<decltype(std::declval<T&>().pAllocations)>> : std::true_type {};

template <typename ArgsT>
WddmAllocationHandle extract_primary_wddm_allocation_handle(const ArgsT& args) {
  if constexpr (has_member_hAllocation<ArgsT>::value &&
                std::is_convertible_v<decltype(std::declval<ArgsT&>().hAllocation), WddmAllocationHandle>) {
    const auto h = static_cast<WddmAllocationHandle>(args.hAllocation);
    if (h != 0) {
      return h;
    }
  }

  if constexpr (has_member_hAllocations<ArgsT>::value) {
    const UINT count = [&]() -> UINT {
      if constexpr (has_member_NumAllocations_res<ArgsT>::value) {
        return static_cast<UINT>(args.NumAllocations);
      }
      return 0;
    }();

    if (count != 0) {
      // `hAllocations` is typically a pointer/array of allocation handles.
      const auto* handles = args.hAllocations;
      if (handles) {
        if constexpr (std::is_convertible_v<std::remove_reference_t<decltype(handles[0])>, WddmAllocationHandle>) {
          const auto h = static_cast<WddmAllocationHandle>(handles[0]);
          if (h != 0) {
            return h;
          }
        }
      }
    }
  }

  if constexpr (has_member_pAllocations<ArgsT>::value) {
    const UINT count = [&]() -> UINT {
      if constexpr (has_member_NumAllocations_res<ArgsT>::value) {
        return static_cast<UINT>(args.NumAllocations);
      }
      // Some structs may omit a count; assume at least 1 when a pointer is present.
      return 1;
    }();

    if (count != 0) {
      const auto* allocs = args.pAllocations;
      if (allocs) {
        using Elem = std::remove_pointer_t<decltype(allocs)>;
        if constexpr (std::is_class_v<Elem> && has_member_hAllocation<Elem>::value &&
                      std::is_convertible_v<decltype(std::declval<Elem&>().hAllocation), WddmAllocationHandle>) {
          const auto h = static_cast<WddmAllocationHandle>(allocs[0].hAllocation);
          if (h != 0) {
            return h;
          }
        } else if constexpr (!std::is_class_v<Elem> && std::is_convertible_v<Elem, WddmAllocationHandle>) {
          const auto h = static_cast<WddmAllocationHandle>(allocs[0]);
          if (h != 0) {
            return h;
          }
        }
      }
    }
  }

  return 0;
}
#endif

WddmAllocationHandle get_wddm_allocation_from_create_resource(const D3D9DDIARG_CREATERESOURCE* args) {
  if (!args) {
    return 0;
  }

#if defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return extract_primary_wddm_allocation_handle(*args);
#else
  return static_cast<WddmAllocationHandle>(args->wddm_hAllocation);
#endif
}

} // namespace

// -----------------------------------------------------------------------------
// D3D9 DDI struct member accessors
// -----------------------------------------------------------------------------
// The portable ABI subset in `include/aerogpu_d3d9_umd.h` intentionally models
// only the fields exercised by the current translation layer. When building
// against real WDK headers, the same structs may use different member spellings
// (e.g. `Type` vs `type`, `OffsetToLock` vs `offset_bytes`). Use compile-time
// member detection to keep the driver buildable across header vintages.

template <typename...>
struct aerogpu_d3d9_always_false : std::false_type {};

#define AEROGPU_D3D9_DEFINE_HAS_MEMBER(member)                                                    \
  template <typename T, typename = void>                                                          \
  struct aerogpu_d3d9_has_member_##member : std::false_type {};                                   \
  template <typename T>                                                                           \
  struct aerogpu_d3d9_has_member_##member<T, std::void_t<decltype(std::declval<T&>().member)>>    \
      : std::true_type {}

// Resource description fields.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Type);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(type);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Format);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(format);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Width);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(width);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Height);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(height);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Depth);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(depth);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(MipLevels);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(mip_levels);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Usage);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(usage);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Pool);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pool);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Size);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(size);

// Present parameters fields.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(BackBufferWidth);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(backbuffer_width);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(BackBufferHeight);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(backbuffer_height);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(BackBufferFormat);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(backbuffer_format);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(BackBufferCount);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(backbuffer_count);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(SwapEffect);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(swap_effect);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(PresentationInterval);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(presentation_interval);

// State-block args (CreateStateBlock/ValidateDevice).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(StateBlockType);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hStateBlock);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pNumPasses);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(NumPasses);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Windowed);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(windowed);

// Common flag-style fields (appear in many arg structs).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Flags);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(flags);
// Some D3D9UMDDI structs (e.g. PresentEx in the repo-local ABI subset) use a more
// explicit name for the same flag field.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(d3d9_present_flags);

// Lock/unlock arg fields.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(OffsetToLock);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(SizeToLock);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(OffsetToUnlock);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(SizeToUnlock);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(offset_bytes);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(size_bytes);

// ProcessVertices arg fields (not present in all header vintages).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(DestStride);

// Present arg fields (member names vary between `hWnd` and `hWindow`, `hSrc` and
// `hSrcResource`, etc).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hWnd);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hWindow);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(SyncInterval);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(sync_interval);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hSrc);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hSrcResource);

// Cursor arg fields (SetCursorProperties/SetCursorPosition/ShowCursor).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(XHotSpot);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(xHotSpot);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(YHotSpot);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(yHotSpot);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hCursor);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hCursorBitmap);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hCursorBitmapResource);
// SetCursorPosition struct spellings.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(X);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(x);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Y);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(y);
// ShowCursor struct spellings.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Show);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(show);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(bShow);

// Query arg fields (member names vary across header sets).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(QueryType);

// Locked box output fields.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pData);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pBits);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(RowPitch);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(rowPitch);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(SlicePitch);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(slicePitch);

// Swap chain / reset arg fields: some header sets embed a present-parameters
// struct, others store a pointer.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pPresentParameters);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pPresentationParameters);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(PresentParameters);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(PresentationParameters);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(present_params);

// Misc arg fields used by the translation layer.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(rect_count);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(RectCount);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(NumRects);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pSrcRects);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pRects);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pRectList);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(resource_count);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(NumResources);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(data_size);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(DataSize);
// Per-allocation private driver data blob fields (shared resources/OpenResource).
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pPrivateDriverData);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pKmdAllocPrivateData);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(PrivateDriverDataSize);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(private_driver_data_size);

// Blt/ColorFill/Update* fields.
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hDst);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hDstResource);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(hDestResource);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(filter);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Filter);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(color_argb);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(Color);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pDstRect);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pDestRect);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pDstPoint);
AEROGPU_D3D9_DEFINE_HAS_MEMBER(pDestPoint);

#undef AEROGPU_D3D9_DEFINE_HAS_MEMBER

template <typename ArgsT>
uint32_t d3d9_resource_type(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Type<ArgsT>::value) {
    return static_cast<uint32_t>(args.Type);
  } else if constexpr (aerogpu_d3d9_has_member_type<ArgsT>::value) {
    return static_cast<uint32_t>(args.type);
  } else {
    static_assert(aerogpu_d3d9_always_false<ArgsT>::value, "D3D9 resource args missing Type/type member");
  }
}

template <typename ArgsT>
uint32_t d3d9_optional_resource_type(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Type<ArgsT>::value) {
    return static_cast<uint32_t>(args.Type);
  } else if constexpr (aerogpu_d3d9_has_member_type<ArgsT>::value) {
    return static_cast<uint32_t>(args.type);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_format(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Format<ArgsT>::value) {
    return static_cast<uint32_t>(args.Format);
  } else if constexpr (aerogpu_d3d9_has_member_format<ArgsT>::value) {
    return static_cast<uint32_t>(args.format);
  } else {
    static_assert(aerogpu_d3d9_always_false<ArgsT>::value, "D3D9 resource args missing Format/format member");
  }
}

template <typename ArgsT>
uint32_t d3d9_optional_resource_format(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Format<ArgsT>::value) {
    return static_cast<uint32_t>(args.Format);
  } else if constexpr (aerogpu_d3d9_has_member_format<ArgsT>::value) {
    return static_cast<uint32_t>(args.format);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_width(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Width<ArgsT>::value) {
    return static_cast<uint32_t>(args.Width);
  } else if constexpr (aerogpu_d3d9_has_member_width<ArgsT>::value) {
    return static_cast<uint32_t>(args.width);
  } else {
    static_assert(aerogpu_d3d9_always_false<ArgsT>::value, "D3D9 resource args missing Width/width member");
  }
}

template <typename ArgsT>
uint32_t d3d9_optional_resource_width(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Width<ArgsT>::value) {
    return static_cast<uint32_t>(args.Width);
  } else if constexpr (aerogpu_d3d9_has_member_width<ArgsT>::value) {
    return static_cast<uint32_t>(args.width);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_height(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Height<ArgsT>::value) {
    return static_cast<uint32_t>(args.Height);
  } else if constexpr (aerogpu_d3d9_has_member_height<ArgsT>::value) {
    return static_cast<uint32_t>(args.height);
  } else {
    static_assert(aerogpu_d3d9_always_false<ArgsT>::value, "D3D9 resource args missing Height/height member");
  }
}

template <typename ArgsT>
uint32_t d3d9_optional_resource_height(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Height<ArgsT>::value) {
    return static_cast<uint32_t>(args.Height);
  } else if constexpr (aerogpu_d3d9_has_member_height<ArgsT>::value) {
    return static_cast<uint32_t>(args.height);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_depth(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Depth<ArgsT>::value) {
    return static_cast<uint32_t>(args.Depth);
  } else if constexpr (aerogpu_d3d9_has_member_depth<ArgsT>::value) {
    return static_cast<uint32_t>(args.depth);
  } else {
    return 1u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_mip_levels(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_MipLevels<ArgsT>::value) {
    return static_cast<uint32_t>(args.MipLevels);
  } else if constexpr (aerogpu_d3d9_has_member_mip_levels<ArgsT>::value) {
    return static_cast<uint32_t>(args.mip_levels);
  } else {
    return 1u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_usage(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Usage<ArgsT>::value) {
    return static_cast<uint32_t>(args.Usage);
  } else if constexpr (aerogpu_d3d9_has_member_usage<ArgsT>::value) {
    return static_cast<uint32_t>(args.usage);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_pool(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Pool<ArgsT>::value) {
    return static_cast<uint32_t>(args.Pool);
  } else if constexpr (aerogpu_d3d9_has_member_pool<ArgsT>::value) {
    return static_cast<uint32_t>(args.pool);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
uint32_t d3d9_resource_size(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_Size<ArgsT>::value) {
    return static_cast<uint32_t>(args.Size);
  } else if constexpr (aerogpu_d3d9_has_member_size<ArgsT>::value) {
    return static_cast<uint32_t>(args.size);
  } else {
    return 0u;
  }
}

template <typename LockT>
uint32_t d3d9_lock_offset(const LockT& lock) {
  if constexpr (aerogpu_d3d9_has_member_OffsetToLock<LockT>::value) {
    return static_cast<uint32_t>(lock.OffsetToLock);
  } else if constexpr (aerogpu_d3d9_has_member_offset_bytes<LockT>::value) {
    return static_cast<uint32_t>(lock.offset_bytes);
  } else {
    return 0u;
  }
}

template <typename LockT>
uint32_t d3d9_lock_size(const LockT& lock) {
  if constexpr (aerogpu_d3d9_has_member_SizeToLock<LockT>::value) {
    return static_cast<uint32_t>(lock.SizeToLock);
  } else if constexpr (aerogpu_d3d9_has_member_size_bytes<LockT>::value) {
    return static_cast<uint32_t>(lock.size_bytes);
  } else {
    return 0u;
  }
}

template <typename LockT>
uint32_t d3d9_lock_flags(const LockT& lock) {
  if constexpr (aerogpu_d3d9_has_member_Flags<LockT>::value) {
    return static_cast<uint32_t>(lock.Flags);
  } else if constexpr (aerogpu_d3d9_has_member_flags<LockT>::value) {
    return static_cast<uint32_t>(lock.flags);
  } else {
    return 0u;
  }
}

template <typename UnlockT>
uint32_t d3d9_unlock_offset(const UnlockT& unlock) {
  if constexpr (aerogpu_d3d9_has_member_OffsetToUnlock<UnlockT>::value) {
    return static_cast<uint32_t>(unlock.OffsetToUnlock);
  } else if constexpr (aerogpu_d3d9_has_member_offset_bytes<UnlockT>::value) {
    return static_cast<uint32_t>(unlock.offset_bytes);
  } else {
    return 0u;
  }
}

template <typename UnlockT>
uint32_t d3d9_unlock_size(const UnlockT& unlock) {
  if constexpr (aerogpu_d3d9_has_member_SizeToUnlock<UnlockT>::value) {
    return static_cast<uint32_t>(unlock.SizeToUnlock);
  } else if constexpr (aerogpu_d3d9_has_member_size_bytes<UnlockT>::value) {
    return static_cast<uint32_t>(unlock.size_bytes);
  } else {
    return 0u;
  }
}

template <typename PresentT>
HWND d3d9_present_hwnd(const PresentT& present) {
  if constexpr (aerogpu_d3d9_has_member_hWnd<PresentT>::value) {
    return present.hWnd;
  } else if constexpr (aerogpu_d3d9_has_member_hWindow<PresentT>::value) {
    return present.hWindow;
  } else {
    return nullptr;
  }
}

template <typename PresentT>
D3DDDI_HRESOURCE d3d9_present_src(const PresentT& present) {
  if constexpr (aerogpu_d3d9_has_member_hSrc<PresentT>::value) {
    return present.hSrc;
  } else if constexpr (aerogpu_d3d9_has_member_hSrcResource<PresentT>::value) {
    return present.hSrcResource;
  } else {
    return {};
  }
}

template <typename PresentT>
uint32_t d3d9_present_sync_interval(const PresentT& present) {
  if constexpr (aerogpu_d3d9_has_member_SyncInterval<PresentT>::value) {
    return static_cast<uint32_t>(present.SyncInterval);
  } else if constexpr (aerogpu_d3d9_has_member_sync_interval<PresentT>::value) {
    return static_cast<uint32_t>(present.sync_interval);
  } else {
    return 0u;
  }
}

template <typename PresentT>
uint32_t d3d9_present_flags(const PresentT& present) {
  if constexpr (aerogpu_d3d9_has_member_Flags<PresentT>::value) {
    return static_cast<uint32_t>(present.Flags);
  } else if constexpr (aerogpu_d3d9_has_member_flags<PresentT>::value) {
    return static_cast<uint32_t>(present.flags);
  } else if constexpr (aerogpu_d3d9_has_member_d3d9_present_flags<PresentT>::value) {
    return static_cast<uint32_t>(present.d3d9_present_flags);
  } else {
    return 0u;
  }
}

template <typename QueryT>
uint32_t d3d9_query_type(const QueryT& query) {
  if constexpr (aerogpu_d3d9_has_member_QueryType<QueryT>::value) {
    return static_cast<uint32_t>(query.QueryType);
  } else if constexpr (aerogpu_d3d9_has_member_Type<QueryT>::value) {
    return static_cast<uint32_t>(query.Type);
  } else if constexpr (aerogpu_d3d9_has_member_type<QueryT>::value) {
    return static_cast<uint32_t>(query.type);
  } else {
    static_assert(aerogpu_d3d9_always_false<QueryT>::value, "D3D9 query args missing QueryType/Type/type member");
  }
}

template <typename LockedBoxT>
void d3d9_locked_box_set_ptr(LockedBoxT* box, void* ptr) {
  if (!box) {
    return;
  }
  if constexpr (aerogpu_d3d9_has_member_pData<LockedBoxT>::value) {
    box->pData = ptr;
  } else if constexpr (aerogpu_d3d9_has_member_pBits<LockedBoxT>::value) {
    box->pBits = ptr;
  } else {
    static_assert(aerogpu_d3d9_always_false<LockedBoxT>::value, "LockedBox missing pData/pBits member");
  }
}

template <typename LockedBoxT>
void d3d9_locked_box_set_row_pitch(LockedBoxT* box, uint32_t pitch) {
  if (!box) {
    return;
  }
  if constexpr (aerogpu_d3d9_has_member_RowPitch<LockedBoxT>::value) {
    box->RowPitch = pitch;
  } else if constexpr (aerogpu_d3d9_has_member_rowPitch<LockedBoxT>::value) {
    box->rowPitch = pitch;
  } else {
    static_assert(aerogpu_d3d9_always_false<LockedBoxT>::value, "LockedBox missing RowPitch/rowPitch member");
  }
}

template <typename LockedBoxT>
void d3d9_locked_box_set_slice_pitch(LockedBoxT* box, uint32_t pitch) {
  if (!box) {
    return;
  }
  if constexpr (aerogpu_d3d9_has_member_SlicePitch<LockedBoxT>::value) {
    box->SlicePitch = pitch;
  } else if constexpr (aerogpu_d3d9_has_member_slicePitch<LockedBoxT>::value) {
    box->slicePitch = pitch;
  } else {
    static_assert(aerogpu_d3d9_always_false<LockedBoxT>::value, "LockedBox missing SlicePitch/slicePitch member");
  }
}

template <typename SwapArgsT>
const D3D9DDI_PRESENT_PARAMETERS* d3d9_get_present_params(const SwapArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_pPresentParameters<SwapArgsT>::value) {
    return args.pPresentParameters;
  } else if constexpr (aerogpu_d3d9_has_member_pPresentationParameters<SwapArgsT>::value) {
    return args.pPresentationParameters;
  } else if constexpr (aerogpu_d3d9_has_member_PresentParameters<SwapArgsT>::value) {
    return &args.PresentParameters;
  } else if constexpr (aerogpu_d3d9_has_member_PresentationParameters<SwapArgsT>::value) {
    return &args.PresentationParameters;
  } else if constexpr (aerogpu_d3d9_has_member_present_params<SwapArgsT>::value) {
    return &args.present_params;
  } else {
    return nullptr;
  }
}

template <typename PpT>
uint32_t d3d9_pp_backbuffer_width(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_BackBufferWidth<PpT>::value) {
    return static_cast<uint32_t>(pp.BackBufferWidth);
  } else if constexpr (aerogpu_d3d9_has_member_backbuffer_width<PpT>::value) {
    return static_cast<uint32_t>(pp.backbuffer_width);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_backbuffer_height(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_BackBufferHeight<PpT>::value) {
    return static_cast<uint32_t>(pp.BackBufferHeight);
  } else if constexpr (aerogpu_d3d9_has_member_backbuffer_height<PpT>::value) {
    return static_cast<uint32_t>(pp.backbuffer_height);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_backbuffer_format(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_BackBufferFormat<PpT>::value) {
    return static_cast<uint32_t>(pp.BackBufferFormat);
  } else if constexpr (aerogpu_d3d9_has_member_backbuffer_format<PpT>::value) {
    return static_cast<uint32_t>(pp.backbuffer_format);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_backbuffer_count(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_BackBufferCount<PpT>::value) {
    return static_cast<uint32_t>(pp.BackBufferCount);
  } else if constexpr (aerogpu_d3d9_has_member_backbuffer_count<PpT>::value) {
    return static_cast<uint32_t>(pp.backbuffer_count);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_swap_effect(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_SwapEffect<PpT>::value) {
    return static_cast<uint32_t>(pp.SwapEffect);
  } else if constexpr (aerogpu_d3d9_has_member_swap_effect<PpT>::value) {
    return static_cast<uint32_t>(pp.swap_effect);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_flags(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_Flags<PpT>::value) {
    return static_cast<uint32_t>(pp.Flags);
  } else if constexpr (aerogpu_d3d9_has_member_flags<PpT>::value) {
    return static_cast<uint32_t>(pp.flags);
  } else {
    return 0u;
  }
}

template <typename PpT>
uint32_t d3d9_pp_presentation_interval(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_PresentationInterval<PpT>::value) {
    return static_cast<uint32_t>(pp.PresentationInterval);
  } else if constexpr (aerogpu_d3d9_has_member_presentation_interval<PpT>::value) {
    return static_cast<uint32_t>(pp.presentation_interval);
  } else {
    return 0u;
  }
}

template <typename PpT>
BOOL d3d9_pp_windowed(const PpT& pp) {
  if constexpr (aerogpu_d3d9_has_member_Windowed<PpT>::value) {
    return pp.Windowed;
  } else if constexpr (aerogpu_d3d9_has_member_windowed<PpT>::value) {
    return pp.windowed;
  } else {
    return TRUE;
  }
}

template <typename CopyRectsT>
uint32_t d3d9_copy_rects_count(const CopyRectsT& args) {
  if constexpr (aerogpu_d3d9_has_member_NumRects<CopyRectsT>::value) {
    return static_cast<uint32_t>(args.NumRects);
  } else if constexpr (aerogpu_d3d9_has_member_RectCount<CopyRectsT>::value) {
    return static_cast<uint32_t>(args.RectCount);
  } else if constexpr (aerogpu_d3d9_has_member_rect_count<CopyRectsT>::value) {
    return static_cast<uint32_t>(args.rect_count);
  } else {
    return 0u;
  }
}

template <typename CopyRectsT>
const RECT* d3d9_copy_rects_rects(const CopyRectsT& args) {
  if constexpr (aerogpu_d3d9_has_member_pRects<CopyRectsT>::value) {
    return args.pRects;
  } else if constexpr (aerogpu_d3d9_has_member_pRectList<CopyRectsT>::value) {
    return args.pRectList;
  } else if constexpr (aerogpu_d3d9_has_member_pSrcRects<CopyRectsT>::value) {
    return args.pSrcRects;
  } else {
    return nullptr;
  }
}

template <typename QueryResT>
uint32_t d3d9_query_resource_residency_count(const QueryResT& args) {
  if constexpr (aerogpu_d3d9_has_member_NumResources<QueryResT>::value) {
    return static_cast<uint32_t>(args.NumResources);
  } else if constexpr (aerogpu_d3d9_has_member_resource_count<QueryResT>::value) {
    return static_cast<uint32_t>(args.resource_count);
  } else {
    return 0u;
  }
}

template <typename QueryDataT>
uint32_t d3d9_query_data_size(const QueryDataT& args) {
  if constexpr (aerogpu_d3d9_has_member_DataSize<QueryDataT>::value) {
    return static_cast<uint32_t>(args.DataSize);
  } else if constexpr (aerogpu_d3d9_has_member_data_size<QueryDataT>::value) {
    return static_cast<uint32_t>(args.data_size);
  } else {
    return 0u;
  }
}

template <typename ArgsT>
const void* d3d9_private_driver_data_ptr(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_pPrivateDriverData<ArgsT>::value) {
    return args.pPrivateDriverData;
  } else if constexpr (aerogpu_d3d9_has_member_pKmdAllocPrivateData<ArgsT>::value) {
    return args.pKmdAllocPrivateData;
  } else {
    return nullptr;
  }
}

template <typename ArgsT>
uint32_t d3d9_private_driver_data_size(const ArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_PrivateDriverDataSize<ArgsT>::value) {
    return static_cast<uint32_t>(args.PrivateDriverDataSize);
  } else if constexpr (aerogpu_d3d9_has_member_private_driver_data_size<ArgsT>::value) {
    return static_cast<uint32_t>(args.private_driver_data_size);
  } else {
    return 0u;
  }
}

template <typename HResArgsT>
D3DDDI_HRESOURCE d3d9_arg_src_resource(const HResArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_hSrc<HResArgsT>::value) {
    return args.hSrc;
  } else if constexpr (aerogpu_d3d9_has_member_hSrcResource<HResArgsT>::value) {
    return args.hSrcResource;
  } else {
    return {};
  }
}

template <typename HResArgsT>
D3DDDI_HRESOURCE d3d9_arg_dst_resource(const HResArgsT& args) {
  if constexpr (aerogpu_d3d9_has_member_hDst<HResArgsT>::value) {
    return args.hDst;
  } else if constexpr (aerogpu_d3d9_has_member_hDstResource<HResArgsT>::value) {
    return args.hDstResource;
  } else if constexpr (aerogpu_d3d9_has_member_hDestResource<HResArgsT>::value) {
    return args.hDestResource;
  } else {
    return {};
  }
}

template <typename BltT>
uint32_t d3d9_blt_filter(const BltT& args) {
  if constexpr (aerogpu_d3d9_has_member_Filter<BltT>::value) {
    return static_cast<uint32_t>(args.Filter);
  } else if constexpr (aerogpu_d3d9_has_member_filter<BltT>::value) {
    return static_cast<uint32_t>(args.filter);
  } else {
    return 0u;
  }
}

template <typename ColorFillT>
uint32_t d3d9_color_fill_color(const ColorFillT& args) {
  if constexpr (aerogpu_d3d9_has_member_Color<ColorFillT>::value) {
    return static_cast<uint32_t>(args.Color);
  } else if constexpr (aerogpu_d3d9_has_member_color_argb<ColorFillT>::value) {
    return static_cast<uint32_t>(args.color_argb);
  } else {
    return 0u;
  }
}

template <typename UpdateSurfT>
const POINT* d3d9_update_surface_dst_point(const UpdateSurfT& args) {
  if constexpr (aerogpu_d3d9_has_member_pDstPoint<UpdateSurfT>::value) {
    return args.pDstPoint;
  } else if constexpr (aerogpu_d3d9_has_member_pDestPoint<UpdateSurfT>::value) {
    return args.pDestPoint;
  } else {
    return nullptr;
  }
}

template <typename UpdateSurfT>
const RECT* d3d9_update_surface_dst_rect(const UpdateSurfT& args) {
  if constexpr (aerogpu_d3d9_has_member_pDstRect<UpdateSurfT>::value) {
    return args.pDstRect;
  } else if constexpr (aerogpu_d3d9_has_member_pDestRect<UpdateSurfT>::value) {
    return args.pDestRect;
  } else {
    return nullptr;
  }
}

uint64_t qpc_now() {
#if defined(_WIN32)
  LARGE_INTEGER li;
  QueryPerformanceCounter(&li);
  return static_cast<uint64_t>(li.QuadPart);
#else
  using namespace std::chrono;
  return static_cast<uint64_t>(duration_cast<nanoseconds>(steady_clock::now().time_since_epoch()).count());
#endif
}

void sleep_ms(uint32_t ms) {
#if defined(_WIN32)
  Sleep(ms);
#else
  std::this_thread::sleep_for(std::chrono::milliseconds(ms));
#endif
}

struct FenceSnapshot {
  uint64_t last_submitted = 0;
  uint64_t last_completed = 0;
};

#if defined(_WIN32)

// Best-effort HDC -> adapter LUID translation.
//
// Win7's D3D9 runtime and DWM may open the same adapter using both the HDC and
// LUID paths. Returning a stable LUID from OpenAdapterFromHdc is critical so our
// adapter cache (keyed by LUID) maps both opens to the same Adapter instance.
using NTSTATUS = LONG;

constexpr bool nt_success(NTSTATUS st) {
  return st >= 0;
}

struct D3DKMT_OPENADAPTERFROMHDC {
  HDC hDc;
  UINT hAdapter;
  LUID AdapterLuid;
  UINT VidPnSourceId;
};

struct D3DKMT_CLOSEADAPTER {
  UINT hAdapter;
};

using PFND3DKMTOpenAdapterFromHdc = NTSTATUS(__stdcall*)(D3DKMT_OPENADAPTERFROMHDC* pData);
using PFND3DKMTCloseAdapter = NTSTATUS(__stdcall*)(D3DKMT_CLOSEADAPTER* pData);

bool get_luid_from_hdc(HDC hdc, LUID* luid_out) {
  if (!hdc || !luid_out) {
    return false;
  }

  HMODULE gdi32 = LoadLibraryW(L"gdi32.dll");
  if (!gdi32) {
    return false;
  }

  auto* open_adapter_from_hdc =
      reinterpret_cast<PFND3DKMTOpenAdapterFromHdc>(GetProcAddress(gdi32, "D3DKMTOpenAdapterFromHdc"));
  auto* close_adapter =
      reinterpret_cast<PFND3DKMTCloseAdapter>(GetProcAddress(gdi32, "D3DKMTCloseAdapter"));
  if (!open_adapter_from_hdc || !close_adapter) {
    FreeLibrary(gdi32);
    return false;
  }

  D3DKMT_OPENADAPTERFROMHDC open{};
  open.hDc = hdc;
  open.hAdapter = 0;
  std::memset(&open.AdapterLuid, 0, sizeof(open.AdapterLuid));
  open.VidPnSourceId = 0;

  const NTSTATUS st = open_adapter_from_hdc(&open);
  if (!nt_success(st) || open.hAdapter == 0) {
    FreeLibrary(gdi32);
    return false;
  }

  *luid_out = open.AdapterLuid;

  D3DKMT_CLOSEADAPTER close{};
  close.hAdapter = open.hAdapter;
  close_adapter(&close);

  FreeLibrary(gdi32);
  return true;
}

#endif

FenceSnapshot refresh_fence_snapshot(Adapter* adapter) {
  FenceSnapshot snap{};
  if (!adapter) {
    return snap;
  }

#if defined(_WIN32)
  // DWM and many D3D9Ex clients poll EVENT queries in tight loops. Querying the
  // KMD fence counter (last completed) requires a D3DKMTEscape call, so throttle
  // it to a small interval to avoid burning CPU in the kernel.
  //
  // Note: we intentionally do *not* use the escape's \"last submitted\" fence as
  // a per-submission fence ID when polling. Under multi-process workloads (DWM +
  // apps) it is global and can be dominated by another process's submissions.
  // Per-submission fence IDs must come from the runtime callbacks (e.g.
  // SubmissionFenceId / NewFenceValue).
  constexpr uint64_t kMinFenceQueryIntervalMs = 4;
  const uint64_t now_ms = monotonic_ms();
  bool should_query_kmd = false;
  {
    std::lock_guard<std::mutex> lock(adapter->fence_mutex);
    if (now_ms >= adapter->last_kmd_fence_query_ms &&
        (now_ms - adapter->last_kmd_fence_query_ms) >= kMinFenceQueryIntervalMs) {
      adapter->last_kmd_fence_query_ms = now_ms;
      should_query_kmd = true;
    }
  }

  if (should_query_kmd && adapter->kmd_query_available.load(std::memory_order_acquire)) {
    uint64_t completed = 0;
    if (adapter->kmd_query.QueryFence(/*last_submitted=*/nullptr, &completed)) {
      bool updated = false;
      {
        std::lock_guard<std::mutex> lock(adapter->fence_mutex);
        const uint64_t prev_completed = adapter->completed_fence;
        adapter->completed_fence = std::max<uint64_t>(adapter->completed_fence, completed);
        updated = (adapter->completed_fence != prev_completed);
      }
      if (updated) {
        adapter->fence_cv.notify_all();
      }
    } else {
      adapter->kmd_query_available.store(false, std::memory_order_release);
    }
  }
#endif

  {
    std::lock_guard<std::mutex> lock(adapter->fence_mutex);
    snap.last_submitted = adapter->last_submitted_fence;
    snap.last_completed = adapter->completed_fence;
  }
  return snap;
}

void retire_completed_presents_locked(Device* dev) {
  if (!dev || !dev->adapter) {
    return;
  }

  const uint64_t completed = refresh_fence_snapshot(dev->adapter).last_completed;
  while (!dev->inflight_present_fences.empty() && dev->inflight_present_fences.front() <= completed) {
    dev->inflight_present_fences.pop_front();
  }
}

enum class FenceWaitResult {
  Complete,
  NotReady,
  Failed,
};

#if defined(_WIN32)
using AerogpuNtStatus = LONG;

constexpr AerogpuNtStatus kStatusSuccess = 0x00000000L;
constexpr AerogpuNtStatus kStatusTimeout = 0x00000102L;
constexpr AerogpuNtStatus kStatusNotSupported = static_cast<AerogpuNtStatus>(0xC00000BBL);
#endif

FenceWaitResult wait_for_fence(Device* dev, uint64_t fence_value, uint32_t timeout_ms) {
  if (!dev || !dev->adapter) {
    return FenceWaitResult::Failed;
  }
  if (fence_value == 0) {
    return FenceWaitResult::Complete;
  }

  Adapter* adapter = dev->adapter;

  {
    std::lock_guard<std::mutex> lock(adapter->fence_mutex);
    if (adapter->completed_fence >= fence_value) {
      return FenceWaitResult::Complete;
    }
  }

#if defined(_WIN32)
  // For bounded waits, prefer letting the kernel wait on the WDDM sync object.
  // This avoids user-mode polling loops (Sleep(1) + repeated fence queries).
  if (timeout_ms != 0) {
    const WddmHandle sync_object = dev->wddm_context.hSyncObject;
    if (sync_object != 0) {
      const AerogpuNtStatus st = static_cast<AerogpuNtStatus>(
          adapter->kmd_query.WaitForSyncObject(static_cast<uint32_t>(sync_object), fence_value, timeout_ms));
      {
        static std::once_flag once;
        std::call_once(once, [st, timeout_ms] {
          aerogpu::logf("aerogpu-d3d9: wait_for_fence using syncobj wait (timeout_ms=%u) NTSTATUS=0x%08lx\n",
                        static_cast<unsigned>(timeout_ms),
                        static_cast<unsigned long>(st));
        });
      }
      if (st == kStatusSuccess) {
        {
          std::lock_guard<std::mutex> lock(adapter->fence_mutex);
          adapter->completed_fence = std::max(adapter->completed_fence, fence_value);
        }
        adapter->fence_cv.notify_all();
        return FenceWaitResult::Complete;
      }
      if (st == kStatusTimeout) {
        return FenceWaitResult::NotReady;
      }
    }
  }
#endif

  // Fast path: for polling callers (GetData), avoid per-call kernel waits. We
  // prefer querying the KMD fence counters (throttled inside
  // refresh_fence_snapshot) so tight polling loops don't spam syscalls.
  if (timeout_ms == 0) {
    if (refresh_fence_snapshot(adapter).last_completed >= fence_value) {
      return FenceWaitResult::Complete;
    }

#if defined(_WIN32)
    // If the KMD fence query path is unavailable, fall back to polling the WDDM
    // sync object once. This keeps EVENT queries functional even if the escape
    // path is missing.
    if (!adapter->kmd_query_available.load(std::memory_order_acquire)) {
      const WddmHandle sync_object = dev->wddm_context.hSyncObject;
      if (sync_object != 0) {
        const AerogpuNtStatus st = static_cast<AerogpuNtStatus>(
            adapter->kmd_query.WaitForSyncObject(static_cast<uint32_t>(sync_object), fence_value, /*timeout_ms=*/0));
        {
          static std::once_flag once;
          std::call_once(once, [st] {
            aerogpu::logf("aerogpu-d3d9: wait_for_fence using syncobj poll NTSTATUS=0x%08lx\n",
                          static_cast<unsigned long>(st));
          });
        }
        if (st == kStatusSuccess) {
          {
            std::lock_guard<std::mutex> lock(adapter->fence_mutex);
            adapter->completed_fence = std::max(adapter->completed_fence, fence_value);
          }
          adapter->fence_cv.notify_all();
          return FenceWaitResult::Complete;
        }
      }
    }
#endif

    return FenceWaitResult::NotReady;
  }

  const uint64_t deadline = monotonic_ms() + timeout_ms;
#if defined(_WIN32)
  {
    static std::once_flag once;
    std::call_once(once, [timeout_ms] {
      aerogpu::logf("aerogpu-d3d9: wait_for_fence falling back to polling (timeout_ms=%u)\n",
                    static_cast<unsigned>(timeout_ms));
    });
  }
#endif
  while (monotonic_ms() < deadline) {
    if (refresh_fence_snapshot(adapter).last_completed >= fence_value) {
      return FenceWaitResult::Complete;
    }

    sleep_ms(1);
  }

  return (refresh_fence_snapshot(adapter).last_completed >= fence_value) ? FenceWaitResult::Complete
                                                                        : FenceWaitResult::NotReady;
}

HRESULT throttle_presents_locked(Device* dev, uint32_t d3d9_present_flags) {
  if (!dev) {
    return E_INVALIDARG;
  }
  if (!dev->adapter) {
    return E_FAIL;
  }

  // Clamp in case callers pass unexpected values.
  if (dev->max_frame_latency < kMaxFrameLatencyMin) {
    dev->max_frame_latency = kMaxFrameLatencyMin;
  }
  if (dev->max_frame_latency > kMaxFrameLatencyMax) {
    dev->max_frame_latency = kMaxFrameLatencyMax;
  }

  retire_completed_presents_locked(dev);

  if (dev->inflight_present_fences.size() < dev->max_frame_latency) {
    return S_OK;
  }

  const bool dont_wait = (d3d9_present_flags & kD3dPresentDoNotWait) != 0;
  if (dont_wait) {
    return kD3dErrWasStillDrawing;
  }

  // Wait for at least one present fence to retire, but never indefinitely.
  const uint64_t deadline = monotonic_ms() + kPresentThrottleMaxWaitMs;
  while (dev->inflight_present_fences.size() >= dev->max_frame_latency) {
    const uint64_t now = monotonic_ms();
    if (now >= deadline) {
      // Forward progress failed; drop the oldest fence to ensure PresentEx
      // returns quickly. This preserves overall system responsiveness at the
      // expense of perfect throttling accuracy under GPU hangs.
      dev->inflight_present_fences.pop_front();
      break;
    }

    const uint64_t oldest = dev->inflight_present_fences.front();
    const uint32_t time_left = static_cast<uint32_t>(std::min<uint64_t>(deadline - now, kPresentThrottleMaxWaitMs));
    (void)wait_for_fence(dev, oldest, time_left);
    retire_completed_presents_locked(dev);
  }

  return S_OK;
}

// D3DFMT_X1R5G5B5 uses an "X" bit where D3DFMT_A1R5G5B5 uses alpha. D3D9 treats
// the alpha component as 1.0 when sampling X1 formats. Since the AeroGPU
// protocol currently exposes only B5G5R5A1, we fix up CPU writes by forcing the
// high bit of each 16-bit texel to 1.
static void force_x1r5g5b5_alpha1_locked_range(
    void* locked_ptr,
    uint32_t locked_offset_bytes,
    uint32_t locked_size_bytes,
    uint32_t write_offset_bytes,
    uint32_t write_size_bytes) {
  if (!locked_ptr || locked_size_bytes == 0 || write_size_bytes == 0) {
    return;
  }

  const uint64_t lock_start = static_cast<uint64_t>(locked_offset_bytes);
  const uint64_t lock_end = lock_start + static_cast<uint64_t>(locked_size_bytes);
  const uint64_t write_start = static_cast<uint64_t>(write_offset_bytes);
  const uint64_t write_end = write_start + static_cast<uint64_t>(write_size_bytes);

  const uint64_t start = std::max(lock_start, write_start);
  const uint64_t end = std::min(lock_end, write_end);
  if (end <= start) {
    return;
  }

  // Expand to 16-bit word boundaries so we also cover misaligned writes that
  // touch only the low byte of a texel. Alpha must be forced to 1 regardless of
  // how the app updated the backing bytes.
  const uint64_t word_start = start & ~1ull;
  const uint64_t word_end = (end + 1ull) & ~1ull; // align up to even

  auto* bytes = static_cast<uint8_t*>(locked_ptr);
  for (uint64_t w = word_start; w < word_end; w += 2ull) {
    const uint64_t alpha_byte_offset = w + 1ull;
    if (alpha_byte_offset < lock_start || alpha_byte_offset >= lock_end) {
      continue;
    }
    const uint64_t rel = alpha_byte_offset - lock_start;
    if (rel > 0xFFFFFFFFull) {
      continue;
    }
    bytes[static_cast<uint32_t>(rel)] |= 0x80u;
  }
}

static bool SupportsBcFormats(const Device* dev) {
  if (!dev || !dev->adapter) {
    return false;
  }

#if defined(_WIN32)
  // On Windows we can usually query the active device ABI version via the
  // UMDRIVERPRIVATE blob. Be conservative: if we cannot query it, assume BC
  // formats are unsupported so we don't emit commands the host cannot parse.
  if (!dev->adapter->umd_private_valid) {
    return false;
  }
  const aerogpu_umd_private_v1& blob = dev->adapter->umd_private;
  const uint32_t major = blob.device_abi_version_u32 >> 16;
  const uint32_t minor = blob.device_abi_version_u32 & 0xFFFFu;
  return (major == AEROGPU_ABI_MAJOR) && (minor >= 2u);
#else
  // Portable builds don't have a real device to query; assume the matching host
  // supports the formats compiled into the protocol headers.
  (void)dev;
  return true;
#endif
}

static bool is_supported_backbuffer_format(D3DDDIFORMAT fmt) {
  // Keep swapchain backbuffer formats conservative: only advertise/accept common
  // 32-bit formats plus packed 16-bit RGB formats that are supported end-to-end
  // by the AeroGPU protocol/host.
  switch (static_cast<uint32_t>(fmt)) {
    case 21u: // D3DFMT_A8R8G8B8
    case 22u: // D3DFMT_X8R8G8B8
    case 23u: // D3DFMT_R5G6B5
    case 24u: // D3DFMT_X1R5G5B5
    case 25u: // D3DFMT_A1R5G5B5
    case 32u: // D3DFMT_A8B8G8R8
      return true;
    default:
      return false;
  }
}

static bool is_supported_depth_stencil_format(D3DDDIFORMAT fmt) {
  return static_cast<uint32_t>(fmt) == 75u; // D3DFMT_D24S8
}

// D3DLOCK_* flags (numeric values from d3d9.h). Only the bits we care about are
// defined here to keep the UMD self-contained.
constexpr uint32_t kD3DLOCK_READONLY = 0x00000010u;
constexpr uint32_t kD3DLOCK_DISCARD = 0x00002000u;
constexpr uint32_t kD3DLOCK_NOOVERWRITE = 0x00001000u;

// D3DPOOL_* (numeric values from d3d9.h).
constexpr uint32_t kD3DPOOL_DEFAULT = 0u;
constexpr uint32_t kD3DPOOL_SYSTEMMEM = 2u;

constexpr uint32_t kD3d9ShaderStageVs = 0u;
constexpr uint32_t kD3d9ShaderStagePs = 1u;

constexpr D3DDDIFORMAT kD3dFmtIndex16 = static_cast<D3DDDIFORMAT>(101); // D3DFMT_INDEX16
constexpr D3DDDIFORMAT kD3dFmtIndex32 = static_cast<D3DDDIFORMAT>(102); // D3DFMT_INDEX32

// SetStreamSourceFreq encodings (from d3d9types.h).
constexpr uint32_t kD3DStreamSourceMask = 0xC0000000u;
constexpr uint32_t kD3DStreamSourceIndexedData = 0x40000000u;
constexpr uint32_t kD3DStreamSourceInstanceData = 0x80000000u;

uint32_t d3d9_stage_to_aerogpu_stage(uint32_t stage) {
  return (stage == kD3d9ShaderStageVs) ? AEROGPU_SHADER_STAGE_VERTEX : AEROGPU_SHADER_STAGE_PIXEL;
}

uint32_t d3d9_index_format_to_aerogpu(D3DDDIFORMAT fmt) {
  return (fmt == kD3dFmtIndex32) ? AEROGPU_INDEX_FORMAT_UINT32 : AEROGPU_INDEX_FORMAT_UINT16;
}

// D3DUSAGE_* subset (numeric values from d3d9types.h).
constexpr uint32_t kD3DUsageRenderTarget = 0x00000001u;
constexpr uint32_t kD3DUsageDepthStencil = 0x00000002u;

uint32_t d3d9_usage_to_aerogpu_usage_flags(uint32_t usage) {
  uint32_t flags = AEROGPU_RESOURCE_USAGE_TEXTURE;
  if (usage & kD3DUsageRenderTarget) {
    flags |= AEROGPU_RESOURCE_USAGE_RENDER_TARGET;
  }
  if (usage & kD3DUsageDepthStencil) {
    flags |= AEROGPU_RESOURCE_USAGE_DEPTH_STENCIL;
  }
  return flags;
}

uint32_t d3d9_prim_to_topology(D3DDDIPRIMITIVETYPE prim) {
  switch (prim) {
    case D3DDDIPT_POINTLIST:
      return AEROGPU_TOPOLOGY_POINTLIST;
    case D3DDDIPT_LINELIST:
      return AEROGPU_TOPOLOGY_LINELIST;
    case D3DDDIPT_LINESTRIP:
      return AEROGPU_TOPOLOGY_LINESTRIP;
    case D3DDDIPT_TRIANGLESTRIP:
      return AEROGPU_TOPOLOGY_TRIANGLESTRIP;
    case D3DDDIPT_TRIANGLEFAN:
      return AEROGPU_TOPOLOGY_TRIANGLEFAN;
    case D3DDDIPT_TRIANGLELIST:
    default:
      return AEROGPU_TOPOLOGY_TRIANGLELIST;
  }
}

uint32_t vertex_count_from_primitive(D3DDDIPRIMITIVETYPE prim, uint32_t primitive_count) {
  switch (prim) {
    case D3DDDIPT_POINTLIST:
      return primitive_count;
    case D3DDDIPT_LINELIST:
      return primitive_count * 2;
    case D3DDDIPT_LINESTRIP:
      return primitive_count + 1;
    case D3DDDIPT_TRIANGLELIST:
      return primitive_count * 3;
    case D3DDDIPT_TRIANGLESTRIP:
    case D3DDDIPT_TRIANGLEFAN:
      return primitive_count + 2;
    default:
      return primitive_count * 3;
  }
}

uint32_t index_count_from_primitive(D3DDDIPRIMITIVETYPE prim, uint32_t primitive_count) {
  // Indexed draws follow the same primitive->index expansion rules.
  return vertex_count_from_primitive(prim, primitive_count);
}

bool clamp_rect(const RECT* in, uint32_t width, uint32_t height, RECT* out) {
  if (!out || width == 0 || height == 0) {
    return false;
  }

  RECT r{};
  if (in) {
    r = *in;
  } else {
    r.left = 0;
    r.top = 0;
    r.right = static_cast<long>(width);
    r.bottom = static_cast<long>(height);
  }

  const long max_x = static_cast<long>(width);
  const long max_y = static_cast<long>(height);

  r.left = std::clamp(r.left, 0l, max_x);
  r.right = std::clamp(r.right, 0l, max_x);
  r.top = std::clamp(r.top, 0l, max_y);
  r.bottom = std::clamp(r.bottom, 0l, max_y);

  if (r.right <= r.left || r.bottom <= r.top) {
    return false;
  }

  *out = r;
  return true;
}

// -----------------------------------------------------------------------------
// Fixed-function vertex formats (FVF) support (bring-up)
// -----------------------------------------------------------------------------
// The portable build keeps local numeric definitions + a compat vertex-element
// struct in `aerogpu_d3d9_objects.h` so we can parse synthesized declarations
// without requiring the Windows SDK/WDK headers.

// Some helper predicates are still expressed in terms of "FVF supported" (as
// consumed by legacy bring-up paths), but their implementation is driven by the
// central FixedFuncVariant table (see `aerogpu_d3d9_objects.h`).

// TEXCOORDSIZE bits are encoded per texcoord set, but only the bits for the
// declared texcoord count are semantically meaningful. Some runtimes appear to
// leave garbage TEXCOORDSIZE bits set for *unused* texcoord sets; clear them so
// internal caches can key off the true vertex layout.
constexpr uint32_t fvf_layout_key(uint32_t fvf) {
  const uint32_t tex_count = (fvf & kD3dFvfTexCountMask) >> kD3dFvfTexCountShift;
  const uint32_t clamped = (tex_count > 8u) ? 8u : tex_count;
  const uint32_t used_size_mask =
      (clamped == 0) ? 0u : (((1u << (clamped * 2u)) - 1u) << 16u);
  return (fvf & ~kD3dFvfTexCoordSizeMask) | (fvf & used_size_mask);
}

inline bool fixedfunc_fvf_supported(uint32_t fvf) {
  // Fixed-function bring-up paths which require a known internal FVF-driven
  // vertex declaration (e.g. patch emulation) are limited to pre-transformed
  // XYZRHW + DIFFUSE variants (optionally TEX1).
  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(fvf);
  return variant == FixedFuncVariant::RHW_COLOR ||
         variant == FixedFuncVariant::RHW_COLOR_TEX1;
}

inline constexpr bool fixedfunc_supported_fvf(uint32_t fvf) {
  // Keep this predicate in sync with fixedfunc_variant_from_fvf(): fixed-function
  // draws are supported iff the active FVF maps to a known FixedFuncVariant.
  //
  // TEXCOORDSIZE bits affect vertex layout (stride/offsets), but do not change
  // which fixed-function shader variant is needed.
  return fixedfunc_variant_from_fvf(fvf) != FixedFuncVariant::NONE;
}

inline constexpr bool fixedfunc_fvf_is_xyzrhw(uint32_t fvf) {
  return fixedfunc_variant_uses_rhw(fixedfunc_variant_from_fvf(fvf));
}

inline constexpr bool fixedfunc_fvf_has_normal(uint32_t fvf) {
  // For the fixed-function bring-up subset, `D3DFVF_NORMAL` cleanly identifies
  // whether the active FVF includes normals. (Unsupported FVFs may still carry
  // this bit; that is OK since lighting is only applied when we have a matching
  // fixed-function pipeline variant.)
  return (fvf & kD3dFvfNormal) != 0;
}

constexpr bool fixedfunc_fvf_needs_matrix(uint32_t fvf) {
  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(fvf);
  // XYZ fixed-function vertices use internal VS variants that apply the cached
  // WORLD/VIEW/PROJECTION matrix via a reserved high VS constant range
  // (c240..c243). Pre-transformed (XYZRHW) variants do not.
  return (variant != FixedFuncVariant::NONE) && !fixedfunc_variant_uses_rhw(variant);
}
// Fixed-function fallback shader constant register ranges.
//
// Note: D3D9 transform matrices are stored in row-major order (D3DMATRIX) and
// fixed-function math uses row-vectors (v * WVP). The fixed-function vertex shader
// uses `dp4 oPos.{x,y,z,w}, v0, c{240,241,242,243}` which computes dot(v, cN), so
// the driver uploads the *columns* of the row-major WVP matrix into a reserved
// high constant register range (i.e. the transpose of the row-major matrix).
//
// Using a high range avoids colliding with app-provided VS constants when apps
// switch between fixed-function and programmable paths.
constexpr uint32_t kFixedfuncMatrixStartRegister = 240u;
constexpr uint32_t kFixedfuncMatrixVec4Count = 4u;

// Fixed-function lighting constant register block.
//
// Fixed-function D3D9 lighting is computed in view space. The lit fixed-function
// VS variants therefore need:
// - world*view columns for transforming normals and positions into view space,
// - a bounded set of enabled lights (directional + point), and
// - material + global ambient terms.
//
// Layout (float4 registers; uploaded only when the lit VS variant is active):
//
//   c208..c210: columns 0..2 of the row-major world*view matrix.
//               - xyz: 3x3 basis for normal transform
//               - w: translation (m41/m42/m43) so dp4(v, cN) yields view-space
//                 position components (v.w is 1 for XYZ FVFs)
//
//   Directional lights (up to 4; packed from the enabled light set):
//     c211..c222: 4 * {dir_view, diffuse, ambient}
//       - dir_view.xyz is the vector from vertex -> light in view space (w=0)
//
//   Point/spot lights (up to 2; packed from the enabled light set):
//     c223..c232: 2 * {pos_view, diffuse, ambient, inv_att0, inv_range2}
//       - pos_view.xyz is the light position in view space (w=1)
//       - inv_att0 is 1 / max(att0, eps) replicated across xyzw
//       - inv_range2 is 1 / (range^2) replicated across xyzw (0 => no range clamp)
//
//   Material + global ambient:
//     c233: material diffuse (RGBA)
//     c234: material ambient (RGBA)
//     c235: material emissive (RGBA)
//     c236: global ambient from D3DRS_AMBIENT (RGBA)
//
// This layout intentionally stays in a high constant register range so switching
// between fixed-function and user shaders is less likely to trample app-provided
// constants. Lighting uploads are additionally gated so they are only performed
// for the full fixed-function fallback pipeline (no user VS/PS bound).
constexpr uint32_t kFixedfuncLightingStartRegister = 208u;
constexpr uint32_t kFixedfuncLightingVec4Count = 29u;
constexpr uint32_t kFixedfuncMaxDirectionalLights = 4u;
constexpr uint32_t kFixedfuncMaxPointLights = 2u;

// D3DTRANSFORMSTATETYPE numeric values (d3d9types.h). Use local constants so we
// can key off them even when building without the Windows SDK/WDK.
constexpr uint32_t kD3dTransformView = 2u;
constexpr uint32_t kD3dTransformProjection = 3u;
constexpr uint32_t kD3dTransformWorld0 = 256u;
uint32_t fvf_decode_texcoord_size(uint32_t fvf, uint32_t tex_index);

uint32_t fixedfunc_min_stride_bytes(uint32_t fvf) {
  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(fvf);
  if (variant == FixedFuncVariant::NONE) {
    return 0u;
  }
  const uint32_t base_fvf = fixedfunc_fvf_from_variant(variant);
  if (base_fvf == 0) {
    return 0u;
  }

  // Positions:
  // - XYZRHW: float4 (x, y, z, rhw) => 16 bytes
  // - XYZ: float3 (x, y, z) => 12 bytes
  uint32_t stride = (base_fvf & kD3dFvfXyzRhw) ? 16u : 12u;

  // NORMAL => float3 => 12 bytes.
  if ((base_fvf & kD3dFvfNormal) != 0) {
    stride += 12u;
  }
  // DIFFUSE => D3DCOLOR => 4 bytes.
  if ((base_fvf & kD3dFvfDiffuse) != 0) {
    stride += 4u;
  }
  // TEX1 => TEXCOORD0 => float{1,2,3,4} => dim * 4 bytes.
  if ((base_fvf & kD3dFvfTex1) != 0) {
    const uint32_t dim = fvf_decode_texcoord_size(fvf, 0);
    if (dim < 1u || dim > 4u) {
      return 0u;
    }
    stride += dim * 4u;
  }

  return stride;
}

HRESULT validate_fixedfunc_vertex_stride(uint32_t fvf, uint32_t stride_bytes) {
  const uint32_t min_stride = fixedfunc_min_stride_bytes(fvf);
  if (min_stride == 0) {
    return S_OK;
  }
  if (stride_bytes < min_stride) {
    return kD3DErrInvalidCall;
  }
  return S_OK;
}

// Row-major 4x4 matrix multiply: out = a * b.
//
// D3D9 fixed-function transforms use row vectors (v * M), so the conventional
// left-to-right matrix multiply matches how WORLD/VIEW/PROJECTION compose.
static void d3d9_mul_mat4_row_major(const float a[16], const float b[16], float out[16]) {
  if (!a || !b || !out) {
    return;
  }
  for (uint32_t r = 0; r < 4; ++r) {
    const float a0 = a[r * 4 + 0];
    const float a1 = a[r * 4 + 1];
    const float a2 = a[r * 4 + 2];
    const float a3 = a[r * 4 + 3];

    out[r * 4 + 0] = a0 * b[0 * 4 + 0] + a1 * b[1 * 4 + 0] + a2 * b[2 * 4 + 0] + a3 * b[3 * 4 + 0];
    out[r * 4 + 1] = a0 * b[0 * 4 + 1] + a1 * b[1 * 4 + 1] + a2 * b[2 * 4 + 1] + a3 * b[3 * 4 + 1];
    out[r * 4 + 2] = a0 * b[0 * 4 + 2] + a1 * b[1 * 4 + 2] + a2 * b[2 * 4 + 2] + a3 * b[3 * 4 + 2];
    out[r * 4 + 3] = a0 * b[0 * 4 + 3] + a1 * b[1 * 4 + 3] + a2 * b[2 * 4 + 3] + a3 * b[3 * 4 + 3];
  }
}

// -----------------------------------------------------------------------------
// FVF -> internal vertex declaration translation (programmable pipeline)
// -----------------------------------------------------------------------------
//
// Some D3D9 apps use `SetFVF` even when binding user shaders. In that case, the
// runtime expects the driver to derive a vertex declaration / input layout from
// the FVF. AeroGPU's fixed-function bring-up path only handled a small hardcoded
// subset; translate a broader common subset so user-shader apps see correct input
// layouts.
//
// Supported subset (layout only; does not imply fixed-function emulation):
// - POSITION: D3DFVF_XYZ, D3DFVF_XYZW, D3DFVF_XYZRHW, or D3DFVF_XYZB1..D3DFVF_XYZB5
// - BLENDWEIGHT: XYZB* (BLENDWEIGHT0) with optional LASTBETA_* (BLENDINDICES0)
// - NORMAL: D3DFVF_NORMAL
// - DIFFUSE: D3DFVF_DIFFUSE (COLOR0)
// - SPECULAR: D3DFVF_SPECULAR (COLOR1)
// - TEXn: D3DFVF_TEX0..D3DFVF_TEX8 with per-set D3DFVF_TEXCOORDSIZE[1..4]
//
// Unsupported bits return false so callers
// can fall back to "cache-only" SetFVF behavior (GetFVF/state-block compatibility).
struct FvfVertexDeclTranslation {
  std::array<D3DVERTEXELEMENT9_COMPAT, 16> elems{};
  uint32_t elem_count = 0;
  uint32_t stride_bytes = 0;
};

constexpr uint32_t kMaxFvfTexCoordSets = 8u;

uint32_t fvf_decode_texcoord_size(uint32_t fvf, uint32_t tex_index) {
  if (tex_index >= kMaxFvfTexCoordSets) {
    return 0;
  }
  // D3DFVF_TEXCOORDSIZE* uses two bits per texcoord set, with the same encoding
  // as D3DFVF_TEXTUREFORMAT[1..4]:
  //   0 -> float2 (default)
  //   1 -> float3
  //   2 -> float4
  //   3 -> float1
  const uint32_t shift = 16u + tex_index * 2u;
  const uint32_t code = (fvf >> shift) & 0x3u;
  switch (code) {
    case 0u:
      return 2u;
    case 1u:
      return 3u;
    case 2u:
      return 4u;
    case 3u:
      return 1u;
    default:
      return 2u;
  }
}

uint8_t fvf_texcoord_size_to_decl_type(uint32_t size) {
  switch (size) {
    case 1u:
      return kD3dDeclTypeFloat1;
    case 2u:
      return kD3dDeclTypeFloat2;
    case 3u:
      return kD3dDeclTypeFloat3;
    case 4u:
      return kD3dDeclTypeFloat4;
    default:
      return kD3dDeclTypeFloat2;
  }
}

bool try_translate_fvf_to_vertex_decl(uint32_t fvf, FvfVertexDeclTranslation* out) {
  if (!out) {
    return false;
  }
  *out = FvfVertexDeclTranslation{};

  if (fvf == 0) {
    return false;
  }

  // Reject FVFs that include bits we don't understand. This keeps behavior
  // deterministic and avoids binding mismatched layouts for complicated legacy
  // formats.
  constexpr uint32_t kSupportedMask =
      kD3dFvfPositionMask |
      kD3dFvfLastBetaUByte4 |
      kD3dFvfLastBetaD3dColor |
      kD3dFvfNormal |
      kD3dFvfPSize |
      kD3dFvfDiffuse |
      kD3dFvfSpecular |
      kD3dFvfTexCountMask |
      kD3dFvfTexCoordSizeMask;
  if ((fvf & ~kSupportedMask) != 0) {
    return false;
  }

  // POSITION
  const uint32_t pos = fvf & kD3dFvfPositionMask;
  const bool pos_xyz = (pos == kD3dFvfXyz);
  const bool pos_xyzrhw = (pos == kD3dFvfXyzRhw);
  const bool pos_xyzw = (pos == kD3dFvfXyzw);
  uint32_t blend_weights = 0;
  switch (pos) {
    case kD3dFvfXyzB1:
      blend_weights = 1;
      break;
    case kD3dFvfXyzB2:
      blend_weights = 2;
      break;
    case kD3dFvfXyzB3:
      blend_weights = 3;
      break;
    case kD3dFvfXyzB4:
      blend_weights = 4;
      break;
    case kD3dFvfXyzB5:
      blend_weights = 5;
      break;
    default:
      break;
  }
  if (!pos_xyz && !pos_xyzrhw && !pos_xyzw && blend_weights == 0) {
    return false;
  }
  const bool has_lastbeta_ubyte4 = (fvf & kD3dFvfLastBetaUByte4) != 0;
  const bool has_lastbeta_color = (fvf & kD3dFvfLastBetaD3dColor) != 0;
  if (has_lastbeta_ubyte4 && has_lastbeta_color) {
    return false;
  }
  if ((has_lastbeta_ubyte4 || has_lastbeta_color) && blend_weights == 0) {
    // LASTBETA flags only make sense for XYZB* encodings.
    return false;
  }

  // TEXn count.
  const uint32_t tex_count = (fvf & kD3dFvfTexCountMask) >> kD3dFvfTexCountShift;
  if (tex_count > kMaxFvfTexCoordSets) {
    return false;
  }

  auto add = [&](uint32_t stream,
                 uint32_t offset_bytes,
                 uint8_t type,
                 uint8_t usage,
                 uint8_t usage_index) -> bool {
    if (out->elem_count >= out->elems.size()) {
      return false;
    }
    if (stream > 0xFFFFu || offset_bytes > 0xFFFFu) {
      return false;
    }
    out->elems[out->elem_count++] = D3DVERTEXELEMENT9_COMPAT{
        static_cast<uint16_t>(stream),
        static_cast<uint16_t>(offset_bytes),
        type,
        kD3dDeclMethodDefault,
        usage,
        usage_index,
    };
    return true;
  };

  uint32_t offset = 0;
  if (pos_xyz) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat3, kD3dDeclUsagePosition, 0)) {
      return false;
    }
    offset += 12;
  } else if (blend_weights != 0) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat3, kD3dDeclUsagePosition, 0)) {
      return false;
    }
    offset += 12;

    uint8_t weight_type = kD3dDeclTypeFloat1;
    uint32_t weight_bytes = 0;
    switch (blend_weights) {
      case 1:
        weight_type = kD3dDeclTypeFloat1;
        weight_bytes = 4;
        break;
      case 2:
        weight_type = kD3dDeclTypeFloat2;
        weight_bytes = 8;
        break;
      case 3:
        weight_type = kD3dDeclTypeFloat3;
        weight_bytes = 12;
        break;
      case 4:
      case 5:
        // D3D9 encodes 5-matrix blending using 4 explicit weights; the last
        // weight is implied. Map both XYZB4 and XYZB5 to FLOAT4.
        weight_type = kD3dDeclTypeFloat4;
        weight_bytes = 16;
        break;
      default:
        return false;
    }
    if (!add(/*stream=*/0, offset, weight_type, kD3dDeclUsageBlendWeight, 0)) {
      return false;
    }
    offset += weight_bytes;

    if (has_lastbeta_ubyte4) {
      if (!add(/*stream=*/0, offset, kD3dDeclTypeUByte4, kD3dDeclUsageBlendIndices, 0)) {
        return false;
      }
      offset += 4;
    } else if (has_lastbeta_color) {
      if (!add(/*stream=*/0, offset, kD3dDeclTypeD3dColor, kD3dDeclUsageBlendIndices, 0)) {
        return false;
      }
      offset += 4;
    }
  } else if (pos_xyzw) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat4, kD3dDeclUsagePosition, 0)) {
      return false;
    }
    offset += 16;
  } else {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat4, kD3dDeclUsagePositionT, 0)) {
      return false;
    }
    offset += 16;
  }

  if (fvf & kD3dFvfNormal) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat3, kD3dDeclUsageNormal, 0)) {
      return false;
    }
    offset += 12;
  }
  if (fvf & kD3dFvfPSize) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeFloat1, kD3dDeclUsagePSize, 0)) {
      return false;
    }
    offset += 4;
  }
  if (fvf & kD3dFvfDiffuse) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeD3dColor, kD3dDeclUsageColor, 0)) {
      return false;
    }
    offset += 4;
  }
  if (fvf & kD3dFvfSpecular) {
    if (!add(/*stream=*/0, offset, kD3dDeclTypeD3dColor, kD3dDeclUsageColor, 1)) {
      return false;
    }
    offset += 4;
  }

  for (uint32_t i = 0; i < tex_count; ++i) {
    const uint32_t dim = fvf_decode_texcoord_size(fvf, i);
    if (dim < 1u || dim > 4u) {
      return false;
    }
    const uint8_t type = fvf_texcoord_size_to_decl_type(dim);
    if (!add(/*stream=*/0, offset, type, kD3dDeclUsageTexCoord, static_cast<uint8_t>(i))) {
      return false;
    }
    offset += dim * 4u;
  }

  // D3DDECL_END terminator.
  if (out->elem_count >= out->elems.size()) {
    return false;
  }
  out->elems[out->elem_count++] = D3DVERTEXELEMENT9_COMPAT{0xFF, 0, kD3dDeclTypeUnused, 0, 0, 0};

  out->stride_bytes = offset;
  return true;
}

// -----------------------------------------------------------------------------
// Handle helpers
// -----------------------------------------------------------------------------

Adapter* as_adapter(D3DDDI_HADAPTER hAdapter) {
  return reinterpret_cast<Adapter*>(hAdapter.pDrvPrivate);
}

Device* as_device(D3DDDI_HDEVICE hDevice) {
  return reinterpret_cast<Device*>(hDevice.pDrvPrivate);
}

Resource* as_resource(D3DDDI_HRESOURCE hRes) {
  return reinterpret_cast<Resource*>(hRes.pDrvPrivate);
}

SwapChain* as_swapchain(D3D9DDI_HSWAPCHAIN hSwapChain) {
  return reinterpret_cast<SwapChain*>(hSwapChain.pDrvPrivate);
}

Shader* as_shader(D3D9DDI_HSHADER hShader) {
  return reinterpret_cast<Shader*>(hShader.pDrvPrivate);
}

VertexDecl* as_vertex_decl(D3D9DDI_HVERTEXDECL hDecl) {
  return reinterpret_cast<VertexDecl*>(hDecl.pDrvPrivate);
}

Query* as_query(D3D9DDI_HQUERY hQuery) {
  return reinterpret_cast<Query*>(hQuery.pDrvPrivate);
}

StateBlock* as_state_block(D3D9DDI_HSTATEBLOCK hStateBlock) {
  return reinterpret_cast<StateBlock*>(hStateBlock.pDrvPrivate);
}

// -----------------------------------------------------------------------------
// State-block recording helpers
// -----------------------------------------------------------------------------
// Callers must hold `Device::mutex`.
inline void stateblock_record_render_state_locked(Device* dev, uint32_t state, uint32_t value) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (state >= 256) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->render_state_mask.set(state);
  sb->render_state_values[state] = value;
}

inline void stateblock_record_sampler_state_locked(Device* dev, uint32_t stage, uint32_t state, uint32_t value) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (stage >= 16 || state >= 16) {
    return;
  }
  const uint32_t idx = stage * 16u + state;
  StateBlock* sb = dev->recording_state_block;
  sb->sampler_state_mask.set(idx);
  sb->sampler_state_values[idx] = value;
}

inline void stateblock_record_texture_stage_state_locked(Device* dev, uint32_t stage, uint32_t state, uint32_t value) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (stage >= 16 || state >= 256) {
    return;
  }
  const uint32_t idx = stage * 256u + state;
  StateBlock* sb = dev->recording_state_block;
  sb->texture_stage_state_mask.set(idx);
  sb->texture_stage_state_values[idx] = value;
}

inline void stateblock_record_transform_locked(Device* dev, uint32_t state, const float* matrix16) {
  if (!dev || !dev->recording_state_block || !matrix16) {
    return;
  }
  if (state >= Device::kTransformCacheCount) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->transform_mask.set(state);
  std::memcpy(sb->transform_values.data() + static_cast<size_t>(state) * 16u,
              matrix16,
              sizeof(float) * 16u);
}

inline void stateblock_record_clip_plane_locked(Device* dev, uint32_t index, const float* plane4) {
  if (!dev || !dev->recording_state_block || !plane4) {
    return;
  }
  if (index >= 6) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->clip_plane_mask.set(index);
  std::memcpy(sb->clip_plane_values.data() + static_cast<size_t>(index) * 4u,
              plane4,
              sizeof(float) * 4u);
}

inline void stateblock_record_software_vertex_processing_locked(Device* dev, BOOL enabled) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->software_vertex_processing_set = true;
  sb->software_vertex_processing = enabled;
}

inline void stateblock_record_npatch_mode_locked(Device* dev, float mode) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->n_patch_mode_set = true;
  sb->n_patch_mode = mode;
}

inline void stateblock_record_material_locked(Device* dev, const D3DMATERIAL9& material, bool valid) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->material_set = true;
  sb->material_valid = valid;
  sb->material = material;
}

inline void stateblock_record_light_locked(Device* dev, uint32_t index, const D3DLIGHT9& light, bool valid) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (index >= Device::kMaxLights) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->light_mask.set(index);
  sb->lights[index] = light;
  if (valid) {
    sb->light_valid_bits.set(index);
  } else {
    sb->light_valid_bits.reset(index);
  }
}

inline void stateblock_record_light_enable_locked(Device* dev, uint32_t index, BOOL enabled) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (index >= Device::kMaxLights) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->light_enable_mask.set(index);
  if (enabled) {
    sb->light_enable_bits.set(index);
  } else {
    sb->light_enable_bits.reset(index);
  }
}

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
inline void stateblock_record_gamma_ramp_locked(Device* dev, const D3DGAMMARAMP& ramp, bool valid) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->gamma_ramp_set = true;
  sb->gamma_ramp_valid = valid;
  sb->gamma_ramp = ramp;
}

inline void stateblock_record_clip_status_locked(Device* dev, const D3DCLIPSTATUS9& status, bool valid) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->clip_status_set = true;
  sb->clip_status_valid = valid;
  sb->clip_status = status;
}

inline void stateblock_record_palette_entries_locked(Device* dev, uint32_t palette, const PALETTEENTRY* entries, bool valid) {
  if (!dev || !dev->recording_state_block || !entries) {
    return;
  }
  if (palette >= Device::kMaxPalettes) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->palette_mask.set(palette);
  std::memcpy(sb->palette_entries[palette].data(), entries, sizeof(PALETTEENTRY) * 256u);
  if (valid) {
    sb->palette_valid_bits.set(palette);
  } else {
    sb->palette_valid_bits.reset(palette);
  }
}

inline void stateblock_record_current_texture_palette_locked(Device* dev, uint32_t palette) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (palette >= Device::kMaxPalettes) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->current_texture_palette_set = true;
  sb->current_texture_palette = palette;
}
#endif

inline void stateblock_record_texture_locked(Device* dev, uint32_t stage, Resource* tex) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (stage >= 16) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->texture_mask.set(stage);
  sb->textures[stage] = tex;
}

inline void stateblock_record_render_target_locked(Device* dev, uint32_t slot, Resource* rt) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (slot >= 4) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->render_target_mask.set(slot);
  sb->render_targets[slot] = rt;
}

inline void stateblock_record_depth_stencil_locked(Device* dev, Resource* ds) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->depth_stencil_set = true;
  sb->depth_stencil = ds;
}

inline void stateblock_record_viewport_locked(Device* dev, const D3DDDIVIEWPORTINFO& vp) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->viewport_set = true;
  sb->viewport = vp;
}

// Some D3D9 runtimes rely on the default viewport (full render target) without
// ever issuing a SetViewport call. D3D9 state blocks and GetViewport should
// still observe a non-empty viewport in that case. Compute an "effective"
// viewport that falls back to the current render target / swapchain / adapter
// dimensions when the cached viewport is unset (Width/Height <= 0).
//
// Callers must hold `Device::mutex`.
inline D3DDDIVIEWPORTINFO viewport_effective_locked(Device* dev) {
  D3DDDIVIEWPORTINFO vp = {0, 0, 0, 0, 0.0f, 1.0f};
  if (!dev) {
    return vp;
  }
  vp = dev->viewport;
  if (vp.Width > 0.0f && vp.Height > 0.0f) {
    return vp;
  }

  int32_t w = 0;
  int32_t h = 0;
  if (dev->render_targets[0]) {
    w = static_cast<int32_t>(dev->render_targets[0]->width);
    h = static_cast<int32_t>(dev->render_targets[0]->height);
  } else if (dev->current_swapchain) {
    w = static_cast<int32_t>(dev->current_swapchain->width);
    h = static_cast<int32_t>(dev->current_swapchain->height);
  } else if (dev->adapter) {
    w = static_cast<int32_t>(dev->adapter->primary_width);
    h = static_cast<int32_t>(dev->adapter->primary_height);
  }

  if (w > 0 && h > 0) {
    vp.X = 0.0f;
    vp.Y = 0.0f;
    vp.Width = static_cast<float>(w);
    vp.Height = static_cast<float>(h);
  }
  if (vp.Width <= 0.0f) {
    vp.Width = 1.0f;
  }
  if (vp.Height <= 0.0f) {
    vp.Height = 1.0f;
  }
  return vp;
}

// Effective scissor rect for GetScissorRect and state blocks:
// - If the app explicitly set a rect, preserve it (even if empty).
// - Otherwise, if the current rect is unset/empty, fall back to the effective
//   viewport rect so enabling scissor without SetScissorRect behaves like the
//   D3D9 default (no clipping).
//
// Callers must hold `Device::mutex`.
inline RECT scissor_rect_effective_locked(Device* dev) {
  RECT r = {0, 0, 0, 0};
  if (!dev) {
    return r;
  }
  r = dev->scissor_rect;
  if (dev->scissor_rect_user_set) {
    return r;
  }
  if (r.right > r.left && r.bottom > r.top) {
    return r;
  }
  const D3DDDIVIEWPORTINFO vp = viewport_effective_locked(dev);
  const int32_t x = static_cast<int32_t>(vp.X);
  const int32_t y = static_cast<int32_t>(vp.Y);
  const int32_t w = static_cast<int32_t>(vp.Width);
  const int32_t h = static_cast<int32_t>(vp.Height);
  if (w > 0 && h > 0) {
    r.left = static_cast<LONG>(x);
    r.top = static_cast<LONG>(y);
    r.right = static_cast<LONG>(x + w);
    r.bottom = static_cast<LONG>(y + h);
  }
  return r;
}

inline void stateblock_record_scissor_locked(Device* dev, const RECT& rect, BOOL enabled) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->scissor_set = true;
  sb->scissor_rect = rect;
  sb->scissor_rect_user_set = dev->scissor_rect_user_set;
  sb->scissor_enabled = enabled;
}

// Some D3D9 runtimes enable scissor testing before ever calling SetScissorRect.
// Leaving the default all-zero rect would then clip everything. When scissor is
// enabled and the app never explicitly set a rect, fall back to a reasonable
// default (viewport-sized, then render-target-sized).
//
// Callers must hold `Device::mutex`.
inline void scissor_fixup_unset_rect_locked(Device* dev) {
  if (!dev || !dev->scissor_enabled || dev->scissor_rect_user_set) {
    return;
  }
  RECT& r = dev->scissor_rect;
  if (r.right > r.left && r.bottom > r.top) {
    return;
  }

  int32_t x = 0;
  int32_t y = 0;
  int32_t w = 0;
  int32_t h = 0;

  // Prefer the current viewport.
  const int32_t vx = static_cast<int32_t>(dev->viewport.X);
  const int32_t vy = static_cast<int32_t>(dev->viewport.Y);
  const int32_t vw = static_cast<int32_t>(dev->viewport.Width);
  const int32_t vh = static_cast<int32_t>(dev->viewport.Height);
  if (vw > 0 && vh > 0) {
    x = vx;
    y = vy;
    w = vw;
    h = vh;
  } else if (dev->render_targets[0]) {
    // Fall back to the current render target dimensions.
    w = static_cast<int32_t>(dev->render_targets[0]->width);
    h = static_cast<int32_t>(dev->render_targets[0]->height);
  } else if (dev->current_swapchain) {
    // Last resort: swapchain size.
    w = static_cast<int32_t>(dev->current_swapchain->width);
    h = static_cast<int32_t>(dev->current_swapchain->height);
  } else if (dev->adapter) {
    // Best-effort adapter mode (may not match the target size).
    w = static_cast<int32_t>(dev->adapter->primary_width);
    h = static_cast<int32_t>(dev->adapter->primary_height);
  }

  if (w > 0 && h > 0) {
    r.left = static_cast<LONG>(x);
    r.top = static_cast<LONG>(y);
    r.right = static_cast<LONG>(x + w);
    r.bottom = static_cast<LONG>(y + h);
  }
}

inline void stateblock_record_stream_source_locked(Device* dev, uint32_t stream, const DeviceStateStream& ss) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (stream >= 16) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->stream_mask.set(stream);
  sb->streams[stream] = ss;
}

inline void stateblock_record_stream_source_freq_locked(Device* dev, uint32_t stream, uint32_t value) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  if (stream >= 16) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->stream_source_freq_mask.set(stream);
  sb->stream_source_freq_values[stream] = value;
}

inline void stateblock_record_index_buffer_locked(Device* dev, Resource* ib, D3DDDIFORMAT fmt, uint32_t offset_bytes) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->index_buffer_set = true;
  sb->index_buffer = ib;
  sb->index_format = fmt;
  sb->index_offset_bytes = offset_bytes;
}

inline void stateblock_record_vertex_decl_locked(Device* dev, VertexDecl* decl, uint32_t fvf) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  sb->vertex_decl_set = true;
  sb->vertex_decl = decl;
  sb->fvf_set = true;
  sb->fvf = fvf;
}

inline void stateblock_record_shader_locked(Device* dev, uint32_t stage, Shader* sh) {
  if (!dev || !dev->recording_state_block) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  // Be permissive: some D3D9 header/runtime combinations may not use the exact
  // {0,1} encoding at the DDI boundary. Match the main shader binding path
  // (`device_set_shader`), which treats any non-VS stage as PS.
  if (stage == kD3d9ShaderStageVs) {
    sb->user_vs_set = true;
    sb->user_vs = sh;
  } else {
    sb->user_ps_set = true;
    sb->user_ps = sh;
  }
}

inline void stateblock_record_shader_const_f_locked(
    Device* dev,
    uint32_t stage,
    uint32_t start_reg,
    const float* pData,
    uint32_t vec4_count) {
  if (!dev || !dev->recording_state_block || !pData || vec4_count == 0) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  std::bitset<256>* mask = nullptr;
  float* dst = nullptr;
  if (stage == kD3d9ShaderStageVs) {
    mask = &sb->vs_const_mask;
    dst = sb->vs_consts.data();
  } else {
    mask = &sb->ps_const_mask;
    dst = sb->ps_consts.data();
  }

  if (start_reg >= 256) {
    return;
  }
  const uint32_t write_regs = std::min(vec4_count, 256u - start_reg);
  for (uint32_t i = 0; i < write_regs; ++i) {
    mask->set(start_reg + i);
    std::memcpy(dst + static_cast<size_t>(start_reg + i) * 4,
                pData + static_cast<size_t>(i) * 4,
                4 * sizeof(float));
  }
}

inline void stateblock_record_shader_const_i_locked(
    Device* dev,
    uint32_t stage,
    uint32_t start_reg,
    const int32_t* pData,
    uint32_t vec4_count) {
  if (!dev || !dev->recording_state_block || !pData || vec4_count == 0) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  std::bitset<256>* mask = nullptr;
  int32_t* dst = nullptr;
  if (stage == kD3d9ShaderStageVs) {
    mask = &sb->vs_const_i_mask;
    dst = sb->vs_consts_i.data();
  } else {
    mask = &sb->ps_const_i_mask;
    dst = sb->ps_consts_i.data();
  }

  if (start_reg >= 256) {
    return;
  }
  const uint32_t write_regs = std::min(vec4_count, 256u - start_reg);
  for (uint32_t i = 0; i < write_regs; ++i) {
    mask->set(start_reg + i);
    std::memcpy(dst + static_cast<size_t>(start_reg + i) * 4,
                pData + static_cast<size_t>(i) * 4,
                4 * sizeof(int32_t));
  }
}

inline void stateblock_record_shader_const_b_locked(
    Device* dev,
    uint32_t stage,
    uint32_t start_reg,
    const uint8_t* pData,
    uint32_t bool_count) {
  if (!dev || !dev->recording_state_block || !pData || bool_count == 0) {
    return;
  }
  StateBlock* sb = dev->recording_state_block;
  std::bitset<256>* mask = nullptr;
  uint8_t* dst = nullptr;
  if (stage == kD3d9ShaderStageVs) {
    mask = &sb->vs_const_b_mask;
    dst = sb->vs_consts_b.data();
  } else {
    mask = &sb->ps_const_b_mask;
    dst = sb->ps_consts_b.data();
  }

  if (start_reg >= 256) {
    return;
  }
  const uint32_t write = std::min(bool_count, 256u - start_reg);
  for (uint32_t i = 0; i < write; ++i) {
    mask->set(start_reg + i);
    dst[start_reg + i] = pData[i] ? 1u : 0u;
  }
}

// Forward-declared so helpers can opportunistically split submissions when the
// runtime-provided DMA buffer / allocation list is full.
uint64_t submit(Device* dev, bool is_present = false);

#if defined(_WIN32)
// Ensures the device has a valid runtime-provided command buffer + allocation
// list bound for recording (CreateContext persistent buffers, or Allocate/
// GetCommandBuffer fallback).
// Callers must hold `Device::mutex`.
bool wddm_ensure_recording_buffers(Device* dev, size_t bytes_needed);
#endif

// -----------------------------------------------------------------------------
// WDDM allocation-list tracking helpers (Win7 / WDDM 1.1)
// -----------------------------------------------------------------------------

// -----------------------------------------------------------------------------
// Command emission helpers (protocol: drivers/aerogpu/protocol/aerogpu_cmd.h)
// -----------------------------------------------------------------------------
bool ensure_cmd_space(Device* dev, size_t bytes_needed) {
  if (!dev) {
    return false;
  }
  if (!dev->adapter) {
    return false;
  }
  if (device_is_lost(dev)) {
    return false;
  }

#if defined(_WIN32)
  if (dev->wddm_context.hContext != 0) {
    // In WDDM builds, never allow command emission to fall back to the
    // vector-backed writer: submissions must be built in runtime-provided DMA
    // buffers so allocation-list tracking and DMA-private-data handoff to the
    // KMD are correct.
    if (!wddm_ensure_recording_buffers(dev, bytes_needed)) {
      return false;
    }
  }
#endif

  if (dev->cmd.bytes_remaining() >= bytes_needed) {
    return true;
  }

  // If the current submission is non-empty, flush it and retry.
  if (!dev->cmd.empty()) {
    (void)submit(dev);
    if (device_is_lost(dev)) {
      return false;
    }
  }

#if defined(_WIN32)
  if (dev->wddm_context.hContext != 0) {
    if (!wddm_ensure_recording_buffers(dev, bytes_needed)) {
      return false;
    }
  }
#endif

  return dev->cmd.bytes_remaining() >= bytes_needed;
}

template <typename T>
T* append_fixed_locked(Device* dev, uint32_t opcode) {
  const size_t needed = align_up(sizeof(T), 4);
  if (!ensure_cmd_space(dev, needed)) {
    return nullptr;
  }
  return dev->cmd.TryAppendFixed<T>(opcode);
}

template <typename HeaderT>
HeaderT* append_with_payload_locked(Device* dev, uint32_t opcode, const void* payload, size_t payload_size) {
  const size_t needed = align_up(sizeof(HeaderT) + payload_size, 4);
  if (!ensure_cmd_space(dev, needed)) {
    return nullptr;
  }
  return dev->cmd.TryAppendWithPayload<HeaderT>(opcode, payload, payload_size);
}

HRESULT track_resource_allocation_locked(Device* dev, Resource* res, bool write) {
  if (!dev || !res) {
    return E_INVALIDARG;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }

  // Only track allocations when running on the WDDM path. Repo/compat builds
  // don't have WDDM allocation handles or runtime-provided allocation lists.
  if (dev->wddm_context.hContext == 0) {
    return S_OK;
  }

#if defined(_WIN32)
  // Ensure the allocation list backing store is available before we attempt to
  // write D3DDDI_ALLOCATIONLIST entries.
  const size_t min_packet = align_up(sizeof(aerogpu_cmd_hdr), 4);
  if (!wddm_ensure_recording_buffers(dev, min_packet)) {
    return E_FAIL;
  }
#endif

  // Allocation tracking requires a bound allocation-list buffer. In portable
  // builds/tests we may toggle `hContext` without wiring a list; treat that as
  // "tracking disabled" so unit tests focused on other behavior keep working.
  if (!dev->alloc_list_tracker.list_base() || dev->alloc_list_tracker.list_capacity_effective() == 0) {
#if defined(_WIN32)
    return E_FAIL;
#else
    return S_OK;
#endif
  }

  if (res->backing_alloc_id == 0) {
    // backing_alloc_id==0 denotes a host-allocated resource (no guest allocation
    // table entry required).
    return S_OK;
  }

  if (res->wddm_hAllocation == 0) {
    logf("aerogpu-d3d9: missing WDDM hAllocation for resource handle=%u alloc_id=%u\n",
         res->handle,
         res->backing_alloc_id);
    return E_FAIL;
  }

  AllocRef ref{};
  if (write) {
    ref = dev->alloc_list_tracker.track_render_target_write(
        res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
  } else if (res->kind == ResourceKind::Buffer) {
    ref = dev->alloc_list_tracker.track_buffer_read(
        res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
  } else {
    ref = dev->alloc_list_tracker.track_texture_read(
        res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
  }

  if (ref.status == AllocRefStatus::kNeedFlush) {
    // Split the submission and retry.
    (void)submit(dev);
    if (device_is_lost(dev)) {
      return device_lost_hresult(dev);
    }

#if defined(_WIN32)
    // AllocateCb/DeallocateCb runtimes may deallocate the active allocation list
    // on every submission. Reacquire/rebind the recording buffers before retrying
    // so the allocation tracker has a valid list to write into.
    const size_t min_packet = align_up(sizeof(aerogpu_cmd_hdr), 4);
    if (!wddm_ensure_recording_buffers(dev, min_packet)) {
      return E_FAIL;
    }
#endif

    // Allocation tracking requires a bound allocation-list buffer. In portable
    // builds/tests we may toggle `hContext` without wiring a list; treat that as
    // "tracking disabled" so unit tests focused on other behavior keep working.
    if (!dev->alloc_list_tracker.list_base() || dev->alloc_list_tracker.list_capacity_effective() == 0) {
#if defined(_WIN32)
      return E_FAIL;
#else
      return S_OK;
#endif
    }

    if (write) {
      ref = dev->alloc_list_tracker.track_render_target_write(
          res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
    } else if (res->kind == ResourceKind::Buffer) {
      ref = dev->alloc_list_tracker.track_buffer_read(
          res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
    } else {
      ref = dev->alloc_list_tracker.track_texture_read(
          res->wddm_hAllocation, res->backing_alloc_id, res->share_token);
    }
  }

  if (ref.status != AllocRefStatus::kOk) {
    logf("aerogpu-d3d9: failed to track allocation (handle=%u alloc_id=%u status=%u)\n",
         res->handle,
         res->backing_alloc_id,
         static_cast<uint32_t>(ref.status));
    if (ref.status == AllocRefStatus::kOutOfMemory) {
      return E_OUTOFMEMORY;
    }
    return E_FAIL;
  }

  return S_OK;
}

HRESULT track_draw_state_locked(Device* dev) {
  if (!dev) {
    return E_INVALIDARG;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }

  if (dev->wddm_context.hContext == 0) {
    return S_OK;
  }

#if defined(_WIN32)
  const size_t min_packet = align_up(sizeof(aerogpu_cmd_hdr), 4);
  if (!wddm_ensure_recording_buffers(dev, min_packet)) {
    return E_FAIL;
  }
#endif

  if (!dev->alloc_list_tracker.list_base() || dev->alloc_list_tracker.list_capacity_effective() == 0) {
#if defined(_WIN32)
    return E_FAIL;
#else
    return S_OK;
#endif
  }

  // The allocation list is keyed by the stable `alloc_id` (backing_alloc_id) and
  // can legally alias multiple per-process WDDM allocation handles to the same
  // alloc_id for shared resources. Count unique alloc_ids rather than WDDM
  // handles so we don't incorrectly reject valid draws on small allocation lists
  // (e.g. shared resources opened multiple times).
  std::array<UINT, 4 + 1 + 16 + 16 + 1> unique_allocs{};
  size_t unique_alloc_len = 0;
  auto add_alloc = [&unique_allocs, &unique_alloc_len](const Resource* res) {
    if (!res) {
      return;
    }
    if (res->backing_alloc_id == 0) {
      return;
    }
    if (res->wddm_hAllocation == 0) {
      return;
    }
    const UINT alloc_id = res->backing_alloc_id;
    for (size_t i = 0; i < unique_alloc_len; ++i) {
      if (unique_allocs[i] == alloc_id) {
        return;
      }
    }
    unique_allocs[unique_alloc_len++] = alloc_id;
  };

  for (uint32_t i = 0; i < 4; i++) {
    add_alloc(dev->render_targets[i]);
  }
  add_alloc(dev->depth_stencil);
  for (uint32_t i = 0; i < 16; i++) {
    add_alloc(dev->textures[i]);
  }
  for (uint32_t i = 0; i < 16; i++) {
    add_alloc(dev->streams[i].vb);
  }
  add_alloc(dev->index_buffer);

  const UINT needed_total = static_cast<UINT>(unique_alloc_len);
  if (needed_total != 0) {
    const UINT cap = dev->alloc_list_tracker.list_capacity_effective();
    if (needed_total > cap) {
      logf("aerogpu-d3d9: draw requires %u allocations but allocation list capacity is %u\n",
           static_cast<unsigned>(needed_total),
           static_cast<unsigned>(cap));
      return E_FAIL;
    }

    UINT needed_new = 0;
    for (size_t i = 0; i < unique_alloc_len; ++i) {
      if (!dev->alloc_list_tracker.contains_alloc_id(unique_allocs[i])) {
        needed_new++;
      }
    }
    const UINT existing = dev->alloc_list_tracker.list_len();
    if (existing > cap || needed_new > cap - existing) {
      (void)submit(dev);
    }
  }

  for (uint32_t i = 0; i < 4; i++) {
    if (dev->render_targets[i]) {
      HRESULT hr = track_resource_allocation_locked(dev, dev->render_targets[i], /*write=*/true);
      if (hr < 0) {
        return hr;
      }
    }
  }

  if (dev->depth_stencil) {
    HRESULT hr = track_resource_allocation_locked(dev, dev->depth_stencil, /*write=*/true);
    if (hr < 0) {
      return hr;
    }
  }

  for (uint32_t i = 0; i < 16; i++) {
    if (dev->textures[i]) {
      HRESULT hr = track_resource_allocation_locked(dev, dev->textures[i], /*write=*/false);
      if (hr < 0) {
        return hr;
      }
    }
  }

  for (uint32_t i = 0; i < 16; i++) {
    if (dev->streams[i].vb) {
      HRESULT hr = track_resource_allocation_locked(dev, dev->streams[i].vb, /*write=*/false);
      if (hr < 0) {
        return hr;
      }
    }
  }

  if (dev->index_buffer) {
    HRESULT hr = track_resource_allocation_locked(dev, dev->index_buffer, /*write=*/false);
    if (hr < 0) {
      return hr;
    }
  }

  return S_OK;
}

HRESULT track_render_targets_locked(Device* dev) {
  if (!dev) {
    return E_INVALIDARG;
  }
  if (dev->wddm_context.hContext == 0) {
    return S_OK;
  }

#if defined(_WIN32)
  const size_t min_packet = align_up(sizeof(aerogpu_cmd_hdr), 4);
  if (!wddm_ensure_recording_buffers(dev, min_packet)) {
    return E_FAIL;
  }
#endif

  if (!dev->alloc_list_tracker.list_base() || dev->alloc_list_tracker.list_capacity_effective() == 0) {
#if defined(_WIN32)
    return E_FAIL;
#else
    return S_OK;
#endif
  }

  std::array<UINT, 4 + 1> unique_allocs{};
  size_t unique_alloc_len = 0;
  auto add_alloc = [&unique_allocs, &unique_alloc_len](const Resource* res) {
    if (!res) {
      return;
    }
    if (res->backing_alloc_id == 0) {
      return;
    }
    if (res->wddm_hAllocation == 0) {
      return;
    }
    const UINT alloc_id = res->backing_alloc_id;
    for (size_t i = 0; i < unique_alloc_len; ++i) {
      if (unique_allocs[i] == alloc_id) {
        return;
      }
    }
    unique_allocs[unique_alloc_len++] = alloc_id;
  };

  for (uint32_t i = 0; i < 4; ++i) {
    add_alloc(dev->render_targets[i]);
  }
  add_alloc(dev->depth_stencil);

  const UINT needed_total = static_cast<UINT>(unique_alloc_len);
  if (needed_total != 0) {
    const UINT cap = dev->alloc_list_tracker.list_capacity_effective();
    if (needed_total > cap) {
      logf("aerogpu-d3d9: render target bindings require %u allocations but allocation list capacity is %u\n",
           static_cast<unsigned>(needed_total),
           static_cast<unsigned>(cap));
      return E_FAIL;
    }

    UINT needed_new = 0;
    for (size_t i = 0; i < unique_alloc_len; ++i) {
      if (!dev->alloc_list_tracker.contains_alloc_id(unique_allocs[i])) {
        needed_new++;
      }
    }
    const UINT existing = dev->alloc_list_tracker.list_len();
    if (existing > cap || needed_new > cap - existing) {
      (void)submit(dev);
    }
  }

  for (uint32_t i = 0; i < 4; i++) {
    if (dev->render_targets[i]) {
      HRESULT hr = track_resource_allocation_locked(dev, dev->render_targets[i], /*write=*/true);
      if (hr < 0) {
        return hr;
      }
    }
  }

  if (dev->depth_stencil) {
    HRESULT hr = track_resource_allocation_locked(dev, dev->depth_stencil, /*write=*/true);
    if (hr < 0) {
      return hr;
    }
  }

  return S_OK;
}

bool emit_set_render_targets_locked(Device* dev) {
  auto* cmd = append_fixed_locked<aerogpu_cmd_set_render_targets>(dev, AEROGPU_CMD_SET_RENDER_TARGETS);
  if (!cmd) {
    return false;
  }

  // The host executor rejects gapped render-target bindings (a null RT followed
  // by a non-null RT). Clamp to the contiguous prefix to avoid emitting a packet
  // that would abort command-stream execution.
  uint32_t color_count = 0;
  while (color_count < 4 && dev->render_targets[color_count]) {
    color_count++;
  }
  for (uint32_t i = color_count; i < 4; ++i) {
    dev->render_targets[i] = nullptr;
  }

  cmd->color_count = color_count;
  cmd->depth_stencil = dev->depth_stencil ? dev->depth_stencil->handle : 0;

  for (uint32_t i = 0; i < AEROGPU_MAX_RENDER_TARGETS; i++) {
    cmd->colors[i] = 0;
  }
  for (uint32_t i = 0; i < color_count; i++) {
    cmd->colors[i] = dev->render_targets[i] ? dev->render_targets[i]->handle : 0;
  }
  return true;
}

bool emit_bind_shaders_locked(Device* dev) {
  const size_t needed = align_up(sizeof(aerogpu_cmd_bind_shaders), 4);
  if (!ensure_cmd_space(dev, needed)) {
    return false;
  }
  auto* cmd = dev->cmd.bind_shaders(
      /*vs=*/dev->vs ? dev->vs->handle : 0,
      /*ps=*/dev->ps ? dev->ps->handle : 0,
      /*cs=*/0);
  return cmd != nullptr;
}

bool emit_set_topology_locked(Device* dev, uint32_t topology) {
  if (dev->topology == topology) {
    return true;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_set_primitive_topology>(dev, AEROGPU_CMD_SET_PRIMITIVE_TOPOLOGY);
  if (!cmd) {
    return false;
  }
  dev->topology = topology;
  cmd->topology = topology;
  cmd->reserved0 = 0;
  return true;
}

bool emit_create_resource_locked(Device* dev, Resource* res) {
  if (!dev || !res) {
    return false;
  }

  if (res->kind == ResourceKind::Buffer) {
    // Ensure the command buffer has space before we track allocations; tracking
    // may force a submission split, and command-buffer splits must not occur
    // after tracking or the allocation list would be out of sync.
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_create_buffer), 4))) {
      return false;
    }
    if (track_resource_allocation_locked(dev, res, /*write=*/false) < 0) {
      return false;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_create_buffer>(dev, AEROGPU_CMD_CREATE_BUFFER);
    if (!cmd) {
      return false;
    }
    cmd->buffer_handle = res->handle;
    cmd->usage_flags = AEROGPU_RESOURCE_USAGE_VERTEX_BUFFER | AEROGPU_RESOURCE_USAGE_INDEX_BUFFER;
    cmd->size_bytes = res->size_bytes;
    cmd->backing_alloc_id = res->backing_alloc_id;
    cmd->backing_offset_bytes = res->backing_offset_bytes;
    cmd->reserved0 = 0;
    return true;
  }

  if (res->kind == ResourceKind::Surface || res->kind == ResourceKind::Texture2D) {
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_create_texture2d), 4))) {
      return false;
    }
    if (track_resource_allocation_locked(dev, res, /*write=*/false) < 0) {
      return false;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_create_texture2d>(dev, AEROGPU_CMD_CREATE_TEXTURE2D);
    if (!cmd) {
      return false;
    }
    cmd->texture_handle = res->handle;
    cmd->usage_flags = d3d9_usage_to_aerogpu_usage_flags(res->usage);
    cmd->format = d3d9_format_to_aerogpu(res->format);
    cmd->width = res->width;
    cmd->height = res->height;
    cmd->mip_levels = res->mip_levels;
    cmd->array_layers = std::max(1u, res->depth);
    cmd->row_pitch_bytes = res->row_pitch;
    cmd->backing_alloc_id = res->backing_alloc_id;
    cmd->backing_offset_bytes = res->backing_offset_bytes;
    cmd->reserved0 = 0;
    return true;
  }
  return false;
}

bool emit_destroy_resource_locked(Device* dev, aerogpu_handle_t handle) {
  if (!dev || !handle) {
    return false;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_destroy_resource>(dev, AEROGPU_CMD_DESTROY_RESOURCE);
  if (!cmd) {
    return false;
  }
  cmd->resource_handle = handle;
  cmd->reserved0 = 0;
  return true;
}

bool emit_export_shared_surface_locked(Device* dev, const Resource* res) {
  if (!dev || !res || !res->handle || !res->share_token) {
    return false;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_export_shared_surface>(dev, AEROGPU_CMD_EXPORT_SHARED_SURFACE);
  if (!cmd) {
    return false;
  }
  logf("aerogpu-d3d9: export shared surface handle=%u share_token=0x%llx\n",
       static_cast<unsigned>(res->handle),
       static_cast<unsigned long long>(res->share_token));
  cmd->resource_handle = res->handle;
  cmd->reserved0 = 0;
  cmd->share_token = res->share_token;
  return true;
}

bool emit_import_shared_surface_locked(Device* dev, const Resource* res) {
  if (!dev || !res || !res->handle || !res->share_token) {
    return false;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_import_shared_surface>(dev, AEROGPU_CMD_IMPORT_SHARED_SURFACE);
  if (!cmd) {
    return false;
  }
  logf("aerogpu-d3d9: import shared surface out_handle=%u share_token=0x%llx\n",
       static_cast<unsigned>(res->handle),
       static_cast<unsigned long long>(res->share_token));
  cmd->out_resource_handle = res->handle;
  cmd->reserved0 = 0;
  cmd->share_token = res->share_token;
  return true;
}

bool emit_create_shader_locked(Device* dev, Shader* sh) {
  if (!dev || !sh) {
    return false;
  }

  auto* cmd = append_with_payload_locked<aerogpu_cmd_create_shader_dxbc>(
      dev,
      AEROGPU_CMD_CREATE_SHADER_DXBC, sh->bytecode.data(), sh->bytecode.size());
  if (!cmd) {
    return false;
  }
  cmd->shader_handle = sh->handle;
  cmd->stage = d3d9_stage_to_aerogpu_stage(sh->stage);
  cmd->dxbc_size_bytes = static_cast<uint32_t>(sh->bytecode.size());
  cmd->reserved0 = 0;
  return true;
}

bool emit_destroy_shader_locked(Device* dev, aerogpu_handle_t handle) {
  if (!dev || !handle) {
    return false;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_destroy_shader>(dev, AEROGPU_CMD_DESTROY_SHADER);
  if (!cmd) {
    return false;
  }
  cmd->shader_handle = handle;
  cmd->reserved0 = 0;
  return true;
}

bool emit_create_input_layout_locked(Device* dev, VertexDecl* decl) {
  if (!dev || !decl) {
    return false;
  }

  auto* cmd = append_with_payload_locked<aerogpu_cmd_create_input_layout>(
      dev,
      AEROGPU_CMD_CREATE_INPUT_LAYOUT, decl->blob.data(), decl->blob.size());
  if (!cmd) {
    return false;
  }
  cmd->input_layout_handle = decl->handle;
  cmd->blob_size_bytes = static_cast<uint32_t>(decl->blob.size());
  cmd->reserved0 = 0;
  return true;
}

bool emit_destroy_input_layout_locked(Device* dev, aerogpu_handle_t handle) {
  if (!dev || !handle) {
    return false;
  }
  auto* cmd = append_fixed_locked<aerogpu_cmd_destroy_input_layout>(dev, AEROGPU_CMD_DESTROY_INPUT_LAYOUT);
  if (!cmd) {
    return false;
  }
  cmd->input_layout_handle = handle;
  cmd->reserved0 = 0;
  return true;
}

bool emit_set_input_layout_locked(Device* dev, VertexDecl* decl) {
  if (!dev) {
    return false;
  }
  if (dev->vertex_decl == decl) {
    return true;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_input_layout>(dev, AEROGPU_CMD_SET_INPUT_LAYOUT);
  if (!cmd) {
    return false;
  }

  dev->vertex_decl = decl;
  cmd->input_layout_handle = decl ? decl->handle : 0;
  cmd->reserved0 = 0;
  return true;
}

bool emit_set_stream_source_locked(
    Device* dev,
    uint32_t stream,
    Resource* vb,
    uint32_t offset_bytes,
    uint32_t stride_bytes) {
  if (!dev || stream >= 16) {
    return false;
  }

  DeviceStateStream& ss = dev->streams[stream];
  if (ss.vb == vb && ss.offset_bytes == offset_bytes && ss.stride_bytes == stride_bytes) {
    return true;
  }

  aerogpu_vertex_buffer_binding binding{};
  binding.buffer = vb ? vb->handle : 0;
  binding.stride_bytes = stride_bytes;
  binding.offset_bytes = offset_bytes;
  binding.reserved0 = 0;

  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
      dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding));
  if (!cmd) {
    return false;
  }
  cmd->start_slot = stream;
  cmd->buffer_count = 1;

  ss.vb = vb;
  ss.offset_bytes = offset_bytes;
  ss.stride_bytes = stride_bytes;
  return true;
}

Shader* create_internal_shader_locked(
    Device* dev,
    uint32_t stage,
    const void* bytecode,
    uint32_t bytecode_size) {
  if (!dev || !dev->adapter || !bytecode || bytecode_size == 0) {
    return nullptr;
  }

  auto sh = make_unique_nothrow<Shader>();
  if (!sh) {
    return nullptr;
  }
  sh->handle = allocate_global_handle(dev->adapter);
  sh->stage = stage;
  try {
    sh->bytecode.resize(bytecode_size);
  } catch (...) {
    return nullptr;
  }
  std::memcpy(sh->bytecode.data(), bytecode, bytecode_size);

  if (!emit_create_shader_locked(dev, sh.get())) {
    return nullptr;
  }
  return sh.release();
}

VertexDecl* create_internal_vertex_decl_locked(Device* dev, const void* pDecl, uint32_t decl_size) {
  if (!dev || !dev->adapter || !pDecl || decl_size == 0) {
    return nullptr;
  }

  auto decl = make_unique_nothrow<VertexDecl>();
  if (!decl) {
    return nullptr;
  }
  decl->handle = allocate_global_handle(dev->adapter);
  try {
    decl->blob.resize(decl_size);
  } catch (...) {
    return nullptr;
  }
  std::memcpy(decl->blob.data(), pDecl, decl_size);

  if (!emit_create_input_layout_locked(dev, decl.get())) {
    return nullptr;
  }
  return decl.release();
}

// Forward declaration (defined below): used by fixed-function helpers to upload
// internal constant registers (e.g. D3DRS_TEXTUREFACTOR).
static bool emit_set_shader_constants_f_locked(
    Device* dev,
    uint32_t stage,
    uint32_t start_reg,
    const float* data,
    uint32_t vec4_count);

// -----------------------------------------------------------------------------
// Fixed-function fallback shader selection (texture stage state -> ps_2_0)
// -----------------------------------------------------------------------------
namespace {

// Texture stage state IDs (numeric values from d3d9types.h).
constexpr uint32_t kD3dTssColorOp = 1u;    // D3DTSS_COLOROP
constexpr uint32_t kD3dTssColorArg1 = 2u;  // D3DTSS_COLORARG1
constexpr uint32_t kD3dTssColorArg2 = 3u;  // D3DTSS_COLORARG2
constexpr uint32_t kD3dTssAlphaOp = 4u;    // D3DTSS_ALPHAOP
constexpr uint32_t kD3dTssAlphaArg1 = 5u;  // D3DTSS_ALPHAARG1
constexpr uint32_t kD3dTssAlphaArg2 = 6u;  // D3DTSS_ALPHAARG2

// D3DTOP_* (numeric values from d3d9types.h).
constexpr uint32_t kD3dTopDisable = 1u;     // D3DTOP_DISABLE
constexpr uint32_t kD3dTopSelectArg1 = 2u;  // D3DTOP_SELECTARG1
constexpr uint32_t kD3dTopSelectArg2 = 3u;  // D3DTOP_SELECTARG2
constexpr uint32_t kD3dTopModulate = 4u;    // D3DTOP_MODULATE
constexpr uint32_t kD3dTopModulate2x = 5u;  // D3DTOP_MODULATE2X
constexpr uint32_t kD3dTopModulate4x = 6u;  // D3DTOP_MODULATE4X
constexpr uint32_t kD3dTopAdd = 7u;         // D3DTOP_ADD
constexpr uint32_t kD3dTopAddSigned = 8u;   // D3DTOP_ADDSIGNED
constexpr uint32_t kD3dTopSubtract = 10u;   // D3DTOP_SUBTRACT
constexpr uint32_t kD3dTopBlendDiffuseAlpha = 12u;  // D3DTOP_BLENDDIFFUSEALPHA
constexpr uint32_t kD3dTopBlendTextureAlpha = 13u;  // D3DTOP_BLENDTEXTUREALPHA

// `D3DTA_*` source selector/modifiers (numeric values from d3d9types.h).
constexpr uint32_t kD3dTaSelectMask = 0xFu;  // D3DTA_SELECTMASK
constexpr uint32_t kD3dTaDiffuse = 0u;       // D3DTA_DIFFUSE
constexpr uint32_t kD3dTaCurrent = 1u;       // D3DTA_CURRENT
constexpr uint32_t kD3dTaTexture = 2u;       // D3DTA_TEXTURE
constexpr uint32_t kD3dTaTFactor = 3u;       // D3DTA_TFACTOR
constexpr uint32_t kD3dTaComplement = 0x10u;      // D3DTA_COMPLEMENT
constexpr uint32_t kD3dTaAlphaReplicate = 0x20u;  // D3DTA_ALPHAREPLICATE

// Fixed-function texture stage state uses D3DRS_TEXTUREFACTOR by uploading it into
// a reserved *high* pixel-shader constant register (c255). This avoids clobbering
// app-provided PS constants in the commonly used low range (especially c0). The
// same constant is reused for all fixed-function stages that reference TFACTOR.
constexpr uint32_t kFixedfuncStage0TextureFactorPsRegister = 255u; // c255
// Keep this in sync with the caps we advertise (MaxTextureBlendStages).
constexpr uint32_t kFixedfuncMaxTextureStages = 4u;

enum class FixedfuncStageArgSrc : uint8_t {
  Diffuse = 0,
  Current = 1,
  Texture = 2,
  TextureFactor = 3,
};

struct FixedfuncStageArg {
  FixedfuncStageArgSrc src = FixedfuncStageArgSrc::Diffuse;
  bool complement = false;
  bool alpha_replicate = false;
};

enum class FixedfuncStageOp : uint8_t {
  Disable = 0,
  SelectArg1 = 1,
  SelectArg2 = 2,
  Modulate = 3,
  Modulate2x = 4,
  Modulate4x = 5,
  Add = 6,
  Subtract = 7,
  AddSigned = 8,
  BlendTextureAlpha = 9,
  BlendDiffuseAlpha = 10,
};

struct FixedfuncStageState {
  FixedfuncStageOp op = FixedfuncStageOp::Disable;
  FixedfuncStageArg arg1{};
  FixedfuncStageArg arg2{};
};

struct FixedfuncPixelStageKey {
  FixedfuncStageState color{};
  FixedfuncStageState alpha{};
  bool uses_texture = false;
  bool uses_tfactor = false; // True when stage state uses D3DRS_TEXTUREFACTOR (passed to the PS as c255).
};

struct FixedfuncPixelShaderKey {
  std::array<FixedfuncPixelStageKey, kFixedfuncMaxTextureStages> stages{};
  uint32_t stage_count = 0;
  bool uses_tfactor = false;
  // True when fixed-function fog is enabled and implemented by the fixed-function
  // fallback pixel shader variant.
  bool fog_enabled = false;
  // False when the stage-state combination is not supported by the fixed-function
  // fallback path; callers must treat this as D3DERR_INVALIDCALL at draw time.
  bool supported = true;
};

uint64_t fixedfunc_arg_signature(const FixedfuncStageArg& arg) {
  // 4-bit signature:
  // - bits 0..1: FixedfuncStageArgSrc
  // - bit 2: complement
  // - bit 3: alpha_replicate
  return static_cast<uint64_t>(arg.src) |
         (static_cast<uint64_t>(arg.complement ? 1u : 0u) << 2) |
         (static_cast<uint64_t>(arg.alpha_replicate ? 1u : 0u) << 3);
}

uint64_t fixedfunc_state_signature(const FixedfuncStageState& st) {
  // 12-bit signature:
  // - bits 0..3: FixedfuncStageOp
  // - bits 4..7: arg1 signature
  // - bits 8..11: arg2 signature
  return static_cast<uint64_t>(st.op) |
         (fixedfunc_arg_signature(st.arg1) << 4) |
         (fixedfunc_arg_signature(st.arg2) << 8);
}

uint64_t fixedfunc_ps_signature(const FixedfuncPixelShaderKey& key) {
  // Hash fixed-function stage state into a stable signature so we can reuse a
  // cached PS without rebuilding the ps_2_0 token stream each time.
  //
  // This intentionally hashes only the *effective* stages: fixed-function stage
  // disabling is keyed off COLOROP, so stage N with COLOROP=DISABLE disables that
  // stage and all subsequent stages. `fixedfunc_ps_key_locked()` canonicalizes
  // this by truncating `stage_count` accordingly.
  constexpr uint64_t kFnvOffset = 14695981039346656037ull;
  constexpr uint64_t kFnvPrime = 1099511628211ull;
  uint64_t h = kFnvOffset;
  auto write_u8 = [&](uint8_t v) {
    h ^= static_cast<uint64_t>(v);
    h *= kFnvPrime;
  };
  auto write_u32 = [&](uint32_t v) {
    write_u8(static_cast<uint8_t>((v >> 0) & 0xFFu));
    write_u8(static_cast<uint8_t>((v >> 8) & 0xFFu));
    write_u8(static_cast<uint8_t>((v >> 16) & 0xFFu));
    write_u8(static_cast<uint8_t>((v >> 24) & 0xFFu));
  };

  write_u32(key.stage_count);
  write_u8(key.uses_tfactor ? 1u : 0u);
  write_u8(key.fog_enabled ? 1u : 0u);
  for (uint32_t i = 0; i < key.stage_count && i < kFixedfuncMaxTextureStages; ++i) {
    const auto& st = key.stages[i];
    write_u32(static_cast<uint32_t>(fixedfunc_state_signature(st.color)));
    write_u32(static_cast<uint32_t>(fixedfunc_state_signature(st.alpha)));
    // Include whether the stage actually samples its texture. This is redundant
    // with the arg/op encoding, but keeps the hash resilient if the internal
    // lowering logic changes (e.g. future canonicalization).
    write_u8(st.uses_texture ? 1u : 0u);
    write_u8(st.uses_tfactor ? 1u : 0u);
  }
  return h;
}

bool shader_bytecode_equals(const Shader* sh, const void* bytes, uint32_t size) {
  if (!sh || !bytes || size == 0) {
    return false;
  }
  if (sh->bytecode.size() != size) {
    return false;
  }
  return std::memcmp(sh->bytecode.data(), bytes, size) == 0;
}

bool fixedfunc_decode_arg(uint32_t stage, uint32_t arg, FixedfuncStageArg* out) {
  if (!out) {
    return false;
  }

  constexpr uint32_t kAllowedMask = kD3dTaSelectMask | kD3dTaComplement | kD3dTaAlphaReplicate;
  if ((arg & ~kAllowedMask) != 0) {
    return false;
  }

  out->complement = (arg & kD3dTaComplement) != 0;
  out->alpha_replicate = (arg & kD3dTaAlphaReplicate) != 0;
  switch (arg & kD3dTaSelectMask) {
    case kD3dTaDiffuse:
      out->src = FixedfuncStageArgSrc::Diffuse;
      return true;
    case kD3dTaCurrent:
      // Stage0 CURRENT is equivalent to DIFFUSE. Canonicalize it so different
      // raw stage-state inputs that lower to identical behavior share a cache key.
      out->src = (stage == 0) ? FixedfuncStageArgSrc::Diffuse : FixedfuncStageArgSrc::Current;
      return true;
    case kD3dTaTexture:
      out->src = FixedfuncStageArgSrc::Texture;
      return true;
    case kD3dTaTFactor:
      out->src = FixedfuncStageArgSrc::TextureFactor;
      return true;
    default:
      return false;
  }
}

bool fixedfunc_decode_op(uint32_t op, FixedfuncStageOp* out) {
  if (!out) {
    return false;
  }
  switch (op) {
    case kD3dTopDisable:
      *out = FixedfuncStageOp::Disable;
      return true;
    case kD3dTopSelectArg1:
      *out = FixedfuncStageOp::SelectArg1;
      return true;
    case kD3dTopSelectArg2:
      *out = FixedfuncStageOp::SelectArg2;
      return true;
    case kD3dTopModulate:
      *out = FixedfuncStageOp::Modulate;
      return true;
    case kD3dTopModulate2x:
      *out = FixedfuncStageOp::Modulate2x;
      return true;
    case kD3dTopModulate4x:
      *out = FixedfuncStageOp::Modulate4x;
      return true;
    case kD3dTopAdd:
      *out = FixedfuncStageOp::Add;
      return true;
    case kD3dTopSubtract:
      *out = FixedfuncStageOp::Subtract;
      return true;
    case kD3dTopAddSigned:
      *out = FixedfuncStageOp::AddSigned;
      return true;
    case kD3dTopBlendTextureAlpha:
      *out = FixedfuncStageOp::BlendTextureAlpha;
      return true;
    case kD3dTopBlendDiffuseAlpha:
      *out = FixedfuncStageOp::BlendDiffuseAlpha;
      return true;
    default:
      return false;
  }
}

bool fixedfunc_op_uses_arg1(FixedfuncStageOp op) {
  switch (op) {
    case FixedfuncStageOp::Disable:
    case FixedfuncStageOp::SelectArg2:
      return false;
    default:
      return true;
  }
}

bool fixedfunc_op_uses_arg2(FixedfuncStageOp op) {
  switch (op) {
    case FixedfuncStageOp::Modulate:
    case FixedfuncStageOp::Modulate2x:
    case FixedfuncStageOp::Modulate4x:
    case FixedfuncStageOp::Add:
    case FixedfuncStageOp::Subtract:
    case FixedfuncStageOp::AddSigned:
    case FixedfuncStageOp::BlendTextureAlpha:
    case FixedfuncStageOp::BlendDiffuseAlpha:
    case FixedfuncStageOp::SelectArg2:
      return true;
    default:
      return false;
  }
}

bool fixedfunc_state_uses_texture(const FixedfuncStageState& st) {
  if (st.op == FixedfuncStageOp::BlendTextureAlpha) {
    // Uses texture alpha as the blend factor regardless of arg sources.
    return true;
  }
  if (fixedfunc_op_uses_arg1(st.op) && st.arg1.src == FixedfuncStageArgSrc::Texture) {
    return true;
  }
  if (fixedfunc_op_uses_arg2(st.op) && st.arg2.src == FixedfuncStageArgSrc::Texture) {
    return true;
  }
  return false;
}

bool fixedfunc_state_uses_tfactor(const FixedfuncStageState& st) {
  if (fixedfunc_op_uses_arg1(st.op) && st.arg1.src == FixedfuncStageArgSrc::TextureFactor) {
    return true;
  }
  if (fixedfunc_op_uses_arg2(st.op) && st.arg2.src == FixedfuncStageArgSrc::TextureFactor) {
    return true;
  }
  return false;
}

HRESULT ensure_fixedfunc_texture_factor_constant_locked(Device* dev) {
  if (!dev) {
    return E_FAIL;
  }

  // Render-state numeric value from d3d9types.h (D3DRS_TEXTUREFACTOR).
  constexpr uint32_t kD3dRsTextureFactor = 60u;

  const uint32_t tf = (kD3dRsTextureFactor < 256) ? dev->render_states[kD3dRsTextureFactor] : 0u;
  const float a = static_cast<float>((tf >> 24) & 0xFFu) * (1.0f / 255.0f);
  const float r = static_cast<float>((tf >> 16) & 0xFFu) * (1.0f / 255.0f);
  const float g = static_cast<float>((tf >> 8) & 0xFFu) * (1.0f / 255.0f);
  const float b = static_cast<float>((tf >> 0) & 0xFFu) * (1.0f / 255.0f);
  const float data[4] = {r, g, b, a};

  const float* cached = dev->ps_consts_f + static_cast<size_t>(kFixedfuncStage0TextureFactorPsRegister) * 4u;
  if (cached[0] == data[0] && cached[1] == data[1] && cached[2] == data[2] && cached[3] == data[3]) {
    return S_OK;
  }

  if (!emit_set_shader_constants_f_locked(dev,
                                          kD3d9ShaderStagePs,
                                          kFixedfuncStage0TextureFactorPsRegister,
                                          data,
                                          /*vec4_count=*/1u)) {
    return E_OUTOFMEMORY;
  }

  return S_OK;
}

HRESULT ensure_fixedfunc_fog_constants_locked(Device* dev) {
  if (!dev) {
    return E_FAIL;
  }

  // Render-state numeric values from d3d9types.h.
  constexpr uint32_t kD3dRsFogColor = 34u; // D3DRS_FOGCOLOR
  constexpr uint32_t kD3dRsFogStart = 36u; // D3DRS_FOGSTART (float bits)
  constexpr uint32_t kD3dRsFogEnd = 37u;   // D3DRS_FOGEND   (float bits)

  // Fixed-function PS constant registers reserved for fog.
  //
  // Layout:
  // - c1: fog color (RGBA normalized from D3DRS_FOGCOLOR ARGB)
  // - c2: fog params (x=fog_start, y=inv_fog_range, z/w unused)
  constexpr uint32_t kPsFogColorRegister = 1u;
  constexpr uint32_t kPsFogParamsRegister = 2u;

  const uint32_t fog_argb = (kD3dRsFogColor < 256) ? dev->render_states[kD3dRsFogColor] : 0u;
  const float a = static_cast<float>((fog_argb >> 24) & 0xFFu) * (1.0f / 255.0f);
  const float r = static_cast<float>((fog_argb >> 16) & 0xFFu) * (1.0f / 255.0f);
  const float g = static_cast<float>((fog_argb >> 8) & 0xFFu) * (1.0f / 255.0f);
  const float b = static_cast<float>((fog_argb >> 0) & 0xFFu) * (1.0f / 255.0f);
  const float color[4] = {r, g, b, a};

  const uint32_t fog_start_bits = (kD3dRsFogStart < 256) ? dev->render_states[kD3dRsFogStart] : 0u;
  const uint32_t fog_end_bits = (kD3dRsFogEnd < 256) ? dev->render_states[kD3dRsFogEnd] : 0u;
  float fog_start = 0.0f;
  float fog_end = 0.0f;
  std::memcpy(&fog_start, &fog_start_bits, sizeof(float));
  std::memcpy(&fog_end, &fog_end_bits, sizeof(float));

  const float range = fog_end - fog_start;
  const float inv_range = (range != 0.0f) ? (1.0f / range) : 0.0f;
  const float params[4] = {fog_start, inv_range, 0.0f, 0.0f};

  const float* cached_color = dev->ps_consts_f + static_cast<size_t>(kPsFogColorRegister) * 4u;
  const float* cached_params = dev->ps_consts_f + static_cast<size_t>(kPsFogParamsRegister) * 4u;
  const bool color_ok = cached_color[0] == color[0] && cached_color[1] == color[1] && cached_color[2] == color[2] &&
                        cached_color[3] == color[3];
  const bool params_ok =
      cached_params[0] == params[0] && cached_params[1] == params[1] && cached_params[2] == params[2] &&
      cached_params[3] == params[3];
  if (color_ok && params_ok) {
    return S_OK;
  }

  float regs[8] = {};
  std::memcpy(&regs[0], color, sizeof(color));
  std::memcpy(&regs[4], params, sizeof(params));

  if (!emit_set_shader_constants_f_locked(dev,
                                          kD3d9ShaderStagePs,
                                          kPsFogColorRegister,
                                          regs,
                                          /*vec4_count=*/2u)) {
    return E_OUTOFMEMORY;
  }

  return S_OK;
}

FixedfuncPixelShaderKey fixedfunc_ps_key_locked(const Device* dev) {
  FixedfuncPixelShaderKey key{};
  if (!dev) {
    return key;
  }

  key.stage_count = 0;
  key.uses_tfactor = false;
  key.fog_enabled = false;
  key.supported = true;

  // ---------------------------------------------------------------------------
  // Fixed-function fog (D3DRS_FOG*)
  // ---------------------------------------------------------------------------
  //
  // The AeroGPU fixed-function fallback implements a minimal, deterministic fog
  // subset in the fixed-function *pixel shader* variants. The fog coordinate is
  // sourced from TEXCOORD0.z which is populated by dedicated fixed-function VS
  // variants (see `aerogpu_d3d9_fixedfunc_shaders.h`).
  //
  // Supported bring-up subset:
  // - D3DRS_FOGENABLE
  // - D3DRS_FOGTABLEMODE or D3DRS_FOGVERTEXMODE: LINEAR only
  // - D3DRS_FOGSTART / D3DRS_FOGEND
  //
  // Important: keep shader-stage interop stable. In the VS-only interop path
  // (user VS, NULL PS), we must not change the fixed-function pixel shader input
  // layout by injecting fog coordinates.
  if (dev->user_vs == nullptr) {
    constexpr uint32_t kD3dRsFogEnable = 28u;      // D3DRS_FOGENABLE
    constexpr uint32_t kD3dRsFogTableMode = 35u;   // D3DRS_FOGTABLEMODE
    constexpr uint32_t kD3dRsFogVertexMode = 140u; // D3DRS_FOGVERTEXMODE
    constexpr uint32_t kD3dFogLinear = 3u;         // D3DFOG_LINEAR

    const uint32_t fog_enable = (kD3dRsFogEnable < 256) ? dev->render_states[kD3dRsFogEnable] : 0u;
    if (fog_enable != 0) {
      const uint32_t table_mode = (kD3dRsFogTableMode < 256) ? dev->render_states[kD3dRsFogTableMode] : 0u;
      const uint32_t vertex_mode = (kD3dRsFogVertexMode < 256) ? dev->render_states[kD3dRsFogVertexMode] : 0u;
      // D3D9 semantics: table fog takes precedence when enabled (non-NONE).
      const uint32_t mode = (table_mode != 0) ? table_mode : vertex_mode;
      key.fog_enabled = (mode == kD3dFogLinear);
    }
  }

  for (uint32_t stage = 0; stage < kFixedfuncMaxTextureStages; ++stage) {
    FixedfuncPixelStageKey st{};

    const uint32_t colorop = dev->texture_stage_states[stage][kD3dTssColorOp];
    const uint32_t colorarg1 = dev->texture_stage_states[stage][kD3dTssColorArg1];
    const uint32_t colorarg2 = dev->texture_stage_states[stage][kD3dTssColorArg2];
    const uint32_t alphaop = dev->texture_stage_states[stage][kD3dTssAlphaOp];
    const uint32_t alphaarg1 = dev->texture_stage_states[stage][kD3dTssAlphaArg1];
    const uint32_t alphaarg2 = dev->texture_stage_states[stage][kD3dTssAlphaArg2];

    if (!fixedfunc_decode_op(colorop, &st.color.op)) {
      key.supported = false;
      return key;
    }

    // D3D9 semantics: if COLOROP is DISABLE, that stage and all subsequent stages
    // are disabled. Ignore ALPHAOP/args so we don't accidentally sample textures.
    if (st.color.op == FixedfuncStageOp::Disable) {
      break;
    }

    if (!fixedfunc_decode_op(alphaop, &st.alpha.op)) {
      key.supported = false;
      return key;
    }

    // Decode only the arguments required for each op so unused args can't cause
    // spurious INVALIDCALL failures.
    if (fixedfunc_op_uses_arg1(st.color.op)) {
      if (!fixedfunc_decode_arg(stage, colorarg1, &st.color.arg1)) {
        key.supported = false;
        return key;
      }
    }
    if (fixedfunc_op_uses_arg2(st.color.op)) {
      if (!fixedfunc_decode_arg(stage, colorarg2, &st.color.arg2)) {
        key.supported = false;
        return key;
      }
    }
    if (fixedfunc_op_uses_arg1(st.alpha.op)) {
      if (!fixedfunc_decode_arg(stage, alphaarg1, &st.alpha.arg1)) {
        key.supported = false;
        return key;
      }
    }
    if (fixedfunc_op_uses_arg2(st.alpha.op)) {
      if (!fixedfunc_decode_arg(stage, alphaarg2, &st.alpha.arg2)) {
        key.supported = false;
        return key;
      }
    }

    st.uses_texture = fixedfunc_state_uses_texture(st.color) || fixedfunc_state_uses_texture(st.alpha);
    st.uses_tfactor = fixedfunc_state_uses_tfactor(st.color) || fixedfunc_state_uses_tfactor(st.alpha);

    // Defensive: avoid selecting a texture-sampling shader when the stage's
    // texture is unbound. Treat the stage chain as disabled/passthrough to avoid
    // emitting texld from an invalid slot.
    const bool has_tex = (dev->textures[stage] != nullptr);
    if (!has_tex && st.uses_texture) {
      break;
    }

    key.stages[stage] = st;
    key.stage_count = stage + 1;
    key.uses_tfactor = key.uses_tfactor || st.uses_tfactor;
  }

  return key;
}

// -----------------------------------------------------------------------------
// Minimal ps_2_0 token builder used for fixed-function texture stage state.
// -----------------------------------------------------------------------------
namespace fixedfunc_ps20 {

constexpr uint32_t kPs20Version = 0xFFFF0200u;
constexpr uint32_t kPsEnd = 0x0000FFFFu;

// Instruction tokens are encoded as:
//   (length_in_dwords << 24) | opcode
// where `length_in_dwords` includes the instruction token itself.
constexpr uint32_t kOpMov = 0x03000001u;   // mov (3 dwords: op + dst + src)
constexpr uint32_t kOpAdd = 0x04000002u;   // add (4 dwords: op + dst + a + b)
constexpr uint32_t kOpMul = 0x04000005u;   // mul (4 dwords: op + dst + a + b)
constexpr uint32_t kOpTexld = 0x04000042u; // texld (4 dwords: op + dst + t0 + s0)
constexpr uint32_t kOpMin = 0x0400000Au;   // min (4 dwords: op + dst + a + b)
constexpr uint32_t kOpMax = 0x0400000Bu;   // max (4 dwords: op + dst + a + b)
constexpr uint32_t kOpDef = 0x06000051u;   // def (6 dwords: op + dst + imm4)

// `D3DSHADER_SRCMOD_*` (numeric values from d3d9types.h).
enum class SrcMod : uint32_t {
  None = 0,
  Neg = 1,
  Bias = 2,
  Comp = 6,
};

constexpr uint32_t swizzle(uint8_t x, uint8_t y, uint8_t z, uint8_t w) {
  return static_cast<uint32_t>(x) | (static_cast<uint32_t>(y) << 2) | (static_cast<uint32_t>(z) << 4) |
         (static_cast<uint32_t>(w) << 6);
}

constexpr uint32_t swizzle_xyzw() {
  return swizzle(0, 1, 2, 3); // 0xE4
}

constexpr uint32_t swizzle_xxxx() {
  return swizzle(0, 0, 0, 0); // 0x00
}

constexpr uint32_t swizzle_yyyy() {
  return swizzle(1, 1, 1, 1); // 0x55
}

constexpr uint32_t swizzle_zzzz() {
  return swizzle(2, 2, 2, 2); // 0xAA
}

constexpr uint32_t swizzle_wwww() {
  return swizzle(3, 3, 3, 3); // 0xFF
}

constexpr uint32_t dst_temp(uint32_t reg, uint32_t mask) {
  return (mask << 16) | reg;
}

constexpr uint32_t dst_oc0(uint32_t mask) {
  // Pixel shader output color register. For ps_2_0 token streams without dcl
  // declarations, this encoding matches the existing fixed-function shader tables.
  return (mask << 16) | 0x0800u;
}

constexpr uint32_t src_reg(uint32_t reg_type, uint32_t reg, uint32_t swz, SrcMod mod) {
  return (reg_type << 28) | (static_cast<uint32_t>(mod) << 24) | (swz << 16) | reg;
}

constexpr uint32_t src_temp(uint32_t reg, uint32_t swz = swizzle_xyzw(), SrcMod mod = SrcMod::None) {
  return src_reg(/*reg_type=*/0, reg, swz, mod);
}

constexpr uint32_t src_input(uint32_t reg, uint32_t swz = swizzle_xyzw(), SrcMod mod = SrcMod::None) {
  return src_reg(/*reg_type=*/1, reg, swz, mod);
}

constexpr uint32_t src_const(uint32_t reg, uint32_t swz = swizzle_xyzw(), SrcMod mod = SrcMod::None) {
  return src_reg(/*reg_type=*/2, reg, swz, mod);
}

constexpr uint32_t src_texcoord(uint32_t reg, uint32_t swz = swizzle_xyzw(), SrcMod mod = SrcMod::None) {
  return src_reg(/*reg_type=*/3, reg, swz, mod);
}

constexpr uint32_t src_sampler(uint32_t reg) {
  // sN token (sampler), with the same encoding used by existing fixed-function
  // shaders (e.g. s0 == 0x20E40800).
  return 0x20E40800u + reg;
}

constexpr uint32_t dst_const(uint32_t reg) {
  // cN destination token used by the `def` instruction.
  return 0x200F0000u | reg;
}

inline uint32_t f32_bits(float f) {
  uint32_t u = 0;
  static_assert(sizeof(u) == sizeof(f), "float and uint32_t must be the same size");
  std::memcpy(&u, &f, sizeof(u));
  return u;
}

struct Builder {
  std::vector<uint32_t> tokens;

  bool begin() {
    tokens.clear();
    // Fixed-function PS token streams are bounded (see `build_fixedfunc_ps`). Pre-reserve so
    // `push_back` does not allocate and cannot throw under memory pressure.
    constexpr size_t kMaxTokens = 512;
    try {
      tokens.reserve(kMaxTokens);
      tokens.push_back(kPs20Version);
    } catch (...) {
      tokens.clear();
      return false;
    }
    return true;
  }

  void end() {
    tokens.push_back(kPsEnd);
  }

  void texld(uint32_t dst, uint32_t coord, uint32_t sampler) {
    tokens.push_back(kOpTexld);
    tokens.push_back(dst);
    tokens.push_back(coord);
    tokens.push_back(sampler);
  }

  void mov(uint32_t dst, uint32_t src) {
    tokens.push_back(kOpMov);
    tokens.push_back(dst);
    tokens.push_back(src);
  }

  void add(uint32_t dst, uint32_t a, uint32_t b) {
    tokens.push_back(kOpAdd);
    tokens.push_back(dst);
    tokens.push_back(a);
    tokens.push_back(b);
  }

  void mul(uint32_t dst, uint32_t a, uint32_t b) {
    tokens.push_back(kOpMul);
    tokens.push_back(dst);
    tokens.push_back(a);
    tokens.push_back(b);
  }

  void min(uint32_t dst, uint32_t a, uint32_t b) {
    tokens.push_back(kOpMin);
    tokens.push_back(dst);
    tokens.push_back(a);
    tokens.push_back(b);
  }

  void max(uint32_t dst, uint32_t a, uint32_t b) {
    tokens.push_back(kOpMax);
    tokens.push_back(dst);
    tokens.push_back(a);
    tokens.push_back(b);
  }

  void def_const(uint32_t reg, float x, float y, float z, float w) {
    tokens.push_back(kOpDef);
    tokens.push_back(dst_const(reg));
    tokens.push_back(f32_bits(x));
    tokens.push_back(f32_bits(y));
    tokens.push_back(f32_bits(z));
    tokens.push_back(f32_bits(w));
  }
};

uint32_t arg_src_token(const FixedfuncStageArg& arg, uint32_t current_reg) {
  const uint32_t swz = arg.alpha_replicate ? swizzle_wwww() : swizzle_xyzw();
  const SrcMod mod = arg.complement ? SrcMod::Comp : SrcMod::None;
  switch (arg.src) {
    case FixedfuncStageArgSrc::Current:
      return src_temp(current_reg, swz, mod);
    case FixedfuncStageArgSrc::Texture:
      // r0 contains texld result.
      return src_temp(/*reg=*/0, swz, mod);
    case FixedfuncStageArgSrc::TextureFactor:
      return src_const(/*reg=*/kFixedfuncStage0TextureFactorPsRegister, swz, mod);
    case FixedfuncStageArgSrc::Diffuse:
    default:
      return src_input(/*reg=*/0, swz, mod);
  }
}

bool build_stage_state(Builder& b, const FixedfuncStageState& st, uint32_t current_reg, uint32_t out_reg, uint32_t out_mask) {
  // Stage-state argument temporaries.
  constexpr uint32_t kArg1Reg = 1;
  constexpr uint32_t kArg2Reg = 2;
  constexpr uint32_t kTmpReg = 4;

  const auto materialize = [&](uint32_t dst_reg, const FixedfuncStageArg& arg) {
    b.mov(dst_temp(dst_reg, /*mask=*/0xFu), arg_src_token(arg, current_reg));
  };

  switch (st.op) {
    case FixedfuncStageOp::Disable:
      return true; // passthrough (already initialized by caller)

    case FixedfuncStageOp::SelectArg1:
      materialize(kArg1Reg, st.arg1);
      b.mov(dst_temp(out_reg, out_mask), src_temp(kArg1Reg));
      return true;

    case FixedfuncStageOp::SelectArg2:
      materialize(kArg2Reg, st.arg2);
      b.mov(dst_temp(out_reg, out_mask), src_temp(kArg2Reg));
      return true;

    case FixedfuncStageOp::Modulate:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.mul(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg));
      return true;

    case FixedfuncStageOp::Modulate2x:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.mul(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg));
      b.add(dst_temp(out_reg, out_mask), src_temp(out_reg), src_temp(out_reg));
      return true;

    case FixedfuncStageOp::Modulate4x:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.mul(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg));
      b.add(dst_temp(out_reg, out_mask), src_temp(out_reg), src_temp(out_reg));
      b.add(dst_temp(out_reg, out_mask), src_temp(out_reg), src_temp(out_reg));
      return true;

    case FixedfuncStageOp::Add:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.add(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg));
      return true;

    case FixedfuncStageOp::Subtract:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.add(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg, swizzle_xyzw(), SrcMod::Neg));
      return true;

    case FixedfuncStageOp::AddSigned:
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      b.add(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), src_temp(kArg2Reg));
      // Bias (x - 0.5) is used to implement ADDSIGNED without constant registers.
      b.mov(dst_temp(out_reg, out_mask), src_temp(out_reg, swizzle_xyzw(), SrcMod::Bias));
      return true;

    case FixedfuncStageOp::BlendTextureAlpha: {
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      const uint32_t alpha = src_temp(/*reg=*/0, swizzle_wwww(), SrcMod::None);
      const uint32_t inv_alpha = src_temp(/*reg=*/0, swizzle_wwww(), SrcMod::Comp);
      b.mul(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), alpha);
      b.mul(dst_temp(kTmpReg, out_mask), src_temp(kArg2Reg), inv_alpha);
      b.add(dst_temp(out_reg, out_mask), src_temp(out_reg), src_temp(kTmpReg));
      return true;
    }

    case FixedfuncStageOp::BlendDiffuseAlpha: {
      materialize(kArg1Reg, st.arg1);
      materialize(kArg2Reg, st.arg2);
      const uint32_t alpha = src_input(/*reg=*/0, swizzle_wwww(), SrcMod::None);
      const uint32_t inv_alpha = src_input(/*reg=*/0, swizzle_wwww(), SrcMod::Comp);
      b.mul(dst_temp(out_reg, out_mask), src_temp(kArg1Reg), alpha);
      b.mul(dst_temp(kTmpReg, out_mask), src_temp(kArg2Reg), inv_alpha);
      b.add(dst_temp(out_reg, out_mask), src_temp(out_reg), src_temp(kTmpReg));
      return true;
    }

    default:
      return false;
  }
}

bool build_fixedfunc_ps(const FixedfuncPixelShaderKey& key, std::vector<uint32_t>* out_tokens) {
  if (!out_tokens) {
    return false;
  }
  if (!key.supported) {
    return false;
  }

  // Builder methods use std::vector::push_back without per-call exception guards.
  // Wrap the entire emission in a catch-all so no std::bad_alloc (or other)
  // exceptions can escape driver code under memory pressure.
  try {
    Builder b;
    if (!b.begin()) {
      return false;
    }

    // r3 = diffuse (baseline current)
    constexpr uint32_t kCurReg = 3;
    b.mov(dst_temp(kCurReg, /*mask=*/0xFu), src_input(/*reg=*/0));

    for (uint32_t stage = 0; stage < key.stage_count && stage < kFixedfuncMaxTextureStages; ++stage) {
      const auto& st = key.stages[stage];

      // r0 = texN
      if (st.uses_texture) {
        b.texld(dst_temp(/*reg=*/0, /*mask=*/0xFu), src_texcoord(/*reg=*/0), src_sampler(stage));
      }

      // StageN RGB into r3.xyz (mask 0x7), alpha into r3.w (mask 0x8).
      if (!build_stage_state(b, st.color, kCurReg, kCurReg, /*mask=*/0x7u)) {
        return false;
      }
      if (!build_stage_state(b, st.alpha, kCurReg, kCurReg, /*mask=*/0x8u)) {
        return false;
      }
    }

    if (key.fog_enabled) {
      // Fixed-function fog is applied after the fixed-function texture stage chain
      // (blend the final RGB toward FOGCOLOR). This is implemented as:
      //
      //   fog_amount = saturate((fog_coord - fog_start) * inv_fog_range)
      //   rgb = lerp(rgb, fog_color, fog_amount)
      //
      // Where:
      // - `fog_coord` is sourced from TEXCOORD0.z, populated by the fixed-function
      //   fallback vertex shaders:
      //   - for XYZRHW vertices: input POSITIONT.z
      //   - for XYZ vertices: post-projection depth (clip_z / clip_w)
      // - `fog_start` and `inv_fog_range` are provided in `c2.{x,y}`
      // - `fog_color` is provided in `c1.rgb`
      //
      // Notes:
      // - Only RGB is fogged; alpha is preserved (D3D9 fixed-function semantics).
      // - We use a dedicated constant register `c3 = 0` (defined via `def`) so the
      //   clamp is deterministic regardless of any app-written PS constant state.

      constexpr uint32_t kFogColorReg = 1;  // c1
      constexpr uint32_t kFogParamsReg = 2; // c2
      constexpr uint32_t kZeroReg = 3;      // c3 (def 0)

      constexpr uint32_t kFogAmountReg = 5; // r5
      constexpr uint32_t kFogTmpReg = 6;    // r6

      b.def_const(kZeroReg, 0.0f, 0.0f, 0.0f, 0.0f);

      // r5 = fog_coord (replicate scalar)
      b.mov(dst_temp(kFogAmountReg, /*mask=*/0xFu), src_texcoord(/*reg=*/0, swizzle_zzzz()));
      // r5 = (fog_coord - fog_start) * inv_fog_range
      b.add(dst_temp(kFogAmountReg, /*mask=*/0xFu),
            src_temp(kFogAmountReg),
            src_const(kFogParamsReg, swizzle_xxxx(), SrcMod::Neg));
      b.mul(dst_temp(kFogAmountReg, /*mask=*/0xFu),
            src_temp(kFogAmountReg),
            src_const(kFogParamsReg, swizzle_yyyy()));

      // Clamp to [0, 1].
      b.max(dst_temp(kFogAmountReg, /*mask=*/0xFu), src_temp(kFogAmountReg), src_const(kZeroReg));
      b.min(dst_temp(kFogAmountReg, /*mask=*/0xFu),
            src_temp(kFogAmountReg),
            src_const(kZeroReg, swizzle_xyzw(), SrcMod::Comp));

      // r6.rgb = fog_color.rgb - cur.rgb
      b.add(dst_temp(kFogTmpReg, /*mask=*/0x7u),
            src_const(kFogColorReg),
            src_temp(kCurReg, swizzle_xyzw(), SrcMod::Neg));
      // r6.rgb *= fog_amount
      b.mul(dst_temp(kFogTmpReg, /*mask=*/0x7u),
            src_temp(kFogTmpReg),
            src_temp(kFogAmountReg, swizzle_xxxx()));
      // cur.rgb += r6.rgb
      b.add(dst_temp(kCurReg, /*mask=*/0x7u), src_temp(kCurReg), src_temp(kFogTmpReg));
    }

    b.mov(dst_oc0(/*mask=*/0xFu), src_temp(kCurReg));
    b.end();

    *out_tokens = std::move(b.tokens);
    return true;
  } catch (...) {
    out_tokens->clear();
    return false;
  }
}

} // namespace fixedfunc_ps20

bool fixedfunc_build_ps_bytes(const FixedfuncPixelShaderKey& key, std::vector<uint32_t>* out_words) {
  return fixedfunc_ps20::build_fixedfunc_ps(key, out_words);
}

HRESULT ensure_fixedfunc_pixel_shader_for_key_locked(Device* dev, const FixedfuncPixelShaderKey& key, Shader** ps_slot) {
  if (!dev || !ps_slot) {
    return E_FAIL;
  }
  if (!key.supported) {
    return kD3DErrInvalidCall;
  }
  const uint64_t sig = fixedfunc_ps_signature(key);

  if (key.uses_tfactor) {
    const HRESULT hr = ensure_fixedfunc_texture_factor_constant_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }
  }
  if (key.fog_enabled) {
    const HRESULT hr = ensure_fixedfunc_fog_constants_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }
  }

  // Fixed-function PS variants are cached per-device in `fixedfunc_ps_variants`
  // so toggling texture stage state does not spam CREATE_SHADER_DXBC/DESTROY_SHADER.
  Shader* desired_ps = nullptr;

  // Fast path: reuse a cached mapping from the stage-state signature to a
  // previously created shader variant. This avoids rebuilding the ps_2_0 token
  // stream on every draw when stage state is stable.
  if (const auto it = dev->fixedfunc_ps_variant_cache.find(sig);
      it != dev->fixedfunc_ps_variant_cache.end()) {
    desired_ps = it->second;
  }

  if (!desired_ps) {
    std::vector<uint32_t> ps_words;
    if (!fixedfunc_build_ps_bytes(key, &ps_words) || ps_words.empty()) {
      return E_OUTOFMEMORY;
    }
    const void* ps_bytes = ps_words.data();
    const uint32_t ps_size = static_cast<uint32_t>(ps_words.size() * sizeof(uint32_t));

    // Look for an existing cached PS with identical bytecode so different
    // signature keys that lower to the same shader can alias a single Shader*.
    for (Shader* ps : dev->fixedfunc_ps_variants) {
      if (!ps) {
        continue;
      }
      if (shader_bytecode_equals(ps, ps_bytes, ps_size)) {
        desired_ps = ps;
        break;
      }
    }

    if (!desired_ps) {
      desired_ps = create_internal_shader_locked(dev, kD3d9ShaderStagePs, ps_bytes, ps_size);
      if (!desired_ps) {
        return E_OUTOFMEMORY;
      }

      bool inserted = false;
      for (Shader*& slot : dev->fixedfunc_ps_variants) {
        if (!slot) {
          slot = desired_ps;
          inserted = true;
          break;
        }
      }

      // In the extremely unlikely event that a device uses more than the reserved
      // number of fixed-function variants, evict an unreferenced cached PS.
      if (!inserted) {
        auto ps_referenced = [&](Shader* s) -> bool {
          if (!s) {
            return false;
          }
          if (s == dev->ps || s == dev->fixedfunc_ps_interop) {
            return true;
          }
          for (size_t i = 0; i < static_cast<size_t>(FixedFuncVariant::COUNT); ++i) {
            if (dev->fixedfunc_pipelines[i].ps == s) {
              return true;
            }
          }
          return false;
        };

        for (Shader*& slot : dev->fixedfunc_ps_variants) {
          Shader* cand = slot;
          if (!cand) {
            continue;
          }
          if (ps_referenced(cand)) {
            continue;
          }

          (void)emit_destroy_shader_locked(dev, cand->handle);
          delete cand;

          // Drop any cached signature mappings that referenced the evicted shader
          // so lookups can't return a freed pointer.
          for (auto it = dev->fixedfunc_ps_variant_cache.begin();
               it != dev->fixedfunc_ps_variant_cache.end();) {
            if (it->second == cand) {
              it = dev->fixedfunc_ps_variant_cache.erase(it);
            } else {
              ++it;
            }
          }

          slot = desired_ps;
          inserted = true;
          break;
        }
      }

      if (!inserted) {
        // Unable to safely evict a cached variant; drop the new shader and fail.
        (void)emit_destroy_shader_locked(dev, desired_ps->handle);
        delete desired_ps;
        return E_OUTOFMEMORY;
      }
    }
  }

  // Cache the signature  shader mapping even if `desired_ps` came from the
  // bytecode-dedup search above (multiple signatures can alias the same Shader*).
  //
  // Keep the map bounded: `fixedfunc_ps_variants` is already capped (100), but a
  // pathological app could cycle through a large number of stage-state signatures
  // that all alias the same limited shader set. The map is a pure optimization,
  // so evicting it is always safe (it only causes more token rebuilding on
  // subsequent misses).
  constexpr size_t kMaxSignatureCacheEntries = 1024;
  if (dev->fixedfunc_ps_variant_cache.size() >= kMaxSignatureCacheEntries &&
      dev->fixedfunc_ps_variant_cache.find(sig) == dev->fixedfunc_ps_variant_cache.end()) {
    dev->fixedfunc_ps_variant_cache.clear();
    try {
      dev->fixedfunc_ps_variant_cache.reserve(128);
    } catch (...) {
      // Cache is best-effort; skip reserving on OOM.
    }
  }
  if (dev->fixedfunc_ps_variant_cache.empty()) {
    // First use: keep the signature cache small and avoid rehashing in the common
    // steady-state case where stage state doesn't churn.
    try {
      dev->fixedfunc_ps_variant_cache.reserve(128);
    } catch (...) {
      // Cache is best-effort; skip reserving on OOM.
    }
  }
  // Best-effort: skip caching on OOM so exceptions never escape driver code.
  try {
    auto [it, inserted] = dev->fixedfunc_ps_variant_cache.emplace(sig, desired_ps);
    if (!inserted) {
      it->second = desired_ps;
    }
  } catch (...) {
  }

  if (*ps_slot == desired_ps) {
    return S_OK;
  }

  Shader* old_ps = *ps_slot;
  *ps_slot = desired_ps;

  // If the PS we're replacing is currently bound (either via the full
  // fixed-function fallback or via shader-stage interop fallbacks where one
  // shader stage is NULL), rebind so the change takes effect immediately.
  const bool ps_bound = (old_ps != nullptr && dev->ps == old_ps);
  if (ps_bound) {
    Shader* prev_ps = dev->ps;
    dev->ps = desired_ps;
    // Defensive: the host executor rejects null shader binds; if the old PS is
    // bound we must have a VS too.
    if (!dev->vs || !emit_bind_shaders_locked(dev)) {
      dev->ps = prev_ps;
      *ps_slot = old_ps;
      return E_OUTOFMEMORY;
    }
  }

  return S_OK;
}

HRESULT ensure_fixedfunc_pixel_shader_locked(Device* dev, Shader** ps_slot) {
  if (!dev || !ps_slot) {
    return E_FAIL;
  }

  const FixedfuncPixelShaderKey key = fixedfunc_ps_key_locked(dev);
  if (!key.supported) {
    return kD3DErrInvalidCall;
  }

  return ensure_fixedfunc_pixel_shader_for_key_locked(dev, key, ps_slot);
}

} // namespace
static bool emit_set_shader_constants_f_locked(
    Device* dev,
    uint32_t stage,
    uint32_t start_reg,
    const float* data,
    uint32_t vec4_count) {
  if (!dev || !data || vec4_count == 0) {
    return false;
  }
  if (start_reg >= 256 || vec4_count > 256u - start_reg) {
    return false;
  }

  float* dst = (stage == kD3d9ShaderStageVs) ? dev->vs_consts_f : dev->ps_consts_f;
  std::memcpy(dst + start_reg * 4u, data, static_cast<size_t>(vec4_count) * 4u * sizeof(float));

  const size_t payload_size = static_cast<size_t>(vec4_count) * 4u * sizeof(float);
  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_f>(
      dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_F, data, payload_size);
  if (!cmd) {
    return false;
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage);
  cmd->start_register = start_reg;
  cmd->vec4_count = vec4_count;
  cmd->reserved0 = 0;
  return true;
}

static void d3d9_colorvalue_to_float4(const D3DCOLORVALUE& c, float out[4]) {
  out[0] = c.r;
  out[1] = c.g;
  out[2] = c.b;
  out[3] = c.a;
}

static void d3d9_argb_to_float4(uint32_t argb, float out[4]) {
  out[0] = static_cast<float>((argb >> 16) & 0xFFu) / 255.0f; // R
  out[1] = static_cast<float>((argb >> 8) & 0xFFu) / 255.0f;  // G
  out[2] = static_cast<float>((argb >> 0) & 0xFFu) / 255.0f;  // B
  out[3] = static_cast<float>((argb >> 24) & 0xFFu) / 255.0f; // A
}

static HRESULT ensure_fixedfunc_wvp_constants_locked(Device* dev) {
  if (!dev) {
    return E_FAIL;
  }
  if (!dev->fixedfunc_matrix_dirty) {
    dev->fixedfunc_matrix_force_upload = false;
    return S_OK;
  }

  float world_view[16] = {};
  float wvp[16] = {};
  d3d9_mul_mat4_row_major(dev->transform_matrices[kD3dTransformWorld0], dev->transform_matrices[kD3dTransformView], world_view);
  d3d9_mul_mat4_row_major(world_view, dev->transform_matrices[kD3dTransformProjection], wvp);

  // Upload WVP as column vectors (transpose of row-major).
  float cols[16] = {};
  for (uint32_t c = 0; c < 4; ++c) {
    cols[c * 4 + 0] = wvp[0 * 4 + c];
    cols[c * 4 + 1] = wvp[1 * 4 + c];
    cols[c * 4 + 2] = wvp[2 * 4 + c];
    cols[c * 4 + 3] = wvp[3 * 4 + c];
  }

  // If the computed WVP constants already match the cached VS constant range,
  // skip re-uploading them. This handles benign "dirty" triggers (e.g. setting
  // the same FVF repeatedly) while still fixing real clobbers: user shaders that
  // overwrite the reserved constant registers will update `vs_consts_f` and
  // therefore fail this comparison.
  const float* cached = dev->vs_consts_f + static_cast<size_t>(kFixedfuncMatrixStartRegister) * 4u;
  if (!dev->fixedfunc_matrix_force_upload && std::memcmp(cached, cols, sizeof(cols)) == 0) {
    dev->fixedfunc_matrix_dirty = false;
    return S_OK;
  }

  if (!emit_set_shader_constants_f_locked(dev,
                                          kD3d9ShaderStageVs,
                                          kFixedfuncMatrixStartRegister,
                                          cols,
                                          kFixedfuncMatrixVec4Count)) {
    return E_OUTOFMEMORY;
  }

  dev->fixedfunc_matrix_dirty = false;
  dev->fixedfunc_matrix_force_upload = false;
  return S_OK;
}

static HRESULT ensure_fixedfunc_lighting_constants_locked(Device* dev) {
  if (!dev) {
    return E_FAIL;
  }
  if (!dev->fixedfunc_lighting_dirty) {
    return S_OK;
  }

  float regs[kFixedfuncLightingVec4Count * 4u] = {};

  // Fixed-function D3D9 lighting is computed in view space. We therefore:
  // - transform normals and positions by world*view
  // - transform light directions/positions by view
  float world_view[16] = {};
  d3d9_mul_mat4_row_major(dev->transform_matrices[kD3dTransformWorld0],
                          dev->transform_matrices[kD3dTransformView],
                          world_view);

  // c208..c210: world*view columns 0..2 (w contains translation).
  for (uint32_t c = 0; c < 3; ++c) {
    regs[c * 4u + 0] = world_view[0 * 4u + c];
    regs[c * 4u + 1] = world_view[1 * 4u + c];
    regs[c * 4u + 2] = world_view[2 * 4u + c];
    regs[c * 4u + 3] = world_view[3 * 4u + c];
  }

  const auto write_reg = [&](uint32_t reg_index, const float v[4]) {
    if (reg_index < kFixedfuncLightingStartRegister) {
      return;
    }
    const uint32_t rel = reg_index - kFixedfuncLightingStartRegister;
    if (rel >= kFixedfuncLightingVec4Count) {
      return;
    }
    std::memcpy(&regs[rel * 4u], v, 4u * sizeof(float));
  };

  const float* view = dev->transform_matrices[kD3dTransformView];

  // Pack enabled lights into a bounded fixed-function subset so lighting state
  // changes don't cause shader variant churn. The lit VS unrolls a fixed number
  // of light slots and unused slots are represented by zero-valued constants.
  uint32_t dir_slot = 0;
  uint32_t point_slot = 0;

  for (uint32_t i = 0; i < Device::kMaxLights; ++i) {
    if (!dev->light_enabled[i] || !dev->light_valid[i]) {
      continue;
    }
    const D3DLIGHT9& l = dev->lights[i];

    if (l.Type == D3DLIGHT_DIRECTIONAL) {
      if (dir_slot >= kFixedfuncMaxDirectionalLights) {
        continue;
      }
      // Normalize direction (best-effort) and transform into view space.
      float dx = l.Direction.x;
      float dy = l.Direction.y;
      float dz = l.Direction.z;
      const float len2 = dx * dx + dy * dy + dz * dz;
      if (len2 > 0.0f) {
        const float inv_len = 1.0f / std::sqrt(len2);
        dx *= inv_len;
        dy *= inv_len;
        dz *= inv_len;
      }
      float dir_view_x = dx * view[0 * 4u + 0] + dy * view[1 * 4u + 0] + dz * view[2 * 4u + 0];
      float dir_view_y = dx * view[0 * 4u + 1] + dy * view[1 * 4u + 1] + dz * view[2 * 4u + 1];
      float dir_view_z = dx * view[0 * 4u + 2] + dy * view[1 * 4u + 2] + dz * view[2 * 4u + 2];

      // Shader expects vector from vertex to light, so negate D3D9's "light direction" (light rays).
      float dir_view[4] = {-dir_view_x, -dir_view_y, -dir_view_z, 0.0f};
      const float dir_len2 = dir_view[0] * dir_view[0] + dir_view[1] * dir_view[1] + dir_view[2] * dir_view[2];
      if (dir_len2 > 0.0f) {
        const float inv_len = 1.0f / std::sqrt(dir_len2);
        dir_view[0] *= inv_len;
        dir_view[1] *= inv_len;
        dir_view[2] *= inv_len;
      }

      float diffuse[4] = {};
      float ambient[4] = {};
      d3d9_colorvalue_to_float4(l.Diffuse, diffuse);
      d3d9_colorvalue_to_float4(l.Ambient, ambient);

      const uint32_t base = 211u + dir_slot * 3u;
      write_reg(base + 0u, dir_view);
      write_reg(base + 1u, diffuse);
      write_reg(base + 2u, ambient);

      ++dir_slot;
      continue;
    }

    if (l.Type == D3DLIGHT_POINT || l.Type == D3DLIGHT_SPOT) {
      if (point_slot >= kFixedfuncMaxPointLights) {
        continue;
      }

      // Transform position into view space (row-vector * view).
      const float px = l.Position.x;
      const float py = l.Position.y;
      const float pz = l.Position.z;
      const float pos_view_x =
          px * view[0 * 4u + 0] + py * view[1 * 4u + 0] + pz * view[2 * 4u + 0] + view[3 * 4u + 0];
      const float pos_view_y =
          px * view[0 * 4u + 1] + py * view[1 * 4u + 1] + pz * view[2 * 4u + 1] + view[3 * 4u + 1];
      const float pos_view_z =
          px * view[0 * 4u + 2] + py * view[1 * 4u + 2] + pz * view[2 * 4u + 2] + view[3 * 4u + 2];
      const float pos_view[4] = {pos_view_x, pos_view_y, pos_view_z, 1.0f};

      float diffuse[4] = {};
      float ambient[4] = {};
      d3d9_colorvalue_to_float4(l.Diffuse, diffuse);
      d3d9_colorvalue_to_float4(l.Ambient, ambient);

      // Bring-up subset: constant attenuation (1/att0) and range clamp based on dist^2.
      constexpr float kEps = 1e-6f;
      const float att0 = (l.Attenuation0 > kEps) ? l.Attenuation0 : 1.0f;
      const float inv_att0_s = 1.0f / att0;
      const float inv_att0[4] = {inv_att0_s, inv_att0_s, inv_att0_s, inv_att0_s};

      float inv_range2_s = 0.0f;
      if (l.Range > kEps) {
        const float r2 = l.Range * l.Range;
        inv_range2_s = (r2 > kEps) ? (1.0f / r2) : 0.0f;
      }
      const float inv_range2[4] = {inv_range2_s, inv_range2_s, inv_range2_s, inv_range2_s};

      const uint32_t base = 223u + point_slot * 5u;
      write_reg(base + 0u, pos_view);
      write_reg(base + 1u, diffuse);
      write_reg(base + 2u, ambient);
      write_reg(base + 3u, inv_att0);
      write_reg(base + 4u, inv_range2);

      ++point_slot;
      continue;
    }
  }

  // Material.
  float mat_diffuse[4] = {};
  float mat_ambient[4] = {};
  float mat_emissive[4] = {};
  if (dev->material_valid) {
    d3d9_colorvalue_to_float4(dev->material.Diffuse, mat_diffuse);
    d3d9_colorvalue_to_float4(dev->material.Ambient, mat_ambient);
    d3d9_colorvalue_to_float4(dev->material.Emissive, mat_emissive);
  }
  write_reg(233u, mat_diffuse);
  write_reg(234u, mat_ambient);
  write_reg(235u, mat_emissive);

  // Global ambient (D3DRS_AMBIENT, numeric value 26).
  float global_ambient[4] = {};
  constexpr uint32_t kD3dRsAmbient = 26u;
  d3d9_argb_to_float4(dev->render_states[kD3dRsAmbient], global_ambient);
  write_reg(236u, global_ambient);

  const float* cached = dev->vs_consts_f + static_cast<size_t>(kFixedfuncLightingStartRegister) * 4u;
  if (std::memcmp(cached, regs, sizeof(regs)) == 0) {
    dev->fixedfunc_lighting_dirty = false;
    return S_OK;
  }

  if (!emit_set_shader_constants_f_locked(dev,
                                          kD3d9ShaderStageVs,
                                          kFixedfuncLightingStartRegister,
                                          regs,
                                          kFixedfuncLightingVec4Count)) {
    return E_OUTOFMEMORY;
  }

  dev->fixedfunc_lighting_dirty = false;
  return S_OK;
}

struct FixedFuncShaderBytes {
  const void* bytes = nullptr;
  uint32_t size_bytes = 0;
};

struct FixedFuncVsTableEntry {
  FixedFuncShaderBytes base{};
  FixedFuncShaderBytes fog{};
  FixedFuncShaderBytes lit{};
  FixedFuncShaderBytes lit_fog{};
};

#define AEROGPU_FIXEDFUNC_VS_BYTES(sym) \
  FixedFuncShaderBytes { fixedfunc::sym, static_cast<uint32_t>(sizeof(fixedfunc::sym)) }

static constexpr FixedFuncVsTableEntry kFixedFuncVsTable[] = {
    /*NONE*/ {},
    /*RHW_COLOR*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosColor),
                   AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosColorFog)},
    /*RHW_COLOR_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosColorTex1),
                        AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosColorTex1Fog)},
    /*XYZ_COLOR*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosColor), AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosColorFog)},
    /*XYZ_COLOR_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosColorTex0), AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosColorTex0Fog)},
    /*RHW_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosWhiteTex1),
                  AEROGPU_FIXEDFUNC_VS_BYTES(kVsPassthroughPosWhiteTex1Fog)},
    /*XYZ_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsTransformPosWhiteTex1),
                  AEROGPU_FIXEDFUNC_VS_BYTES(kVsTransformPosWhiteTex1Fog)},
    /*XYZ_NORMAL*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalWhite),
                    AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalWhiteFog),
                    AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormal),
                    AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalFog)},
    /*XYZ_NORMAL_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalWhiteTex0),
                         AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalWhiteTex0Fog),
                         AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalTex1),
                         AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalTex1Fog)},
    /*XYZ_NORMAL_COLOR*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalDiffuse),
                          AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalDiffuseFog),
                          AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalDiffuse),
                          AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalDiffuseFog)},
    /*XYZ_NORMAL_COLOR_TEX1*/ {AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalDiffuseTex1),
                               AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpPosNormalDiffuseTex1Fog),
                               AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalDiffuseTex1),
                               AEROGPU_FIXEDFUNC_VS_BYTES(kVsWvpLitPosNormalDiffuseTex1Fog)},
};
static_assert(sizeof(kFixedFuncVsTable) / sizeof(kFixedFuncVsTable[0]) == static_cast<size_t>(FixedFuncVariant::COUNT),
              "kFixedFuncVsTable must match FixedFuncVariant::COUNT");

#undef AEROGPU_FIXEDFUNC_VS_BYTES

HRESULT ensure_fixedfunc_pipeline_locked(Device* dev) {
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }

  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
  if (variant == FixedFuncVariant::NONE) {
    return D3DERR_INVALIDCALL;
  }

  constexpr uint32_t kD3dRsLighting = 137u; // D3DRS_LIGHTING
  const bool lighting_enabled = dev->render_states[kD3dRsLighting] != 0;
  // Fixed-function lighting must only affect the full fixed-function fallback
  // pipeline (no user VS/PS bound). When user shaders are bound, keep lighting
  // state cached-only and avoid clobbering app constants.
  const bool full_fixedfunc = (dev->user_vs == nullptr && dev->user_ps == nullptr);
  const bool fixedfunc_lighting_active = lighting_enabled && full_fixedfunc;
  const bool fvf_xyzrhw = fixedfunc_fvf_is_xyzrhw(dev->fvf);

  // MVP behavior: if the app requests fixed-function lighting but the active FVF
  // does not carry normals, fail cleanly instead of silently falling back to the
  // unlit shader variants.
  //
  // Note: pre-transformed vertices (D3DFVF_XYZRHW*) are already lit; D3DRS_LIGHTING
  // is ignored by D3D9 for these FVFs, so do not reject them just because lighting
  // happens to be enabled.
  if (lighting_enabled && full_fixedfunc && !fvf_xyzrhw && !fixedfunc_fvf_has_normal(dev->fvf)) {
    return D3DERR_INVALIDCALL;
  }

  // Texture stage state is only relevant to the fixed-function *pixel* stage.
  // When a user pixel shader is bound (PS-only interop path), stage state must
  // not cause spurious INVALIDCALL failures.
  const bool need_fixedfunc_ps = (dev->user_ps == nullptr);
  FixedfuncPixelShaderKey ps_key{};
  if (need_fixedfunc_ps) {
    // Validate texture stage state before emitting any shader creation / bind
    // commands so unsupported fixed-function draws fail cleanly.
    ps_key = fixedfunc_ps_key_locked(dev);
    if (!ps_key.supported) {
      return D3DERR_INVALIDCALL;
    }
  }

  // Fixed-function fog is implemented by a fixed-function PS variant that reads
  // the fog coordinate from TEXCOORD0.z. When fog is enabled, some fixed-function
  // VS variants switch to dedicated fog versions that pack the fog coordinate
  // into TEXCOORD0.z.
  //
  // Important: keep PS-only interop stable. If a user PS is bound, we must not
  // change the fixed-function VS output layout by injecting fog coordinates.
  bool use_fog_vs = need_fixedfunc_ps && ps_key.fog_enabled;

  auto& pipe = dev->fixedfunc_pipelines[static_cast<size_t>(variant)];
  const bool needs_matrix = fixedfunc_fvf_needs_matrix(dev->fvf);
  const bool needs_lighting = fixedfunc_lighting_active && fixedfunc_fvf_has_normal(dev->fvf);

  // Ensure a matching internal vertex decl exists (used by SetFVF path).
  if (!pipe.vertex_decl) {
    const FixedFuncVariantDeclDesc* decl_desc = fixedfunc_decl_desc(variant);
    if (!decl_desc || !decl_desc->elems || decl_desc->elem_count == 0) {
      return E_FAIL;
    }
    const uint32_t decl_size = static_cast<uint32_t>(decl_desc->elem_count * sizeof(D3DVERTEXELEMENT9_COMPAT));
    pipe.vertex_decl = create_internal_vertex_decl_locked(dev, decl_desc->elems, decl_size);
    if (!pipe.vertex_decl) {
      return E_OUTOFMEMORY;
    }
  }

  const size_t variant_index = static_cast<size_t>(variant);
  if (variant_index >= static_cast<size_t>(FixedFuncVariant::COUNT)) {
    return D3DERR_INVALIDCALL;
  }
  const FixedFuncVsTableEntry& vs_desc = kFixedFuncVsTable[variant_index];

  // Some fixed-function vertex formats may not have dedicated fog-capable VS
  // variants. In that case, disable fog in the fixed-function PS as well so we
  // don't read an invalid fog coordinate from TEXCOORD0.z.
  //
  // Note: for the RHW variants, fog requires post-projection depth
  // (`clip_z / clip_w`), so the dedicated fog VS variants must explicitly divide
  // by w and pack the result into TEXCOORD0.z.
  if (use_fog_vs) {
    bool fog_coord_available =
        needs_lighting ? (vs_desc.lit_fog.bytes != nullptr) : (vs_desc.fog.bytes != nullptr);
    if (!fog_coord_available) {
      ps_key.fog_enabled = false;
      use_fog_vs = false;
    }
  }

  // Select a VS variant (priority: lit+fog > lit > fog > base).
  Shader** vs_slot = &pipe.vs;
  FixedFuncShaderBytes shader = vs_desc.base;
  if (needs_lighting && use_fog_vs && vs_desc.lit_fog.bytes) {
    vs_slot = &pipe.vs_lit_fog;
    shader = vs_desc.lit_fog;
  } else if (needs_lighting && vs_desc.lit.bytes) {
    vs_slot = &pipe.vs_lit;
    shader = vs_desc.lit;
  } else if (use_fog_vs && vs_desc.fog.bytes) {
    vs_slot = &pipe.vs_fog;
    shader = vs_desc.fog;
  }

  if (!vs_slot || !shader.bytes || shader.size_bytes == 0) {
    return E_FAIL;
  }

  if (!*vs_slot) {
    *vs_slot = create_internal_shader_locked(dev, kD3d9ShaderStageVs, shader.bytes, shader.size_bytes);
    if (!*vs_slot) {
      return E_OUTOFMEMORY;
    }
  }
  const Shader* const desired_vs = *vs_slot;

  if (need_fixedfunc_ps) {
    const HRESULT ps_hr = ensure_fixedfunc_pixel_shader_for_key_locked(dev, ps_key, &pipe.ps);
    if (FAILED(ps_hr)) {
      return ps_hr;
    }
  }

  // Ensure the FVF-derived declaration is bound when the app is using the
  // SetFVF path (internal declaration). When the app uses an explicit vertex
  // declaration (SetVertexDecl), preserve it so GetVertexDecl/state blocks
  // remain consistent.
  if (pipe.vertex_decl && (!dev->vertex_decl || dev->vertex_decl == pipe.vertex_decl)) {
    if (!emit_set_input_layout_locked(dev, pipe.vertex_decl)) {
      return E_OUTOFMEMORY;
    }
  }

  // Bind the fixed-function shaders iff the app did not set explicit shaders.
  if (!dev->user_vs && !dev->user_ps) {
    if (dev->vs != desired_vs || dev->ps != pipe.ps) {
      Shader* prev_vs = dev->vs;
      Shader* prev_ps = dev->ps;
      dev->vs = *vs_slot;
      dev->ps = pipe.ps;
      // Do not force `fixedfunc_matrix_dirty`/`fixedfunc_lighting_dirty` here:
      // other entrypoints already mark the constant ranges dirty on relevant
      // state changes (SetFVF / SetVertexDecl, SetTransform/MultiplyTransform,
      // SetLight/SetMaterial, user shader/constant writes that overlap the
      // reserved ranges). Forcing it here can cause redundant uploads because
      // SetTransform/MultiplyTransform may have already updated the fixed-function
      // constant range eagerly.
      if (!emit_bind_shaders_locked(dev)) {
        dev->vs = prev_vs;
        dev->ps = prev_ps;
        return E_OUTOFMEMORY;
      }
    }
  }

  if (needs_matrix) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }
  }
  if (needs_lighting) {
    const HRESULT hr = ensure_fixedfunc_lighting_constants_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }
  }

  return S_OK;
}

namespace {

Shader* fixedfunc_vs_variant_for_fvf_locked(const Device* dev) {
  if (!dev) {
    return nullptr;
  }
  constexpr uint32_t kD3dRsLighting = 137u; // D3DRS_LIGHTING
  const bool lighting_enabled = dev->render_states[kD3dRsLighting] != 0;
  const bool full_fixedfunc = (dev->user_vs == nullptr && dev->user_ps == nullptr);
  const bool fixedfunc_lighting_active = lighting_enabled && full_fixedfunc;

  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
  if (variant == FixedFuncVariant::NONE) {
    return nullptr;
  }
  const auto& pipe = dev->fixedfunc_pipelines[static_cast<size_t>(variant)];
  if (fixedfunc_lighting_active && fixedfunc_fvf_has_normal(dev->fvf) && pipe.vs_lit) {
    return pipe.vs_lit;
  }
  return pipe.vs;
}

HRESULT ensure_passthrough_pixel_shader_locked(Device* dev, Shader** out_ps) {
  if (!dev || !out_ps) {
    return E_INVALIDARG;
  }

  // This fallback PS is used only to keep the host command stream valid when we
  // cannot derive a fixed-function PS variant (e.g. unsupported stage state). It
  // must be independent of current stage state.
  FixedfuncPixelShaderKey key{};
  key.stage_count = 0;
  key.uses_tfactor = false;
  key.supported = true;

  Shader* ps = nullptr;
  const HRESULT hr = ensure_fixedfunc_pixel_shader_for_key_locked(dev, key, &ps);
  if (FAILED(hr)) {
    return hr;
  }
  *out_ps = ps;
  return S_OK;
}

HRESULT ensure_passthrough_shaders_locked(Device* dev, Shader** vs_out, Shader** ps_out) {
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }

  // Ensure a minimal internal shader pair exists so the UMD can always emit
  // non-null shader binds, even when the requested fixed-function/FVF state is
  // unsupported (draws will still fail at ensure_draw_pipeline_locked()).
  auto& pipe = dev->fixedfunc_pipelines[static_cast<size_t>(FixedFuncVariant::RHW_COLOR)];
  if (!pipe.vs) {
    pipe.vs = create_internal_shader_locked(
        dev,
        kD3d9ShaderStageVs,
        fixedfunc::kVsPassthroughPosColor,
        static_cast<uint32_t>(sizeof(fixedfunc::kVsPassthroughPosColor)));
    if (!pipe.vs) {
      return E_OUTOFMEMORY;
    }
  }

  Shader* passthrough_ps = nullptr;
  if (ps_out) {
    const HRESULT ps_hr = ensure_passthrough_pixel_shader_locked(dev, &passthrough_ps);
    if (FAILED(ps_hr)) {
      return ps_hr;
    }
  }

  if (vs_out) {
    *vs_out = pipe.vs;
  }
  if (ps_out) {
    *ps_out = passthrough_ps;
  }
  return S_OK;
}

HRESULT ensure_shader_bindings_locked(Device* dev, bool strict_draw_validation) {
  if (!dev) {
    return E_INVALIDARG;
  }

  Shader* desired_vs = nullptr;
  Shader* desired_ps = nullptr;

  // Mixed-pipeline interop:
  // - Both shaders set: bind directly.
  // - One shader set: bind the user shader for that stage and bind a fixed-function
  //   fallback shader for the missing stage.
  // - Neither shader set: fixed-function path (FVF-limited).
  if (dev->user_vs && dev->user_ps) {
    desired_vs = dev->user_vs;
    desired_ps = dev->user_ps;
  } else if (dev->user_vs && !dev->user_ps) {
    // VS-only: fixed-function PS fallback selected from texture stage state.
    desired_vs = dev->user_vs;
    Shader** ps_slot = &dev->fixedfunc_ps_interop;
    const HRESULT ps_hr = ensure_fixedfunc_pixel_shader_locked(dev, ps_slot);
    if (FAILED(ps_hr)) {
      if (strict_draw_validation) {
        // E_FAIL historically represented "unsupported fixed-function stage state";
        // map it to the D3D9-visible INVALIDCALL for draws.
        return (ps_hr == E_FAIL) ? kD3DErrInvalidCall : ps_hr;
      }
      // Non-draw bindings must tolerate unsupported fixed-function stage state so
      // state-setting (SetShader, state blocks, DestroyShader, etc) doesn't fail.
      if (ps_hr != kD3DErrInvalidCall) {
        return ps_hr;
      }
      Shader* fallback_ps = nullptr;
      const HRESULT fb_hr = ensure_passthrough_pixel_shader_locked(dev, &fallback_ps);
      if (FAILED(fb_hr)) {
        return fb_hr;
      }
      *ps_slot = fallback_ps;
      desired_ps = fallback_ps;
    } else {
      desired_ps = *ps_slot;
    }
  } else if (!dev->user_vs && dev->user_ps) {
    // PS-only: fixed-function VS fallback derived from current FVF/decl.
    desired_ps = dev->user_ps;

    const HRESULT ff_hr = ensure_fixedfunc_pipeline_locked(dev);
    if (FAILED(ff_hr)) {
      if (strict_draw_validation) {
        return ff_hr;
      }
      Shader* fallback_vs = nullptr;
      const HRESULT fb_hr = ensure_passthrough_shaders_locked(dev, &fallback_vs, nullptr);
      if (FAILED(fb_hr)) {
        return fb_hr;
      }
      desired_vs = fallback_vs;
    } else {
      desired_vs = fixedfunc_vs_variant_for_fvf_locked(dev);
    }
  } else {
    // Fixed-function path: requires a supported FVF subset.
    const HRESULT ff_hr = ensure_fixedfunc_pipeline_locked(dev);
    if (FAILED(ff_hr)) {
      if (strict_draw_validation) {
        return ff_hr;
      }
      Shader* fallback_vs = nullptr;
      Shader* fallback_ps = nullptr;
      const HRESULT fb_hr = ensure_passthrough_shaders_locked(dev, &fallback_vs, &fallback_ps);
      if (FAILED(fb_hr)) {
        return fb_hr;
      }
      desired_vs = fallback_vs;
      desired_ps = fallback_ps;
    } else {
      desired_vs = dev->vs;
      desired_ps = dev->ps;
    }
  }

  if (!desired_vs || !desired_ps) {
    if (strict_draw_validation) {
      return kD3DErrInvalidCall;
    }
    Shader* fallback_vs = nullptr;
    Shader* fallback_ps = nullptr;
    const HRESULT fb_hr = ensure_passthrough_shaders_locked(dev, &fallback_vs, &fallback_ps);
    if (FAILED(fb_hr)) {
      return fb_hr;
    }
    if (!desired_vs) {
      desired_vs = fallback_vs;
    }
    if (!desired_ps) {
      desired_ps = fallback_ps;
    }
  }

  // Defensive: the host command stream executor rejects null shader binds.
  if (!desired_vs || !desired_ps) {
    return strict_draw_validation ? kD3DErrInvalidCall : E_FAIL;
  }

  if (dev->vs != desired_vs || dev->ps != desired_ps) {
    Shader* prev_vs = dev->vs;
    Shader* prev_ps = dev->ps;
    dev->vs = desired_vs;
    dev->ps = desired_ps;
    if (!emit_bind_shaders_locked(dev)) {
      dev->vs = prev_vs;
      dev->ps = prev_ps;
      return E_OUTOFMEMORY;
    }
  }

  if (!dev->vs || !dev->ps) {
    return strict_draw_validation ? kD3DErrInvalidCall : E_FAIL;
  }

  return S_OK;
}

} // namespace

// Validates that a draw call would execute with a usable shader pipeline and
// ensures the AeroGPU command stream will observe a non-null VS+PS pair.
//
// This covers:
// - Fixed-function draws (no user shaders bound; FVF-limited).
// - Shader-stage interop (VS-only / PS-only): inject a fixed-function fallback
//   shader for the missing stage at draw time.
//
// This is primarily a robustness guard: when an app uses an unsupported
// fixed-function/FVF configuration, the UMD previously could emit draw packets
// with vs=0/ps=0, producing an invalid command stream. Instead, fail the draw
// deterministically at the D3D9 API boundary.
//
// Callers must hold `Device::mutex` and must invoke this *before* emitting any
// draw packets (AEROGPU_CMD_DRAW / AEROGPU_CMD_DRAW_INDEXED).
HRESULT ensure_draw_pipeline_locked(Device* dev) {
  if (!dev) {
    return E_INVALIDARG;
  }
  const HRESULT hr = ensure_shader_bindings_locked(dev, /*strict_draw_validation=*/true);
  if (FAILED(hr)) {
    return hr;
  }
  // Defensive: the host command stream executor rejects null shader binds.
  if (!dev->vs || !dev->ps) {
    return kD3DErrInvalidCall;
  }
  return S_OK;
}
HRESULT ensure_up_vertex_buffer_locked(Device* dev, uint32_t required_size) {
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }
  if (required_size == 0) {
    return E_INVALIDARG;
  }

  const uint32_t current_size = dev->up_vertex_buffer ? dev->up_vertex_buffer->size_bytes : 0;
  if (dev->up_vertex_buffer && current_size >= required_size) {
    return S_OK;
  }

  // Grow to the next power-of-two-ish size to avoid reallocating every draw.
  uint32_t new_size = current_size ? current_size : 4096u;
  while (new_size < required_size) {
    new_size = (new_size > (0x7FFFFFFFu / 2)) ? required_size : (new_size * 2);
  }

  auto vb = make_unique_nothrow<Resource>();
  if (!vb) {
    return E_OUTOFMEMORY;
  }
  vb->handle = allocate_global_handle(dev->adapter);
  vb->kind = ResourceKind::Buffer;
  vb->size_bytes = new_size;
  try {
    vb->storage.resize(new_size);
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  if (!emit_create_resource_locked(dev, vb.get())) {
    return E_OUTOFMEMORY;
  }

  Resource* old = dev->up_vertex_buffer;
  dev->up_vertex_buffer = vb.release();
  if (old) {
    (void)emit_destroy_resource_locked(dev, old->handle);
    delete old;
  }
  return S_OK;
}

HRESULT ensure_instancing_vertex_buffer_locked(Device* dev, uint32_t stream, uint32_t required_size) {
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }
  if (stream >= 16 || required_size == 0) {
    return E_INVALIDARG;
  }

  Resource*& slot = dev->instancing_vertex_buffers[stream];
  const uint32_t current_size = slot ? slot->size_bytes : 0;
  if (slot && current_size >= required_size) {
    return S_OK;
  }

  // Grow to the next power-of-two-ish size to avoid reallocating every draw.
  uint32_t new_size = current_size ? current_size : 4096u;
  while (new_size < required_size) {
    new_size = (new_size > (0x7FFFFFFFu / 2)) ? required_size : (new_size * 2);
  }

  auto vb = make_unique_nothrow<Resource>();
  if (!vb) {
    return E_OUTOFMEMORY;
  }
  vb->handle = allocate_global_handle(dev->adapter);
  vb->kind = ResourceKind::Buffer;
  vb->size_bytes = new_size;
  try {
    vb->storage.resize(new_size);
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  if (!emit_create_resource_locked(dev, vb.get())) {
    return E_OUTOFMEMORY;
  }

  Resource* old = slot;
  slot = vb.release();
  if (old) {
    (void)emit_destroy_resource_locked(dev, old->handle);
    delete old;
  }
  return S_OK;
}

HRESULT ensure_up_index_buffer_locked(Device* dev, uint32_t required_size) {
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }
  if (required_size == 0) {
    return E_INVALIDARG;
  }

  const uint32_t current_size = dev->up_index_buffer ? dev->up_index_buffer->size_bytes : 0;
  if (dev->up_index_buffer && current_size >= required_size) {
    return S_OK;
  }

  uint32_t new_size = current_size ? current_size : 2048u;
  while (new_size < required_size) {
    new_size = (new_size > (0x7FFFFFFFu / 2)) ? required_size : (new_size * 2);
  }

  auto ib = make_unique_nothrow<Resource>();
  if (!ib) {
    return E_OUTOFMEMORY;
  }
  ib->handle = allocate_global_handle(dev->adapter);
  ib->kind = ResourceKind::Buffer;
  ib->size_bytes = new_size;
  try {
    ib->storage.resize(new_size);
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  if (!emit_create_resource_locked(dev, ib.get())) {
    return E_OUTOFMEMORY;
  }

  Resource* old = dev->up_index_buffer;
  dev->up_index_buffer = ib.release();
  if (old) {
    (void)emit_destroy_resource_locked(dev, old->handle);
    delete old;
  }
  return S_OK;
}

HRESULT emit_upload_buffer_locked(Device* dev, Resource* res, const void* data, uint32_t size_bytes) {
  if (!dev || !res || !data || size_bytes == 0) {
    return E_INVALIDARG;
  }
  const bool is_buffer = (res->kind == ResourceKind::Buffer);

  if (res->backing_alloc_id != 0) {
    // Host-side validation rejects UPLOAD_RESOURCE for guest-backed resources.
    // Callers must update guest-backed buffers via Lock/Unlock + RESOURCE_DIRTY_RANGE.
    logf("aerogpu-d3d9: emit_upload_buffer_locked called on guest-backed resource handle=%u alloc_id=%u\n",
         static_cast<unsigned>(res->handle),
         static_cast<unsigned>(res->backing_alloc_id));
    return E_INVALIDARG;
  }

  // WebGPU buffer copies require 4-byte alignment. Pad uploads for buffer resources so
  // callers can upload D3D9-sized data (e.g. 3x u16 indices = 6 bytes) without
  // tripping host validation.
  const uint32_t aligned_size_bytes =
      is_buffer ? static_cast<uint32_t>(align_up(static_cast<size_t>(size_bytes), 4)) : size_bytes;

  if (aligned_size_bytes > res->size_bytes) {
    return E_INVALIDARG;
  }

  // Keep a CPU copy for debug/validation and for fixed-function emulation that
  // reads from buffers.
  if (res->storage.size() < aligned_size_bytes) {
    try {
      res->storage.resize(aligned_size_bytes);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
  }
  // Use memmove because some call sites may upload from memory already backed by
  // `res->storage` (overlapping ranges).
  std::memmove(res->storage.data(), data, size_bytes);
  if (aligned_size_bytes > size_bytes) {
    std::memset(res->storage.data() + size_bytes, 0, aligned_size_bytes - size_bytes);
  }

  const uint8_t* src = res->storage.data();
  uint32_t remaining = aligned_size_bytes;
  uint32_t cur_offset = 0;

  while (remaining) {
    // Ensure we can fit at least a minimal upload packet (header + N bytes).
    const size_t min_payload = is_buffer ? 4 : 1;
    const size_t min_needed = align_up(sizeof(aerogpu_cmd_upload_resource) + min_payload, 4);
    if (!ensure_cmd_space(dev, min_needed)) {
      return E_OUTOFMEMORY;
    }

    // Uploads write into the destination buffer. Track its backing allocation
    // so the KMD alloc table contains the mapping for guest-backed resources.
    // (For internal host-only buffers backing_alloc_id==0, this is a no-op.)
    HRESULT track_hr = track_resource_allocation_locked(dev, res, /*write=*/true);
    if (FAILED(track_hr)) {
      return track_hr;
    }

    // Allocation tracking may have split/flushed the submission; ensure we
    // still have room for at least a minimal upload packet before sizing the
    // next chunk.
    if (!ensure_cmd_space(dev, min_needed)) {
      return E_OUTOFMEMORY;
    }

    const size_t avail = dev->cmd.bytes_remaining();
    size_t chunk = 0;
    if (avail > sizeof(aerogpu_cmd_upload_resource)) {
      chunk = std::min<size_t>(remaining, avail - sizeof(aerogpu_cmd_upload_resource));
    }
    if (is_buffer) {
      chunk &= ~static_cast<size_t>(3);
      // If we can't fit a 4-byte-aligned chunk, force a split and retry.
      if (chunk == 0) {
        submit(dev);
        continue;
      }
    } else {
      while (chunk && align_up(sizeof(aerogpu_cmd_upload_resource) + chunk, 4) > avail) {
        chunk--;
      }
    }
    if (!chunk) {
      // Should only happen if the command buffer is extremely small; try a forced
      // submit and retry.
      submit(dev);
      continue;
    }

    auto* cmd = append_with_payload_locked<aerogpu_cmd_upload_resource>(
        dev, AEROGPU_CMD_UPLOAD_RESOURCE, src, chunk);
    if (!cmd) {
      return E_OUTOFMEMORY;
    }

    cmd->resource_handle = res->handle;
    cmd->reserved0 = 0;
    cmd->offset_bytes = cur_offset;
    cmd->size_bytes = chunk;

    src += chunk;
    cur_offset += static_cast<uint32_t>(chunk);
    remaining -= static_cast<uint32_t>(chunk);
  }
  return S_OK;
}

static HRESULT emit_upload_resource_range_locked(Device* dev, Resource* res, uint32_t offset, uint32_t size) {
  if (!dev || !res || !res->handle) {
    return E_INVALIDARG;
  }
  if (size == 0) {
    return S_OK;
  }
  if (res->backing_alloc_id != 0) {
    // Host-side validation rejects UPLOAD_RESOURCE for guest-backed resources.
    return E_INVALIDARG;
  }
  if (offset > res->size_bytes || size > res->size_bytes - offset) {
    return E_INVALIDARG;
  }

  const bool is_buffer = (res->kind == ResourceKind::Buffer);
  if (is_buffer) {
    uint32_t upload_offset = offset;
    uint32_t upload_size = size;
    // WebGPU buffer copies require 4-byte alignment.
    const uint32_t start = upload_offset & ~3u;
    const uint64_t end_u64 = static_cast<uint64_t>(upload_offset) + static_cast<uint64_t>(upload_size);
    const uint32_t end = static_cast<uint32_t>((end_u64 + 3ull) & ~3ull);
    if (end > res->size_bytes || end < start) {
      return E_INVALIDARG;
    }
    upload_offset = start;
    upload_size = end - start;

    const size_t end_bytes = static_cast<size_t>(upload_offset) + static_cast<size_t>(upload_size);
    if (res->storage.size() < end_bytes) {
      return E_FAIL;
    }

    const uint8_t* src = res->storage.data() + upload_offset;
    uint32_t remaining = upload_size;
    uint32_t cur_offset = upload_offset;

    while (remaining) {
      const size_t min_payload = 4;
      const size_t min_needed = align_up(sizeof(aerogpu_cmd_upload_resource) + min_payload, 4);
      if (!ensure_cmd_space(dev, min_needed)) {
        return E_OUTOFMEMORY;
      }

      // Uploads write into the resource. Track its backing allocation so the
      // KMD/emulator can resolve the destination memory via the per-submit alloc
      // table even though we keep the patch-location list empty.
      const HRESULT track_hr = track_resource_allocation_locked(dev, res, /*write=*/true);
      if (FAILED(track_hr)) {
        return track_hr;
      }

      // Allocation tracking may have split/flushed the submission; ensure we
      // still have room for at least a minimal upload packet before sizing the
      // next chunk.
      if (!ensure_cmd_space(dev, min_needed)) {
        return E_OUTOFMEMORY;
      }

      const size_t avail = dev->cmd.bytes_remaining();
      size_t chunk = 0;
      if (avail > sizeof(aerogpu_cmd_upload_resource)) {
        chunk = std::min<size_t>(remaining, avail - sizeof(aerogpu_cmd_upload_resource));
      }

      chunk &= ~static_cast<size_t>(3);
      if (!chunk) {
        submit(dev);
        continue;
      }

      auto* cmd = append_with_payload_locked<aerogpu_cmd_upload_resource>(
          dev, AEROGPU_CMD_UPLOAD_RESOURCE, src, chunk);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }

      cmd->resource_handle = res->handle;
      cmd->reserved0 = 0;
      cmd->offset_bytes = cur_offset;
      cmd->size_bytes = chunk;

      src += chunk;
      cur_offset += static_cast<uint32_t>(chunk);
      remaining -= static_cast<uint32_t>(chunk);
    }

    return S_OK;
  }

  // Texture uploads must be aligned to whole rows within the destination
  // subresource. The host executor validates that UPLOAD_RESOURCE ranges for
  // textures start/end on row boundaries (multiples of row_pitch).
  const uint64_t range_start = static_cast<uint64_t>(offset);
  const uint64_t range_end = range_start + static_cast<uint64_t>(size);

  const uint32_t array_layers = std::max(1u, res->depth);

  auto align_up_to_multiple = [](uint64_t v, uint64_t a, uint64_t* out) -> bool {
    if (!out || a == 0) {
      return false;
    }
    const uint64_t rem = v % a;
    if (rem == 0) {
      *out = v;
      return true;
    }
    const uint64_t add = a - rem;
    if (v > std::numeric_limits<uint64_t>::max() - add) {
      return false;
    }
    *out = v + add;
    return true;
  };

  uint64_t cur = range_start;
  while (cur < range_end) {
    Texture2dSubresourceLayout sub{};
    if (!calc_texture2d_subresource_layout_for_offset(
            res->format,
            res->width,
            res->height,
            res->mip_levels,
            array_layers,
            cur,
            &sub)) {
      return E_INVALIDARG;
    }

    const uint64_t sub_start = sub.subresource_start_bytes;
    const uint64_t sub_end = sub.subresource_end_bytes;
    if (sub_start >= sub_end) {
      return E_INVALIDARG;
    }

    const uint64_t inter_start = std::max(cur, sub_start);
    const uint64_t inter_end = std::min(range_end, sub_end);
    if (inter_start >= inter_end) {
      cur = inter_end;
      continue;
    }

    const uint64_t row_pitch = static_cast<uint64_t>(sub.row_pitch_bytes);
    if (row_pitch == 0) {
      return E_INVALIDARG;
    }

    const uint64_t rel_start = inter_start - sub_start;
    const uint64_t rel_end = inter_end - sub_start;
    const uint64_t aligned_rel_start = (rel_start / row_pitch) * row_pitch;

    uint64_t aligned_rel_end = 0;
    if (!align_up_to_multiple(rel_end, row_pitch, &aligned_rel_end)) {
      return E_INVALIDARG;
    }

    const uint64_t aligned_start = sub_start + aligned_rel_start;
    uint64_t aligned_end = sub_start + aligned_rel_end;
    if (aligned_end > sub_end) {
      aligned_end = sub_end;
    }
    if (aligned_end <= aligned_start) {
      return E_INVALIDARG;
    }

    const size_t aligned_end_sz = static_cast<size_t>(aligned_end);
    if (res->storage.size() < aligned_end_sz) {
      return E_FAIL;
    }

    const size_t row_pitch_sz = static_cast<size_t>(row_pitch);
    const size_t min_needed = align_up(sizeof(aerogpu_cmd_upload_resource) + row_pitch_sz, 4);

    const uint8_t* src = res->storage.data() + static_cast<size_t>(aligned_start);
    size_t remaining = static_cast<size_t>(aligned_end - aligned_start);
    uint64_t cur_offset = aligned_start;

    while (remaining) {
      if (!ensure_cmd_space(dev, min_needed)) {
        return E_OUTOFMEMORY;
      }

      const HRESULT track_hr = track_resource_allocation_locked(dev, res, /*write=*/true);
      if (FAILED(track_hr)) {
        return track_hr;
      }

      if (!ensure_cmd_space(dev, min_needed)) {
        return E_OUTOFMEMORY;
      }

      const size_t avail = dev->cmd.bytes_remaining();
      size_t chunk = 0;
      if (avail > sizeof(aerogpu_cmd_upload_resource)) {
        chunk = std::min<size_t>(remaining, avail - sizeof(aerogpu_cmd_upload_resource));
      }
      // Account for 4-byte alignment padding at the end of the packet.
      while (chunk && align_up(sizeof(aerogpu_cmd_upload_resource) + chunk, 4) > avail) {
        chunk--;
      }

      // Ensure chunk is a whole number of rows.
      chunk = (chunk / row_pitch_sz) * row_pitch_sz;

      if (!chunk) {
        submit(dev);
        continue;
      }

      auto* cmd = append_with_payload_locked<aerogpu_cmd_upload_resource>(
          dev, AEROGPU_CMD_UPLOAD_RESOURCE, src, chunk);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }

      cmd->resource_handle = res->handle;
      cmd->reserved0 = 0;
      cmd->offset_bytes = cur_offset;
      cmd->size_bytes = chunk;

      src += chunk;
      cur_offset += static_cast<uint64_t>(chunk);
      remaining -= chunk;
    }

    cur = inter_end;
  }

  return S_OK;
}

float read_f32_unaligned(const uint8_t* p) {
  float v = 0.0f;
  std::memcpy(&v, p, sizeof(v));
  return v;
}

void write_f32_unaligned(uint8_t* p, float v) {
  std::memcpy(p, &v, sizeof(v));
}

uint64_t fnv1a64_hash(const uint8_t* data, size_t len) {
  // 64-bit FNV-1a (deterministic across platforms).
  uint64_t h = 14695981039346656037ull;
  for (size_t i = 0; i < len; ++i) {
    h ^= static_cast<uint64_t>(data[i]);
    h *= 1099511628211ull;
  }
  return h;
}

void get_viewport_dims_locked(Device* dev, float* out_x, float* out_y, float* out_w, float* out_h) {
  const D3DDDIVIEWPORTINFO vp = viewport_effective_locked(dev);
  *out_x = vp.X;
  *out_y = vp.Y;
  *out_w = vp.Width;
  *out_h = vp.Height;
}

HRESULT convert_xyzrhw_to_clipspace_locked(
    Device* dev,
    const void* src_vertices,
    uint32_t stride_bytes,
    uint32_t vertex_count,
    std::vector<uint8_t>* out_bytes) {
  if (!out_bytes) {
    return E_INVALIDARG;
  }
  out_bytes->clear();
  if (!dev || !src_vertices || stride_bytes < 20 || vertex_count == 0) {
    return E_INVALIDARG;
  }

  float vp_x = 0.0f;
  float vp_y = 0.0f;
  float vp_w = 1.0f;
  float vp_h = 1.0f;
  get_viewport_dims_locked(dev, &vp_x, &vp_y, &vp_w, &vp_h);
  // Defensive: viewport dimensions are used as divisors for XYZRHW -> NDC
  // conversion. Some runtimes may pass garbage state; ensure we keep conversion
  // math finite.
  if (!std::isfinite(vp_x)) {
    vp_x = 0.0f;
  }
  if (!std::isfinite(vp_y)) {
    vp_y = 0.0f;
  }
  if (!std::isfinite(vp_w) || vp_w == 0.0f) {
    vp_w = 1.0f;
  }
  if (!std::isfinite(vp_h) || vp_h == 0.0f) {
    vp_h = 1.0f;
  }

  const uint64_t total_bytes_u64 = static_cast<uint64_t>(stride_bytes) * static_cast<uint64_t>(vertex_count);
  if (total_bytes_u64 == 0 || total_bytes_u64 > 0x7FFFFFFFu) {
    return E_INVALIDARG;
  }
  try {
    out_bytes->resize(static_cast<size_t>(total_bytes_u64));
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  const uint8_t* src_base = reinterpret_cast<const uint8_t*>(src_vertices);
  uint8_t* dst_base = out_bytes->data();

  for (uint32_t i = 0; i < vertex_count; i++) {
    const uint8_t* src = src_base + static_cast<size_t>(i) * stride_bytes;
    uint8_t* dst = dst_base + static_cast<size_t>(i) * stride_bytes;

    // Preserve any trailing fields (diffuse color etc).
    std::memcpy(dst, src, stride_bytes);

    float x = read_f32_unaligned(src + 0);
    float y = read_f32_unaligned(src + 4);
    float z = read_f32_unaligned(src + 8);
    const float rhw = read_f32_unaligned(src + 12);
    // Keep coordinates finite. If the app provides NaN/Inf XYZRHW positions,
    // default to the center of the effective viewport (0,0 in NDC).
    if (!std::isfinite(x)) {
      x = vp_x + vp_w * 0.5f - 0.5f;
    }
    if (!std::isfinite(y)) {
      y = vp_y + vp_h * 0.5f - 0.5f;
    }
    if (!std::isfinite(z)) {
      z = 0.0f;
    }

    // `rhw` is the reciprocal clip-space w. Some apps (and fuzz tests) may pass
    // non-finite values; keep conversion math finite so downstream shader math
    // does not get poisoned by NaNs/Infs.
    float w = 1.0f;
    if (rhw != 0.0f && std::isfinite(rhw)) {
      w = 1.0f / rhw;
      if (!std::isfinite(w)) {
        w = 1.0f;
      }
    }
    // D3D9's viewport transform uses a -0.5 pixel center convention. Invert it
    // so typical D3D9 pre-transformed vertex coordinates line up with pixel
    // centers.
    float ndc_x = ((x + 0.5f - vp_x) / vp_w) * 2.0f - 1.0f;
    float ndc_y = 1.0f - ((y + 0.5f - vp_y) / vp_h) * 2.0f;
    float ndc_z = z;
    if (!std::isfinite(ndc_x)) {
      ndc_x = 0.0f;
    }
    if (!std::isfinite(ndc_y)) {
      ndc_y = 0.0f;
    }
    if (!std::isfinite(ndc_z)) {
      ndc_z = 0.0f;
    }

    float clip_x = ndc_x * w;
    float clip_y = ndc_y * w;
    float clip_z = ndc_z * w;
    if (!std::isfinite(clip_x)) {
      clip_x = 0.0f;
    }
    if (!std::isfinite(clip_y)) {
      clip_y = 0.0f;
    }
    if (!std::isfinite(clip_z)) {
      clip_z = 0.0f;
    }

    write_f32_unaligned(dst + 0, clip_x);
    write_f32_unaligned(dst + 4, clip_y);
    write_f32_unaligned(dst + 8, clip_z);
    write_f32_unaligned(dst + 12, w);
  }
  return S_OK;
}

namespace {

struct ProcessVerticesDeclInfo {
  uint32_t stride_bytes = 0;
  // Position is expected to be a float4 (XYZRHW) written by ProcessVertices.
  uint32_t pos_offset = 0;
  bool has_pos = false;
  uint32_t diffuse_offset = 0;
  bool has_diffuse = false;
  uint32_t tex0_offset = 0;
  uint32_t tex0_size_bytes = 0;
  bool has_tex0 = false;
};

uint32_t d3d9_decl_type_size_bytes(uint8_t type) {
  switch (type) {
    // D3DDECLTYPE_* values (d3d9types.h). Keep local sizes so we can infer
    // strides on non-Windows hosts and for header vintages that don't provide
    // complete declarations.
    case 0: // FLOAT1
      return 4u;
    case kD3dDeclTypeFloat4:
      return 16u;
    case kD3dDeclTypeFloat2:
      return 8u;
    case kD3dDeclTypeFloat3:
      return 12u;
    case kD3dDeclTypeD3dColor:
      return 4u;
    case 5: // UBYTE4
      return 4u;
    case 6: // SHORT2
      return 4u;
    case 7: // SHORT4
      return 8u;
    case 8: // UBYTE4N
      return 4u;
    case 9: // SHORT2N
      return 4u;
    case 10: // SHORT4N
      return 8u;
    case 11: // USHORT2N
      return 4u;
    case 12: // USHORT4N
      return 8u;
    case 13: // UDEC3
      return 4u;
    case 14: // DEC3N
      return 4u;
    case 15: // FLOAT16_2
      return 4u;
    case 16: // FLOAT16_4
      return 8u;
    default:
      return 0u;
  }
}

bool parse_process_vertices_dest_decl(const VertexDecl* decl, ProcessVerticesDeclInfo* out) {
  if (!out) {
    return false;
  }
  *out = {};
  if (!decl || decl->blob.empty()) {
    return false;
  }
  if ((decl->blob.size() % sizeof(D3DVERTEXELEMENT9_COMPAT)) != 0) {
    return false;
  }

  const auto* elems = reinterpret_cast<const D3DVERTEXELEMENT9_COMPAT*>(decl->blob.data());
  const size_t count = decl->blob.size() / sizeof(D3DVERTEXELEMENT9_COMPAT);
  uint32_t stride = 0;

  // First pass: compute the inferred destination stride. ProcessVertices writes a
  // single destination buffer (stream 0); ignore declaration elements in other
  // streams.
  for (size_t i = 0; i < count; ++i) {
    const auto& e = elems[i];
    if (e.Stream == 0xFF && e.Type == kD3dDeclTypeUnused) {
      break;
    }

    const uint32_t elem_size = d3d9_decl_type_size_bytes(e.Type);
    if (elem_size == 0) {
      continue;
    }
    if (e.Stream == 0) {
      const uint32_t end = static_cast<uint32_t>(e.Offset) + elem_size;
      stride = std::max(stride, end);
    }
  }

  // Second pass: locate output elements we care about.
  bool pos_is_positiont = false;
  for (size_t i = 0; i < count; ++i) {
    const auto& e = elems[i];
    if (e.Stream == 0xFF && e.Type == kD3dDeclTypeUnused) {
      break;
    }
    if (e.Stream != 0) {
      continue;
    }

    const uint32_t elem_size = d3d9_decl_type_size_bytes(e.Type);
    if (elem_size == 0) {
      continue;
    }

    // Position is expected to be a float4 (XYZRHW).
    //
    // Some runtimes use Usage=POSITION (0) for position in synthesized decls.
    // However, other runtimes also appear to leave TEXCOORD0 Usage as POSITION.
    // Avoid confusing TEX0 with the actual position element by preferring
    // POSITIONT when present, otherwise selecting the POSITION float4 with the
    // smallest offset.
    if (e.Type == kD3dDeclTypeFloat4 && e.UsageIndex == 0 &&
        (e.Usage == kD3dDeclUsagePositionT || e.Usage == kD3dDeclUsagePosition)) {
      if (e.Usage == kD3dDeclUsagePositionT) {
        if (!out->has_pos || !pos_is_positiont || e.Offset < out->pos_offset) {
          out->has_pos = true;
          out->pos_offset = e.Offset;
          pos_is_positiont = true;
        }
      } else if (!pos_is_positiont) {
        if (!out->has_pos || e.Offset < out->pos_offset) {
          out->has_pos = true;
          out->pos_offset = e.Offset;
        }
      }
      continue;
    }
    if (e.Usage == kD3dDeclUsageColor && e.Type == kD3dDeclTypeD3dColor && e.UsageIndex == 0) {
      out->has_diffuse = true;
      out->diffuse_offset = e.Offset;
      continue;
    }
  }

  // Third pass: locate TEXCOORD0.
  //
  // Some runtimes appear to leave Usage as 0 (POSITION) when synthesizing fixed-
  // function decls. Be permissive and accept Usage=POSITION for TEXCOORD0 as long
  // as it is not the position element itself.
  if (out->has_pos) {
    for (size_t i = 0; i < count; ++i) {
      const auto& e = elems[i];
      if (e.Stream == 0xFF && e.Type == kD3dDeclTypeUnused) {
        break;
      }
      if (e.Stream != 0) {
        continue;
      }

      const uint32_t elem_size = d3d9_decl_type_size_bytes(e.Type);
      if (elem_size == 0) {
        continue;
      }

      if (e.UsageIndex != 0) {
        continue;
      }
      if (e.Usage != kD3dDeclUsageTexCoord && e.Usage != kD3dDeclUsagePosition) {
        continue;
      }
      if (e.Offset == out->pos_offset) {
        // Position element.
        continue;
      }
      if (e.Type != kD3dDeclTypeFloat1 && e.Type != kD3dDeclTypeFloat2 && e.Type != kD3dDeclTypeFloat3 &&
          e.Type != kD3dDeclTypeFloat4) {
        continue;
      }
      out->has_tex0 = true;
      out->tex0_offset = e.Offset;
      out->tex0_size_bytes = elem_size;
      break;
    }
  }

  out->stride_bytes = stride;
  return out->has_pos && out->stride_bytes != 0;
}

void d3d9_mul_vec4_mat4_row_major(const float v[4], const float m[16], float out[4]) {
  out[0] = v[0] * m[0] + v[1] * m[4] + v[2] * m[8] + v[3] * m[12];
  out[1] = v[0] * m[1] + v[1] * m[5] + v[2] * m[9] + v[3] * m[13];
  out[2] = v[0] * m[2] + v[1] * m[6] + v[2] * m[10] + v[3] * m[14];
  out[3] = v[0] * m[3] + v[1] * m[7] + v[2] * m[11] + v[3] * m[15];
}

} // namespace

HRESULT AEROGPU_D3D9_CALL device_process_vertices_internal(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_PROCESSVERTICES* pProcessVertices) {
  if (!hDevice.pDrvPrivate || !pProcessVertices) {
    return E_INVALIDARG;
  }

  auto* dev = as_device(hDevice);
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }

  // D3DPV_* flags passed through from IDirect3DDevice9::ProcessVertices.
  //
  // D3DPV_DONOTCOPYDATA tells the driver to avoid copying non-position output
  // elements (e.g. DIFFUSE/TEX). Preserve destination bytes for those fields.
  constexpr uint32_t kD3dPvDoNotCopyData = 0x1u;
  const bool do_not_copy_data = (pProcessVertices->Flags & kD3dPvDoNotCopyData) != 0;

  const uint32_t vertex_count = pProcessVertices->VertexCount;
  if (vertex_count == 0) {
    return S_OK;
  }

  // Only handle a small fixed-function subset here; allow the main ProcessVertices
  // path to fall back to memcpy for other cases.
  // ProcessVertices runs the *vertex* pipeline only. Pixel shaders do not affect
  // the output, so treat the call as fixed-function as long as no user vertex
  // shader is bound (even if a pixel shader is set for later draws).
  const bool fixedfunc = (!dev->user_vs);
  // D3DFVF_TEXCOORDSIZE* bits affect vertex layout but not the "kind" of FVF.
  // Mask them out for supported-FVF detection and then decode texcoord sizes
  // explicitly when copying TEX0.
  const uint32_t fvf = dev->fvf;
  const uint32_t fvf_base = fvf & ~kD3dFvfTexCoordSizeMask;
  const FixedFuncVariant base_variant = fixedfunc_variant_from_fvf(fvf_base);
  const bool src_xyz_plain = (fvf_base == kD3dFvfXyz);
  const bool src_xyzw_plain = (fvf_base == kD3dFvfXyzw);
  const bool src_xyzrhw_plain = (fvf_base == kD3dFvfXyzRhw);
  const bool src_xyz_diffuse = (base_variant == FixedFuncVariant::XYZ_COLOR);
  const bool src_xyzw_diffuse = (fvf_base == (kD3dFvfXyzw | kD3dFvfDiffuse));
  const bool src_xyz_diffuse_tex1 = (base_variant == FixedFuncVariant::XYZ_COLOR_TEX1);
  const bool src_xyzw_diffuse_tex1 = (fvf_base == (kD3dFvfXyzw | kD3dFvfDiffuse | kD3dFvfTex1));
  const bool src_xyz_tex1 = (base_variant == FixedFuncVariant::XYZ_TEX1);
  const bool src_xyzw_tex1 = (fvf_base == (kD3dFvfXyzw | kD3dFvfTex1));
  const bool src_xyzrhw_diffuse = (base_variant == FixedFuncVariant::RHW_COLOR);
  const bool src_xyzrhw_diffuse_tex1 = (base_variant == FixedFuncVariant::RHW_COLOR_TEX1);
  const bool src_xyzrhw_tex1 = (base_variant == FixedFuncVariant::RHW_TEX1);
  const bool src_xyzrhw = (src_xyzrhw_plain || src_xyzrhw_diffuse || src_xyzrhw_diffuse_tex1 || src_xyzrhw_tex1);
  const bool src_xyzw = (src_xyzw_plain || src_xyzw_diffuse || src_xyzw_diffuse_tex1 || src_xyzw_tex1);
  // Note: XYZRHW inputs are already pre-transformed. We still handle a minimal
  // subset here to support fixed-function default values (e.g. diffuse=white
  // when requested by the destination decl) and basic TEX0 relocation.
  //
  // For deterministic output, the fixed-function subset clears the full
  // destination vertex stride (including padding / extra decl elements, e.g.
  // TEX0 present in the destination decl but not written by the source FVF) when
  // D3DPV_DONOTCOPYDATA is not set.
  //
  // When D3DPV_DONOTCOPYDATA is set, the UMD writes only POSITIONT and preserves
  // all other destination bytes.
  if (!(fixedfunc &&
        (src_xyzrhw || src_xyz_plain || src_xyzw || src_xyz_diffuse || src_xyzw_diffuse || src_xyz_diffuse_tex1 ||
         src_xyzw_diffuse_tex1 || src_xyz_tex1 || src_xyzw_tex1))) {
    return D3DERR_NOTAVAILABLE;
  }

  const uint32_t src_start = pProcessVertices->SrcStartIndex;
  const uint32_t dest_index = pProcessVertices->DestIndex;

  Resource* dst_res = as_resource(pProcessVertices->hDestBuffer);
  if (!dst_res) {
    return E_INVALIDARG;
  }

  // Source vertices are read from stream 0 (per D3D9 ProcessVertices contract).
  DeviceStateStream& ss = dev->streams[0];
  if (!ss.vb || ss.stride_bytes == 0) {
    return E_INVALIDARG;
  }
  Resource* src_res = ss.vb;
  const uint32_t src_stride = ss.stride_bytes;

  // Destination layout is described by the provided vertex declaration.
  VertexDecl* dst_decl = as_vertex_decl(pProcessVertices->hVertexDecl);
  if (!dst_decl) {
    return kD3DErrInvalidCall;
  }

  ProcessVerticesDeclInfo dst_layout{};
  if (!parse_process_vertices_dest_decl(dst_decl, &dst_layout)) {
    return kD3DErrInvalidCall;
  }
  uint32_t dst_stride = dst_layout.stride_bytes;
  if constexpr (aerogpu_d3d9_has_member_DestStride<D3DDDIARG_PROCESSVERTICES>::value) {
    if (pProcessVertices->DestStride != 0) {
      dst_stride = pProcessVertices->DestStride;
    }
  }
  if (dst_stride < dst_layout.stride_bytes) {
    return kD3DErrInvalidCall;
  }

  // Bounds checks (source and destination).
  const uint64_t src_offset_u64 =
      static_cast<uint64_t>(ss.offset_bytes) + static_cast<uint64_t>(src_start) * static_cast<uint64_t>(src_stride);
  const uint64_t src_size_u64 = static_cast<uint64_t>(vertex_count) * static_cast<uint64_t>(src_stride);
  const uint64_t src_capacity_u64 =
      src_res->size_bytes ? static_cast<uint64_t>(src_res->size_bytes) : static_cast<uint64_t>(src_res->storage.size());
  if (src_offset_u64 > src_capacity_u64 || src_size_u64 > src_capacity_u64 - src_offset_u64) {
    return E_INVALIDARG;
  }

  const uint64_t dst_offset_u64 = static_cast<uint64_t>(dest_index) * static_cast<uint64_t>(dst_stride);
  const uint64_t dst_size_u64 = static_cast<uint64_t>(vertex_count) * static_cast<uint64_t>(dst_stride);
  const uint64_t dst_capacity_u64 =
      dst_res->size_bytes ? static_cast<uint64_t>(dst_res->size_bytes) : static_cast<uint64_t>(dst_res->storage.size());
  if (dst_offset_u64 > dst_capacity_u64 || dst_size_u64 > dst_capacity_u64 - dst_offset_u64) {
    return E_INVALIDARG;
  }

  // Map source and destination bytes.
  const uint8_t* src_vertices = nullptr;
  uint8_t* dst_vertices = nullptr;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  void* src_ptr = nullptr;
  void* dst_ptr = nullptr;
  bool src_locked = false;
  bool dst_locked = false;
#endif

  bool use_src_storage = src_res->storage.size() >= static_cast<size_t>(src_offset_u64 + src_size_u64);
  bool use_dst_storage = dst_res->storage.size() >= static_cast<size_t>(dst_offset_u64 + dst_size_u64);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (src_res->backing_alloc_id != 0) {
    use_src_storage = false;
  }
  if (dst_res->backing_alloc_id != 0) {
    use_dst_storage = false;
  }
#endif

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  const bool same_resource = (src_res == dst_res);
  const bool needs_wddm_lock = (!use_src_storage || !use_dst_storage);
  if (same_resource && needs_wddm_lock) {
    // Avoid locking the same allocation twice: lock a single union range and
    // derive pointers for src/dst from it.
    if (src_res->wddm_hAllocation == 0 || dev->wddm_device == 0) {
      return E_INVALIDARG;
    }

    const uint64_t src_begin = src_offset_u64;
    const uint64_t src_end = src_offset_u64 + src_size_u64;
    const uint64_t dst_begin = dst_offset_u64;
    const uint64_t dst_end = dst_offset_u64 + dst_size_u64;
    const uint64_t lock_begin = std::min(src_begin, dst_begin);
    const uint64_t lock_end = std::max(src_end, dst_end);
    const uint64_t lock_size = lock_end - lock_begin;

    const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                 dev->wddm_device,
                                                 src_res->wddm_hAllocation,
                                                 lock_begin,
                                                 lock_size,
                                                 /*lock_flags=*/0,
                                                 &src_ptr,
                                                 dev->wddm_context.hContext);
    if (FAILED(lock_hr) || !src_ptr) {
      return FAILED(lock_hr) ? lock_hr : E_FAIL;
    }
    src_locked = true;
    auto* base = static_cast<uint8_t*>(src_ptr);
    src_vertices = base + static_cast<size_t>(src_begin - lock_begin);
    dst_vertices = base + static_cast<size_t>(dst_begin - lock_begin);
  } else
#endif
  {
    if (use_src_storage) {
      src_vertices = src_res->storage.data() + static_cast<size_t>(src_offset_u64);
    } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (src_res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                     dev->wddm_device,
                                                     src_res->wddm_hAllocation,
                                                     src_offset_u64,
                                                     src_size_u64,
                                                     kD3DLOCK_READONLY,
                                                     &src_ptr,
                                                     dev->wddm_context.hContext);
        if (FAILED(lock_hr) || !src_ptr) {
          return FAILED(lock_hr) ? lock_hr : E_FAIL;
        }
        src_locked = true;
        src_vertices = static_cast<const uint8_t*>(src_ptr);
      } else
#endif
      {
        return E_INVALIDARG;
      }
    }

    if (use_dst_storage) {
      dst_vertices = dst_res->storage.data() + static_cast<size_t>(dst_offset_u64);
    } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                     dev->wddm_device,
                                                     dst_res->wddm_hAllocation,
                                                     dst_offset_u64,
                                                     dst_size_u64,
                                                     /*lock_flags=*/0,
                                                     &dst_ptr,
                                                     dev->wddm_context.hContext);
        if (FAILED(lock_hr) || !dst_ptr) {
          if (src_locked) {
            (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
          }
          return FAILED(lock_hr) ? lock_hr : E_FAIL;
        }
        dst_locked = true;
        dst_vertices = static_cast<uint8_t*>(dst_ptr);
      } else
#endif
      {
        if (use_src_storage == false) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
          if (src_locked) {
            (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
          }
#endif
        }
        return E_INVALIDARG;
      }
    }
  }

  // If source and destination ranges overlap within the same buffer, take a
  // temporary copy of the source slice to avoid self-overwrite.
  std::vector<uint8_t> src_copy;
  if (src_res == dst_res) {
    const uint64_t src_begin = src_offset_u64;
    const uint64_t src_end = src_offset_u64 + src_size_u64;
    const uint64_t dst_begin = dst_offset_u64;
    const uint64_t dst_end = dst_offset_u64 + dst_size_u64;
    const bool overlap = (src_begin < dst_end) && (dst_begin < src_end);
    if (overlap) {
      try {
        src_copy.resize(static_cast<size_t>(src_size_u64));
      } catch (...) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (dst_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
        }
        if (src_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
        }
#endif
        return E_OUTOFMEMORY;
      }
      std::memcpy(src_copy.data(), src_vertices, static_cast<size_t>(src_size_u64));
      src_vertices = src_copy.data();
    }
  }

  // Fixed-function fallback: implement a minimal CPU vertex transform for common
  // FVF paths when no user shaders are bound.
  if (fixedfunc &&
      (src_xyzrhw || src_xyz_plain || src_xyzw || src_xyz_diffuse || src_xyzw_diffuse || src_xyz_diffuse_tex1 ||
       src_xyzw_diffuse_tex1 || src_xyz_tex1 || src_xyzw_tex1)) {
    uint32_t src_min_stride = 0;
    bool src_has_diffuse = false;
    uint32_t src_diffuse_offset = 0;
    bool src_has_tex0 = false;
    uint32_t src_tex0_offset = 0;
    uint32_t src_tex0_size_bytes = 0;
    bool src_is_xyzrhw = false;
    bool src_is_xyzw = false;
    if (src_xyz_plain) {
      src_min_stride = 12u;
    } else if (src_xyzw_plain) {
      src_is_xyzw = true;
      src_min_stride = 16u;
    } else if (src_xyz_diffuse) {
      src_min_stride = 16u;
      src_has_diffuse = true;
      src_diffuse_offset = 12u;
    } else if (src_xyzw_diffuse) {
      src_is_xyzw = true;
      src_min_stride = 20u;
      src_has_diffuse = true;
      src_diffuse_offset = 16u;
    } else if (src_xyz_diffuse_tex1) {
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 16u + src_tex0_size_bytes;
      src_has_diffuse = true;
      src_diffuse_offset = 12u;
      src_has_tex0 = true;
      src_tex0_offset = 16u;
    } else if (src_xyzw_diffuse_tex1) {
      src_is_xyzw = true;
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 20u + src_tex0_size_bytes;
      src_has_diffuse = true;
      src_diffuse_offset = 16u;
      src_has_tex0 = true;
      src_tex0_offset = 20u;
    } else if (src_xyz_tex1) {
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 12u + src_tex0_size_bytes;
      src_has_tex0 = true;
      src_tex0_offset = 12u;
    } else if (src_xyzw_tex1) {
      src_is_xyzw = true;
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 16u + src_tex0_size_bytes;
      src_has_tex0 = true;
      src_tex0_offset = 16u;
    } else if (src_xyzrhw_plain) {
      src_is_xyzrhw = true;
      src_min_stride = 16u;
    } else if (src_xyzrhw_diffuse) {
      src_is_xyzrhw = true;
      src_min_stride = 20u;
      src_has_diffuse = true;
      src_diffuse_offset = 16u;
    } else if (src_xyzrhw_diffuse_tex1) {
      src_is_xyzrhw = true;
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 20u + src_tex0_size_bytes;
      src_has_diffuse = true;
      src_diffuse_offset = 16u;
      src_has_tex0 = true;
      src_tex0_offset = 20u;
    } else if (src_xyzrhw_tex1) {
      src_is_xyzrhw = true;
      const uint32_t tex0_dim = fvf_decode_texcoord_size(fvf, 0);
      if (tex0_dim < 1u || tex0_dim > 4u) {
        return E_FAIL;
      }
      src_tex0_size_bytes = tex0_dim * 4u;
      src_min_stride = 16u + src_tex0_size_bytes;
      src_has_tex0 = true;
      src_tex0_offset = 16u;
    } else {
      // Unexpected: the early supported-FVF check should have rejected this.
      return E_FAIL;
    }

    if (src_has_tex0 && src_tex0_size_bytes == 0) {
      return E_FAIL;
    }

    if (src_stride < src_min_stride) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
      if (src_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
#endif
      return E_INVALIDARG;
    }

    // Destination must provide position (XYZRHW). Copy optional diffuse/tex fields
    // when present.
    if (dst_layout.pos_offset + 16u > dst_stride) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
      if (src_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
#endif
      return kD3DErrInvalidCall;
    }
    if (dst_layout.has_diffuse && dst_layout.diffuse_offset + 4u > dst_stride) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
      if (src_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
#endif
      return kD3DErrInvalidCall;
    }
    if (src_has_tex0 && dst_layout.has_tex0 &&
        (dst_layout.tex0_size_bytes == 0 || dst_layout.tex0_offset + dst_layout.tex0_size_bytes > dst_stride)) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
      if (src_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
#endif
      return kD3DErrInvalidCall;
    }

    // XYZ vertices need a WVP transform. XYZRHW vertices are already in screen
    // space and are passed through as-is.
    float wvp[16] = {};
    float vp_x = 0.0f;
    float vp_y = 0.0f;
    float vp_w = 1.0f;
    float vp_h = 1.0f;
    if (!src_is_xyzrhw) {
      // Compute WVP matrix (row-major, row-vector convention).
      if (kD3dTransformWorld0 >= Device::kTransformCacheCount ||
          kD3dTransformView >= Device::kTransformCacheCount ||
          kD3dTransformProjection >= Device::kTransformCacheCount) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (dst_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
        }
        if (src_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
        }
#endif
        return E_FAIL;
      }

      float wv[16];
      d3d9_mul_mat4_row_major(dev->transform_matrices[kD3dTransformWorld0],
                              dev->transform_matrices[kD3dTransformView],
                              wv);
      d3d9_mul_mat4_row_major(wv, dev->transform_matrices[kD3dTransformProjection], wvp);

      const D3DDDIVIEWPORTINFO vp = viewport_effective_locked(dev);
      vp_x = vp.X;
      vp_y = vp.Y;
      vp_w = vp.Width;
      vp_h = vp.Height;
    }
    for (uint32_t i = 0; i < vertex_count; ++i) {
      const uint8_t* src = src_vertices + static_cast<size_t>(i) * src_stride;
      uint8_t* dst = dst_vertices + static_cast<size_t>(i) * dst_stride;
      // Deterministic output: clear any destination elements that are not written
      // by the source FVF/decl mapping (e.g. dst has TEX0 but src does not).
      // Honor D3DPV_DONOTCOPYDATA by preserving the existing destination vertex
      // data where we are not explicitly writing.
      if (!do_not_copy_data) {
        std::memset(dst, 0, dst_stride);
      }

      if (src_is_xyzrhw) {
        // Pre-transformed vertices: POSITIONT is already in screen space.
        std::memcpy(dst + dst_layout.pos_offset, src + 0, 16);
      } else {
        const float in_x = read_f32_unaligned(src + 0);
        const float in_y = read_f32_unaligned(src + 4);
        const float in_z = read_f32_unaligned(src + 8);
        const float in_w = src_is_xyzw ? read_f32_unaligned(src + 12) : 1.0f;

        const float v[4] = {in_x, in_y, in_z, in_w};
        float clip[4] = {};
        d3d9_mul_vec4_mat4_row_major(v, wvp, clip);

        float w = clip[3];
        if (!std::isfinite(w) || w == 0.0f) {
          // Avoid division-by-zero / NaNs. A real D3D9 implementation would likely
          // clip these vertices; for bring-up keep the math finite.
          w = 1.0f;
        }
        float inv_w = 1.0f / w;
        if (!std::isfinite(inv_w)) {
          // Keep math finite; an infinite/NaN inv_w would poison all downstream
          // calculations.
          inv_w = 0.0f;
        }

        float ndc_x = clip[0] * inv_w;
        float ndc_y = clip[1] * inv_w;
        float ndc_z = clip[2] * inv_w;
        if (!std::isfinite(ndc_x)) {
          ndc_x = 0.0f;
        }
        if (!std::isfinite(ndc_y)) {
          ndc_y = 0.0f;
        }
        if (!std::isfinite(ndc_z)) {
          ndc_z = 0.0f;
        }

        // D3D9 viewport transform uses a -0.5 pixel center convention.
        const float out_x = ((ndc_x + 1.0f) * 0.5f) * vp_w + vp_x - 0.5f;
        const float out_y = ((1.0f - ndc_y) * 0.5f) * vp_h + vp_y - 0.5f;
        const float out_z = ndc_z;
        const float out_rhw = inv_w;

        write_f32_unaligned(dst + dst_layout.pos_offset + 0, out_x);
        write_f32_unaligned(dst + dst_layout.pos_offset + 4, out_y);
        write_f32_unaligned(dst + dst_layout.pos_offset + 8, out_z);
        write_f32_unaligned(dst + dst_layout.pos_offset + 12, out_rhw);
      }

      if (!do_not_copy_data) {
        if (dst_layout.has_diffuse) {
          if (src_has_diffuse) {
            std::memcpy(dst + dst_layout.diffuse_offset, src + src_diffuse_offset, 4);
          } else {
            // Match fixed-function FVF behavior: when the input vertex format does
            // not include a diffuse color, treat it as white.
            const uint32_t white = 0xFFFFFFFFu;
            std::memcpy(dst + dst_layout.diffuse_offset, &white, sizeof(white));
          }
        }
        if (src_has_tex0 && dst_layout.has_tex0) {
          const uint32_t copy_bytes = std::min(src_tex0_size_bytes, dst_layout.tex0_size_bytes);
          if (copy_bytes) {
            std::memcpy(dst + dst_layout.tex0_offset, src + src_tex0_offset, copy_bytes);
          }
        }
      }
    }
  } else {
    // Fallback: only support a strict memcpy when the destination stride matches
    // stream 0 stride.
    if (dst_stride != src_stride) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dst_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
      if (src_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
      }
#endif
      return kD3DErrInvalidCall;
    }

    std::memmove(dst_vertices, src_vertices, static_cast<size_t>(src_size_u64));
  }

  // Unmap resources.
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dst_locked) {
    const HRESULT unlock_hr =
        wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, dst_res->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(unlock_hr)) {
      return unlock_hr;
    }
  }
  if (src_locked) {
    const HRESULT unlock_hr =
        wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, src_res->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(unlock_hr)) {
      return unlock_hr;
    }
  }
#endif

  // Notify the host that the destination resource bytes changed (equivalent to
  // Lock/Unlock writes).
  if (dst_res->handle != 0 && dst_size_u64) {
    const uint32_t upload_offset = static_cast<uint32_t>(dst_offset_u64 & 0xFFFFFFFFu);
    const uint32_t upload_size = static_cast<uint32_t>(dst_size_u64 & 0xFFFFFFFFu);

    if (dst_res->backing_alloc_id != 0) {
      if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
        return E_OUTOFMEMORY;
      }
      const HRESULT track_hr = track_resource_allocation_locked(dev, dst_res, /*write=*/false);
      if (FAILED(track_hr)) {
        return track_hr;
      }
      auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->resource_handle = dst_res->handle;
      cmd->reserved0 = 0;
      cmd->offset_bytes = dst_offset_u64;
      cmd->size_bytes = dst_size_u64;
    } else {
      // Host-allocated resources: emit UPLOAD_RESOURCE for the written range.
      if (dst_res->storage.size() < static_cast<size_t>(dst_capacity_u64)) {
        // Destination bytes were written through a non-storage mapping; no safe
        // fallback upload path exists for portable builds.
        return E_FAIL;
      }

      const bool is_buffer = (dst_res->kind == ResourceKind::Buffer);
      uint32_t up_off = upload_offset;
      uint32_t up_size = upload_size;
      if (is_buffer) {
        const uint32_t start = up_off & ~3u;
        const uint64_t end_u64 = static_cast<uint64_t>(up_off) + static_cast<uint64_t>(up_size);
        const uint32_t end = static_cast<uint32_t>((end_u64 + 3ull) & ~3ull);
        if (end > dst_res->size_bytes || end < start) {
          return E_INVALIDARG;
        }
        up_off = start;
        up_size = end - start;
      }

      const HRESULT upload_hr = emit_upload_resource_range_locked(dev, dst_res, up_off, up_size);
      if (FAILED(upload_hr)) {
        return upload_hr;
      }
    }
  }

  return S_OK;
}

// -----------------------------------------------------------------------------
// Submission
// -----------------------------------------------------------------------------
//
// Shared allocations must use stable `alloc_id` values that are extremely
// unlikely to collide across guest processes: DWM can reference many redirected
// surfaces from different processes in a single submission, and the KMD's
// per-submit allocation table is keyed by `alloc_id`.
//
// The D3D9 UMD uses a best-effort cross-process monotonic counter (implemented
// via a named file mapping) to derive 31-bit alloc_id values for shared
// allocations.
//
// The mapping name is stable across processes in the current session and is
// keyed by the adapter LUID so multiple adapters don't alias the same counter.
uint64_t allocate_shared_alloc_id_token(Adapter* adapter) {
  if (!adapter) {
    return 0;
  }

#if defined(_WIN32)
  {
    std::lock_guard<std::mutex> lock(adapter->share_token_mutex);

    if (!adapter->share_token_view) {
      wchar_t name[128];
      // Keep the object name stable across processes within a session.
      // Multiple adapters can disambiguate via LUID when available.
      swprintf(name,
               sizeof(name) / sizeof(name[0]),
               L"Local\\AeroGPU.D3D9.ShareToken.%08X%08X",
               static_cast<unsigned>(adapter->luid.HighPart),
               static_cast<unsigned>(adapter->luid.LowPart));

      // This mapping backs the cross-process alloc_id allocator used for D3D9Ex
      // shared surfaces. DWM may open and submit shared allocations from many
      // *different* processes in a single batch, so alloc_id values must be
      // unique across guest processes, not just within one process.
      //
      // Use a permissive DACL so the mapping can be opened by other processes in
      // the session (e.g. DWM, sandboxed apps, different integrity levels).
      HANDLE mapping =
          win32::CreateFileMappingWBestEffortLowIntegrity(
              INVALID_HANDLE_VALUE, PAGE_READWRITE, 0, sizeof(uint64_t), name);
      if (mapping) {
        void* view = MapViewOfFile(mapping, FILE_MAP_ALL_ACCESS, 0, 0, sizeof(uint64_t));
        if (view) {
          adapter->share_token_mapping = mapping;
          adapter->share_token_view = view;
        } else {
          CloseHandle(mapping);
        }
      }
    }

    if (adapter->share_token_view) {
      auto* counter = reinterpret_cast<volatile LONG64*>(adapter->share_token_view);
      LONG64 token = InterlockedIncrement64(counter);
      const uint32_t alloc_id =
          static_cast<uint32_t>(static_cast<uint64_t>(token) & AEROGPU_WDDM_ALLOC_ID_UMD_MAX);
      if (alloc_id == 0) {
        token = InterlockedIncrement64(counter);
      }
      return static_cast<uint64_t>(token);
    }
  }

  // If we fail to set up the cross-process allocator, we must still return a
  // value that produces an alloc_id unlikely to collide across processes.
  //
  // NOTE: alloc_id is derived by masking to 31 bits
  // (`token & AEROGPU_WDDM_ALLOC_ID_UMD_MAX`). A previous PID+counter fallback
  // placed the PID in the high 32 bits, which are discarded by the mask, making
  // collisions across processes *deterministic* (every process would generate
  // alloc_id=1,2,3,...).
  static std::once_flag warn_once;
  std::call_once(warn_once, [] {
    logf("aerogpu-d3d9: alloc_id allocator: shared mapping unavailable; using RNG fallback\n");
  });

  // Best-effort: use the same crypto RNG strategy as the shared-surface
  // ShareTokenAllocator so collisions across processes are vanishingly unlikely.
  for (;;) {
    const uint64_t token = adapter->share_token_allocator.allocate_share_token();
    const uint32_t alloc_id =
        static_cast<uint32_t>(token & AEROGPU_WDDM_ALLOC_ID_UMD_MAX);
    if (alloc_id != 0) {
      return token;
    }
  }
#else
  (void)adapter;
  static std::atomic<uint64_t> next_token{1};
  return next_token.fetch_add(1);
#endif
}

uint32_t allocate_umd_alloc_id(Adapter* adapter) {
  if (!adapter) {
    return 0;
  }

  // Use the same cross-process monotonic allocator used by shared resources so
  // alloc_id values never collide when DWM batches resources from many
  // processes in a single submission.
  for (;;) {
    const uint64_t token = allocate_shared_alloc_id_token(adapter);
    if (token == 0) {
      return 0;
    }

    const uint32_t alloc_id = static_cast<uint32_t>(token & AEROGPU_WDDM_ALLOC_ID_UMD_MAX);
    if (alloc_id != 0) {
      return alloc_id;
    }
  }
}

namespace {
#if defined(_WIN32)
template <typename T, typename = void>
struct has_pfnRenderCb : std::false_type {};
template <typename T>
struct has_pfnRenderCb<T, std::void_t<decltype(std::declval<T>().pfnRenderCb)>> : std::true_type {};

template <typename T, typename = void>
struct has_pfnPresentCb : std::false_type {};
template <typename T>
struct has_pfnPresentCb<T, std::void_t<decltype(std::declval<T>().pfnPresentCb)>> : std::true_type {};

template <typename T, typename = void>
struct has_pfnSubmitCommandCb : std::false_type {};
template <typename T>
struct has_pfnSubmitCommandCb<T, std::void_t<decltype(std::declval<T>().pfnSubmitCommandCb)>> : std::true_type {};

template <typename T, typename = void>
struct has_pfnAllocateCb : std::false_type {};
template <typename T>
struct has_pfnAllocateCb<T, std::void_t<decltype(std::declval<T>().pfnAllocateCb)>> {
  using MemberT = decltype(std::declval<T>().pfnAllocateCb);
  static constexpr bool value =
      std::is_pointer_v<MemberT> && std::is_function_v<std::remove_pointer_t<MemberT>>;
};

template <typename T, typename = void>
struct has_pfnDeallocateCb : std::false_type {};
template <typename T>
struct has_pfnDeallocateCb<T, std::void_t<decltype(std::declval<T>().pfnDeallocateCb)>> {
  using MemberT = decltype(std::declval<T>().pfnDeallocateCb);
  static constexpr bool value =
      std::is_pointer_v<MemberT> && std::is_function_v<std::remove_pointer_t<MemberT>>;
};

template <typename T, typename = void>
struct has_pfnGetCommandBufferCb : std::false_type {};
template <typename T>
struct has_pfnGetCommandBufferCb<T, std::void_t<decltype(std::declval<T>().pfnGetCommandBufferCb)>> {
  using MemberT = decltype(std::declval<T>().pfnGetCommandBufferCb);
  static constexpr bool value =
      std::is_pointer_v<MemberT> && std::is_function_v<std::remove_pointer_t<MemberT>>;
};

template <typename Fn>
struct fn_first_param;

template <typename Ret, typename Arg0, typename... Rest>
struct fn_first_param<Ret(__stdcall*)(Arg0, Rest...)> {
  using type = Arg0;
};

template <typename Ret, typename Arg0, typename... Rest>
struct fn_first_param<Ret(*)(Arg0, Rest...)> {
  using type = Arg0;
};

template <typename T, typename = void>
struct has_member_hContext : std::false_type {};
template <typename T>
struct has_member_hContext<T, std::void_t<decltype(std::declval<T>().hContext)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_hDevice : std::false_type {};
template <typename T>
struct has_member_hDevice<T, std::void_t<decltype(std::declval<T>().hDevice)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pCommandBuffer : std::false_type {};
template <typename T>
struct has_member_pCommandBuffer<T, std::void_t<decltype(std::declval<T>().pCommandBuffer)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pDmaBuffer : std::false_type {};
template <typename T>
struct has_member_pDmaBuffer<T, std::void_t<decltype(std::declval<T>().pDmaBuffer)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_CommandLength : std::false_type {};
template <typename T>
struct has_member_CommandLength<T, std::void_t<decltype(std::declval<T>().CommandLength)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_CommandBufferSize : std::false_type {};
template <typename T>
struct has_member_CommandBufferSize<T, std::void_t<decltype(std::declval<T>().CommandBufferSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_DmaBufferSize : std::false_type {};
template <typename T>
struct has_member_DmaBufferSize<T, std::void_t<decltype(std::declval<T>().DmaBufferSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pAllocationList : std::false_type {};
template <typename T>
struct has_member_pAllocationList<T, std::void_t<decltype(std::declval<T>().pAllocationList)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_AllocationListSize : std::false_type {};
template <typename T>
struct has_member_AllocationListSize<T, std::void_t<decltype(std::declval<T>().AllocationListSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NumAllocations : std::false_type {};
template <typename T>
struct has_member_NumAllocations<T, std::void_t<decltype(std::declval<T>().NumAllocations)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pPatchLocationList : std::false_type {};
template <typename T>
struct has_member_pPatchLocationList<T, std::void_t<decltype(std::declval<T>().pPatchLocationList)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_PatchLocationListSize : std::false_type {};
template <typename T>
struct has_member_PatchLocationListSize<T, std::void_t<decltype(std::declval<T>().PatchLocationListSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NumPatchLocations : std::false_type {};
template <typename T>
struct has_member_NumPatchLocations<T, std::void_t<decltype(std::declval<T>().NumPatchLocations)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_Flags : std::false_type {};
template <typename T>
struct has_member_Flags<T, std::void_t<decltype(std::declval<T>().Flags)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_Present : std::false_type {};
template <typename T>
struct has_member_Present<T, std::void_t<decltype(std::declval<T>().Present)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pNewCommandBuffer : std::false_type {};
template <typename T>
struct has_member_pNewCommandBuffer<T, std::void_t<decltype(std::declval<T>().pNewCommandBuffer)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NewCommandBufferSize : std::false_type {};
template <typename T>
struct has_member_NewCommandBufferSize<T, std::void_t<decltype(std::declval<T>().NewCommandBufferSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pNewAllocationList : std::false_type {};
template <typename T>
struct has_member_pNewAllocationList<T, std::void_t<decltype(std::declval<T>().pNewAllocationList)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NewAllocationListSize : std::false_type {};
template <typename T>
struct has_member_NewAllocationListSize<T, std::void_t<decltype(std::declval<T>().NewAllocationListSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pNewPatchLocationList : std::false_type {};
template <typename T>
struct has_member_pNewPatchLocationList<T, std::void_t<decltype(std::declval<T>().pNewPatchLocationList)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NewPatchLocationListSize : std::false_type {};
template <typename T>
struct has_member_NewPatchLocationListSize<T, std::void_t<decltype(std::declval<T>().NewPatchLocationListSize)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_SubmissionFenceId : std::false_type {};
template <typename T>
struct has_member_SubmissionFenceId<T, std::void_t<decltype(std::declval<T>().SubmissionFenceId)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_NewFenceValue : std::false_type {};
template <typename T>
struct has_member_NewFenceValue<T, std::void_t<decltype(std::declval<T>().NewFenceValue)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pSubmissionFenceId : std::false_type {};
template <typename T>
struct has_member_pSubmissionFenceId<T, std::void_t<decltype(std::declval<T>().pSubmissionFenceId)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_FenceValue : std::false_type {};
template <typename T>
struct has_member_FenceValue<T, std::void_t<decltype(std::declval<T>().FenceValue)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pFenceValue : std::false_type {};
template <typename T>
struct has_member_pFenceValue<T, std::void_t<decltype(std::declval<T>().pFenceValue)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pDmaBufferPrivateData : std::false_type {};
template <typename T>
struct has_member_pDmaBufferPrivateData<T, std::void_t<decltype(std::declval<T>().pDmaBufferPrivateData)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_DmaBufferPrivateDataSize : std::false_type {};
template <typename T>
struct has_member_DmaBufferPrivateDataSize<T, std::void_t<decltype(std::declval<T>().DmaBufferPrivateDataSize)>>
    : std::true_type {};

template <typename ArgsT>
constexpr bool submit_args_can_signal_present() {
  if constexpr (has_member_Present<ArgsT>::value) {
    using PresentT = std::remove_reference_t<decltype(std::declval<ArgsT>().Present)>;
    if constexpr (std::is_integral_v<PresentT>) {
      return true;
    }
  }
  if constexpr (has_member_Flags<ArgsT>::value) {
    using FlagsT = std::remove_reference_t<decltype(std::declval<ArgsT>().Flags)>;
    if constexpr (has_member_Present<FlagsT>::value) {
      return true;
    }
  }
  return false;
}

template <typename CallbackFn>
constexpr bool submit_callback_can_signal_present() {
  using ArgPtr = typename fn_first_param<CallbackFn>::type;
  using Arg = std::remove_const_t<std::remove_pointer_t<ArgPtr>>;
  return submit_args_can_signal_present<Arg>();
}

template <typename ArgsT>
void fill_submit_args(ArgsT& args, Device* dev, uint32_t command_length_bytes, bool is_present) {
  [[maybe_unused]] const bool patch_list_available = (dev->wddm_context.pPatchLocationList != nullptr);
  [[maybe_unused]] const uint32_t patch_list_used = patch_list_available ? dev->wddm_context.patch_location_entries_used : 0;
  [[maybe_unused]] const uint32_t patch_list_capacity = patch_list_available ? dev->wddm_context.PatchLocationListSize : 0;
  if constexpr (has_member_hDevice<ArgsT>::value) {
    args.hDevice = dev->wddm_device;
  }
  if constexpr (has_member_hContext<ArgsT>::value) {
    args.hContext = dev->wddm_context.hContext;
  }
  if constexpr (has_member_pCommandBuffer<ArgsT>::value) {
    args.pCommandBuffer = dev->wddm_context.pCommandBuffer;
  }
  if constexpr (has_member_pDmaBuffer<ArgsT>::value) {
    args.pDmaBuffer = dev->wddm_context.pDmaBuffer ? dev->wddm_context.pDmaBuffer : dev->wddm_context.pCommandBuffer;
  }
  if constexpr (has_member_CommandLength<ArgsT>::value) {
    args.CommandLength = command_length_bytes;
  }
  if constexpr (has_member_CommandBufferSize<ArgsT>::value) {
    args.CommandBufferSize = dev->wddm_context.CommandBufferSize;
  }
  if constexpr (has_member_DmaBufferSize<ArgsT>::value) {
    // DmaBufferSize is consistently interpreted by Win7-era callback structs as
    // the number of bytes used in the DMA buffer (not the total capacity).
    // Populate it with the used byte count to avoid dxgkrnl/KMD reading
    // uninitialized command buffer bytes.
    args.DmaBufferSize = command_length_bytes;
  }
  if constexpr (has_member_pAllocationList<ArgsT>::value) {
    args.pAllocationList = dev->wddm_context.pAllocationList;
  }
  if constexpr (has_member_AllocationListSize<ArgsT>::value) {
    // DDI structs disagree on whether AllocationListSize means "capacity" or
    // "entries used". When NumAllocations is present, treat AllocationListSize
    // as the capacity returned by CreateContext. Otherwise fall back to the used
    // count (legacy submit structs with only a single size field).
    if constexpr (has_member_NumAllocations<ArgsT>::value) {
      args.AllocationListSize = dev->wddm_context.AllocationListSize;
    } else {
      args.AllocationListSize = dev->wddm_context.allocation_list_entries_used;
    }
  }
  if constexpr (has_member_NumAllocations<ArgsT>::value) {
    args.NumAllocations = dev->wddm_context.allocation_list_entries_used;
  }
  if constexpr (has_member_pPatchLocationList<ArgsT>::value) {
    args.pPatchLocationList = patch_list_available ? dev->wddm_context.pPatchLocationList : nullptr;
  }
  if constexpr (has_member_PatchLocationListSize<ArgsT>::value) {
    // AeroGPU intentionally submits with an empty patch-location list.
    //
    // - Callback structs that split capacity vs. used across
    //   {PatchLocationListSize, NumPatchLocations} expect PatchLocationListSize to
    //   describe the list capacity returned by CreateContext.
    // - Legacy structs with only PatchLocationListSize interpret it as the number
    //   of patch locations used.
    if constexpr (has_member_NumPatchLocations<ArgsT>::value) {
      args.PatchLocationListSize = patch_list_capacity;
    } else {
      args.PatchLocationListSize = patch_list_used;
    }
  }
  if constexpr (has_member_NumPatchLocations<ArgsT>::value) {
    args.NumPatchLocations = patch_list_used;
  }
  if constexpr (has_member_pDmaBufferPrivateData<ArgsT>::value) {
    args.pDmaBufferPrivateData = dev->wddm_context.pDmaBufferPrivateData;
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<ArgsT>::value) {
    // Clamp to the driver-private ABI size so dxgkrnl doesn't copy extra
    // user-mode bytes into kernel buffers.
    args.DmaBufferPrivateDataSize = dev->wddm_context.DmaBufferPrivateDataSize;
    if (args.DmaBufferPrivateDataSize > AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES) {
      args.DmaBufferPrivateDataSize = AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES;
    }
  }

  // Some WDDM callback arg structs include flags distinguishing render vs present.
  // If such flags are present, populate them so present submissions prefer the
  // DxgkDdiPresent path when routed via RenderCb fallback.
  if constexpr (has_member_Present<ArgsT>::value) {
    using PresentT = std::remove_reference_t<decltype(args.Present)>;
    if constexpr (std::is_integral_v<PresentT>) {
      args.Present = is_present ? 1 : 0;
    }
  }
  if constexpr (has_member_Flags<ArgsT>::value) {
    using FlagsT = std::remove_reference_t<decltype(args.Flags)>;
    if constexpr (has_member_Present<FlagsT>::value) {
      args.Flags.Present = is_present ? 1 : 0;
    }
  }
}

template <typename ArgsT>
void update_context_from_submit_args(Device* dev, const ArgsT& args) {
  const uint8_t* prev_cmd_buffer = dev->wddm_context.pCommandBuffer;
  bool updated_cmd_buffer = false;
  if constexpr (has_member_pNewCommandBuffer<ArgsT>::value && has_member_NewCommandBufferSize<ArgsT>::value) {
    if (args.pNewCommandBuffer && args.NewCommandBufferSize) {
      dev->wddm_context.pCommandBuffer = static_cast<uint8_t*>(args.pNewCommandBuffer);
      dev->wddm_context.CommandBufferSize = args.NewCommandBufferSize;
      updated_cmd_buffer = true;
    }
  }

  if (!updated_cmd_buffer) {
    if constexpr (has_member_pCommandBuffer<ArgsT>::value) {
      if (args.pCommandBuffer) {
        dev->wddm_context.pCommandBuffer = static_cast<uint8_t*>(args.pCommandBuffer);
      }
    }
    if constexpr (has_member_CommandBufferSize<ArgsT>::value) {
      if (args.CommandBufferSize) {
        dev->wddm_context.CommandBufferSize = args.CommandBufferSize;
      }
    }
  }

  // Track pDmaBuffer separately when exposed by the callback struct. Some WDK
  // vintages include both pDmaBuffer and pCommandBuffer; preserve the DMA buffer
  // pointer so we can pass it back to dxgkrnl.
  bool updated_dma_buffer = false;
  if constexpr (has_member_pDmaBuffer<ArgsT>::value) {
    if (args.pDmaBuffer) {
      dev->wddm_context.pDmaBuffer = static_cast<uint8_t*>(args.pDmaBuffer);
      updated_dma_buffer = true;
    }
  }
  if (!updated_dma_buffer && dev->wddm_context.pCommandBuffer) {
    // If pDmaBuffer is unset (or was previously tracking the old command buffer
    // pointer), keep it in sync with the current command buffer.
    if (!dev->wddm_context.pDmaBuffer || dev->wddm_context.pDmaBuffer == prev_cmd_buffer) {
      dev->wddm_context.pDmaBuffer = dev->wddm_context.pCommandBuffer;
    }
  }

  bool updated_allocation_list = false;
  if constexpr (has_member_pNewAllocationList<ArgsT>::value && has_member_NewAllocationListSize<ArgsT>::value) {
    if (args.pNewAllocationList && args.NewAllocationListSize) {
      dev->wddm_context.pAllocationList = args.pNewAllocationList;
      dev->wddm_context.AllocationListSize = args.NewAllocationListSize;
      updated_allocation_list = true;
    }
  }

  if (!updated_allocation_list) {
    if constexpr (has_member_pAllocationList<ArgsT>::value) {
      if (args.pAllocationList) {
        dev->wddm_context.pAllocationList = args.pAllocationList;
      }
    }
    if constexpr (has_member_AllocationListSize<ArgsT>::value && has_member_NumAllocations<ArgsT>::value) {
      if (args.AllocationListSize) {
        dev->wddm_context.AllocationListSize = args.AllocationListSize;
      }
    }
  }

  bool updated_patch_list = false;
  if constexpr (has_member_pNewPatchLocationList<ArgsT>::value && has_member_NewPatchLocationListSize<ArgsT>::value) {
    // Some runtimes can legitimately provide a 0-sized patch list. Treat the
    // pointer as the authoritative signal that a new patch list is being rotated
    // in, and always copy the size (even if it is 0).
    if (args.pNewPatchLocationList) {
      dev->wddm_context.pPatchLocationList = args.pNewPatchLocationList;
      dev->wddm_context.PatchLocationListSize = args.NewPatchLocationListSize;
      updated_patch_list = true;
    }
  }

  if (!updated_patch_list) {
    if constexpr (has_member_pPatchLocationList<ArgsT>::value) {
      dev->wddm_context.pPatchLocationList = args.pPatchLocationList;
    }
    if constexpr (has_member_PatchLocationListSize<ArgsT>::value && has_member_NumPatchLocations<ArgsT>::value) {
      dev->wddm_context.PatchLocationListSize = args.PatchLocationListSize;
    }
  }

  // pDmaBufferPrivateData is required by the AeroGPU Win7 KMD (DxgkDdiRender /
  // DxgkDdiPresent expect it to be non-null). The runtime may rotate it along
  // with the command buffer, so treat it as an in/out field.
  if constexpr (has_member_pDmaBufferPrivateData<ArgsT>::value) {
    if (args.pDmaBufferPrivateData) {
      dev->wddm_context.pDmaBufferPrivateData = args.pDmaBufferPrivateData;
    }
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<ArgsT>::value) {
    if (args.DmaBufferPrivateDataSize) {
      dev->wddm_context.DmaBufferPrivateDataSize = args.DmaBufferPrivateDataSize;
    }
  }
}

template <typename CallbackFn>
HRESULT invoke_submit_callback(Device* dev,
                               CallbackFn cb,
                               uint32_t command_length_bytes,
                               bool is_present,
                               uint64_t* out_submission_fence) {
  if (out_submission_fence) {
    *out_submission_fence = 0;
  }

  using ArgPtr = typename fn_first_param<CallbackFn>::type;
  using Arg = std::remove_const_t<std::remove_pointer_t<ArgPtr>>;

  // Zero-initialize the entire callback struct (including any padding). The D3D9
  // runtime may copy these bytes into kernel mode; leaving padding uninitialized
  // can leak stack bytes and make submission behavior nondeterministic.
  Arg args;
  std::memset(&args, 0, sizeof(args));
  fill_submit_args(args, dev, command_length_bytes, is_present);

  if constexpr (has_member_NewFenceValue<Arg>::value) {
    args.NewFenceValue = 0;
  }

  // Security: `pDmaBufferPrivateData` is copied by dxgkrnl from user mode to
  // kernel mode for every submission. Ensure the blob is explicitly initialized
  // so we never leak uninitialized user-mode stack/heap bytes into the kernel
  // copy.
  //
  // The AeroGPU Win7 KMD overwrites AEROGPU_DMA_PRIV in DxgkDdiRender /
  // DxgkDdiPresent, but some runtimes route submissions through SubmitCommandCb
  // (bypassing those DDIs). Always stamp a deterministic AEROGPU_DMA_PRIV header
  // before invoking the runtime submission callback.
  const uint32_t expected_dma_priv_bytes = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);
  void* dma_priv_ptr = dev ? dev->wddm_context.pDmaBufferPrivateData : nullptr;
  uint32_t dma_priv_bytes = dev ? dev->wddm_context.DmaBufferPrivateDataSize : 0;
  if constexpr (has_member_pDmaBufferPrivateData<Arg>::value) {
    dma_priv_ptr = args.pDmaBufferPrivateData;
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    dma_priv_bytes = args.DmaBufferPrivateDataSize;
  }

  if (!InitWin7DmaBufferPrivateData(dma_priv_ptr, dma_priv_bytes, is_present)) {
    std::call_once(g_dma_priv_invalid_once, [dma_priv_ptr, dma_priv_bytes, expected_dma_priv_bytes] {
      aerogpu::logf("aerogpu-d3d9: submit missing/invalid dma private data ptr=%p bytes=%u (need >=%u)\n",
                    dma_priv_ptr,
                    static_cast<unsigned>(dma_priv_bytes),
                    static_cast<unsigned>(expected_dma_priv_bytes));
    });
    return E_INVALIDARG;
  }

  // Safety: if the runtime reports a larger private-data size than the KMD/UMD
  // contract, clamp to the expected size so dxgkrnl does not copy extra bytes of
  // user-mode memory into kernel-mode buffers.
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    const uint32_t runtime_bytes = dev ? static_cast<uint32_t>(dev->wddm_context.DmaBufferPrivateDataSize) : 0;
    if (runtime_bytes > expected_dma_priv_bytes) {
      std::call_once(g_dma_priv_size_mismatch_once, [runtime_bytes, expected_dma_priv_bytes] {
        aerogpu::logf("aerogpu-d3d9: runtime DmaBufferPrivateDataSize=%u (expected=%u); clamping\n",
                      static_cast<unsigned>(runtime_bytes),
                      static_cast<unsigned>(expected_dma_priv_bytes));
      });
    }
    if (args.DmaBufferPrivateDataSize > expected_dma_priv_bytes) {
      args.DmaBufferPrivateDataSize = expected_dma_priv_bytes;
    }
  }

  uint64_t submission_fence = 0;

  HRESULT hr = E_FAIL;
  if constexpr (has_member_SubmissionFenceId<Arg>::value) {
    using FenceMemberT = std::remove_reference_t<decltype(args.SubmissionFenceId)>;
    using FenceStorageT = std::remove_pointer_t<FenceMemberT>;
    FenceStorageT fence_storage{};

    if constexpr (std::is_pointer<FenceMemberT>::value) {
      // Some header/interface versions expose SubmissionFenceId as an output
      // pointer rather than an in-struct value. Provide a valid storage location
      // so the runtime can write back the exact per-submission fence ID.
      args.SubmissionFenceId = &fence_storage;
    } else {
      args.SubmissionFenceId = 0;
    }

    hr = cb(static_cast<ArgPtr>(&args));
    if (SUCCEEDED(hr)) {
      if constexpr (has_member_NewFenceValue<Arg>::value) {
        if (args.NewFenceValue) {
          submission_fence = static_cast<uint64_t>(args.NewFenceValue);
        } else if constexpr (std::is_pointer<FenceMemberT>::value) {
          submission_fence = static_cast<uint64_t>(fence_storage);
        } else {
          submission_fence = static_cast<uint64_t>(args.SubmissionFenceId);
        }
      } else {
        if constexpr (std::is_pointer<FenceMemberT>::value) {
          submission_fence = static_cast<uint64_t>(fence_storage);
        } else {
          submission_fence = static_cast<uint64_t>(args.SubmissionFenceId);
        }
      }
    }
  } else if constexpr (has_member_pSubmissionFenceId<Arg>::value) {
    using FenceMemberT = std::remove_reference_t<decltype(args.pSubmissionFenceId)>;
    using FenceStorageT = std::remove_pointer_t<FenceMemberT>;
    FenceStorageT fence_storage{};

    if constexpr (std::is_pointer<FenceMemberT>::value) {
      args.pSubmissionFenceId = &fence_storage;
    }

    hr = cb(static_cast<ArgPtr>(&args));
    if (SUCCEEDED(hr)) {
      if constexpr (has_member_NewFenceValue<Arg>::value) {
        if (args.NewFenceValue) {
          submission_fence = static_cast<uint64_t>(args.NewFenceValue);
        } else if constexpr (std::is_pointer<FenceMemberT>::value) {
          submission_fence = static_cast<uint64_t>(fence_storage);
        }
      } else if constexpr (std::is_pointer<FenceMemberT>::value) {
        submission_fence = static_cast<uint64_t>(fence_storage);
      }
    }
  } else if constexpr (has_member_pFenceValue<Arg>::value) {
    using FenceMemberT = std::remove_reference_t<decltype(args.pFenceValue)>;
    using FenceStorageT = std::remove_pointer_t<FenceMemberT>;
    FenceStorageT fence_storage{};

    if constexpr (std::is_pointer<FenceMemberT>::value) {
      args.pFenceValue = &fence_storage;
    }

    hr = cb(static_cast<ArgPtr>(&args));
    if (SUCCEEDED(hr)) {
      if constexpr (has_member_NewFenceValue<Arg>::value) {
        if (args.NewFenceValue) {
          submission_fence = static_cast<uint64_t>(args.NewFenceValue);
        } else if constexpr (std::is_pointer<FenceMemberT>::value) {
          submission_fence = static_cast<uint64_t>(fence_storage);
        }
      } else if constexpr (std::is_pointer<FenceMemberT>::value) {
        submission_fence = static_cast<uint64_t>(fence_storage);
      }
    }
  } else if constexpr (has_member_FenceValue<Arg>::value) {
    args.FenceValue = 0;
    hr = cb(static_cast<ArgPtr>(&args));
    if (SUCCEEDED(hr)) {
      if constexpr (has_member_NewFenceValue<Arg>::value) {
        if (args.NewFenceValue) {
          submission_fence = static_cast<uint64_t>(args.NewFenceValue);
        } else {
          submission_fence = static_cast<uint64_t>(args.FenceValue);
        }
      } else {
        submission_fence = static_cast<uint64_t>(args.FenceValue);
      }
    }
  } else {
    hr = cb(static_cast<ArgPtr>(&args));
    if (SUCCEEDED(hr)) {
      if constexpr (has_member_NewFenceValue<Arg>::value) {
        submission_fence = static_cast<uint64_t>(args.NewFenceValue);
      }
    }
  }

  if (FAILED(hr)) {
    return hr;
  }

  if (out_submission_fence) {
    *out_submission_fence = submission_fence;
  }

  // The runtime may rotate command buffers/lists after a submission. Preserve the
  // updated pointers and reset the book-keeping so the next submission starts
  // from a clean command stream header.
  update_context_from_submit_args(dev, args);
  // Keep the command stream writer bound to the currently active command buffer.
  // The runtime is allowed to return a new DMA buffer pointer/size in the
  // callback out-params; failing to rebind would cause us to write into a stale
  // buffer on the next submission.
  if (dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= sizeof(aerogpu_cmd_stream_header)) {
    dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
  }
  dev->wddm_context.reset_submission_buffers();
  return hr;
}
#endif
} // namespace

#if defined(_WIN32)
template <typename CallbackFn>
void wddm_deallocate_buffers_impl(Device* dev,
                                  CallbackFn cb,
                                  void* dma_buffer,
                                  void* command_buffer,
                                  WddmAllocationList* allocation_list,
                                  WddmPatchLocationList* patch_location_list,
                                  void* dma_priv,
                                  uint32_t dma_priv_bytes) {
  if (!dev || !cb) {
    return;
  }

  using ArgPtr = typename fn_first_param<CallbackFn>::type;
  using Arg = std::remove_const_t<std::remove_pointer_t<ArgPtr>>;
  Arg args;
  std::memset(&args, 0, sizeof(args));

  if constexpr (has_member_hContext<Arg>::value) {
    args.hContext = dev->wddm_context.hContext;
  }
  if constexpr (has_member_hDevice<Arg>::value) {
    args.hDevice = dev->wddm_device;
  }
  if constexpr (has_member_pDmaBuffer<Arg>::value) {
    args.pDmaBuffer = dma_buffer;
  }
  if constexpr (has_member_pCommandBuffer<Arg>::value) {
    args.pCommandBuffer = command_buffer;
  }
  if constexpr (has_member_pAllocationList<Arg>::value) {
    args.pAllocationList = allocation_list;
  }
  if constexpr (has_member_pPatchLocationList<Arg>::value) {
    args.pPatchLocationList = patch_location_list;
  }
  if constexpr (has_member_pDmaBufferPrivateData<Arg>::value) {
    args.pDmaBufferPrivateData = dma_priv;
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    args.DmaBufferPrivateDataSize = dma_priv_bytes;
  }

  (void)cb(static_cast<ArgPtr>(&args));
}

void wddm_deallocate_active_buffers(Device* dev) {
  if (!dev || !dev->adapter) {
    return;
  }
  if (dev->wddm_context.hContext == 0 || !dev->wddm_context.buffers_need_deallocate) {
    return;
  }

  // Snapshot the pointers returned by AllocateCb (the submit callback is allowed
  // to rotate the context's live pointers).
  void* dma_buffer = dev->wddm_context.allocated_pDmaBuffer;
  void* cmd_buffer = dev->wddm_context.allocated_pCommandBuffer;
  WddmAllocationList* alloc_list = dev->wddm_context.allocated_pAllocationList;
  WddmPatchLocationList* patch_list = dev->wddm_context.allocated_pPatchLocationList;
  void* dma_priv = dev->wddm_context.allocated_pDmaBufferPrivateData;
  uint32_t dma_priv_bytes = dev->wddm_context.allocated_DmaBufferPrivateDataSize;
  const bool dma_priv_from_allocate = dev->wddm_context.dma_priv_from_allocate;

  if constexpr (has_pfnDeallocateCb<WddmDeviceCallbacks>::value) {
    if (dev->wddm_callbacks.pfnDeallocateCb) {
      wddm_deallocate_buffers_impl(dev, dev->wddm_callbacks.pfnDeallocateCb, dma_buffer, cmd_buffer, alloc_list, patch_list, dma_priv, dma_priv_bytes);
    }
  }

  // Prevent use-after-free on any deallocated runtime-provided buffers.
  //
  // In the AllocateCb/DeallocateCb acquisition model, treat any "rotated" submit
  // buffer pointers (pNewCommandBuffer/pNewAllocationList/...) as advisory: once
  // we return the AllocateCb buffers, the rotated pointers are not guaranteed to
  // remain valid. Force the next `ensure_cmd_space()` to reacquire buffers via
  // GetCommandBufferCb/AllocateCb.
  dev->wddm_context.pDmaBuffer = nullptr;
  dev->wddm_context.pCommandBuffer = nullptr;
  dev->wddm_context.CommandBufferSize = 0;
  dev->wddm_context.pAllocationList = nullptr;
  dev->wddm_context.AllocationListSize = 0;
  dev->wddm_context.pPatchLocationList = nullptr;
  dev->wddm_context.PatchLocationListSize = 0;
  if (dma_priv_from_allocate || (dma_priv && dev->wddm_context.pDmaBufferPrivateData == dma_priv)) {
    dev->wddm_context.pDmaBufferPrivateData = nullptr;
    dev->wddm_context.DmaBufferPrivateDataSize = 0;
  }
  dev->wddm_context.dma_priv_from_allocate = false;

  dev->wddm_context.buffers_need_deallocate = false;
  dev->wddm_context.allocated_pDmaBuffer = nullptr;
  dev->wddm_context.allocated_pCommandBuffer = nullptr;
  dev->wddm_context.allocated_pAllocationList = nullptr;
  dev->wddm_context.allocated_pPatchLocationList = nullptr;
  dev->wddm_context.allocated_pDmaBufferPrivateData = nullptr;
  dev->wddm_context.allocated_DmaBufferPrivateDataSize = 0;

  dev->cmd.set_span(nullptr, 0);
  dev->alloc_list_tracker.rebind(nullptr, 0, dev->adapter->max_allocation_list_slot_id);
}

template <typename CallbackFn>
HRESULT wddm_acquire_submit_buffers_allocate_impl(Device* dev, CallbackFn cb, uint32_t request_bytes) {
  if (!dev || !dev->adapter || !cb) {
    return E_INVALIDARG;
  }

  using ArgPtr = typename fn_first_param<CallbackFn>::type;
  using Arg = std::remove_const_t<std::remove_pointer_t<ArgPtr>>;

  Arg args;
  std::memset(&args, 0, sizeof(args));
  if constexpr (has_member_hContext<Arg>::value) {
    args.hContext = dev->wddm_context.hContext;
  }
  if constexpr (has_member_hDevice<Arg>::value) {
    args.hDevice = dev->wddm_device;
  }
  if constexpr (has_member_DmaBufferSize<Arg>::value) {
    args.DmaBufferSize = request_bytes;
  }
  if constexpr (has_member_CommandBufferSize<Arg>::value) {
    args.CommandBufferSize = request_bytes;
  }
  if constexpr (has_member_AllocationListSize<Arg>::value) {
    // Some runtimes treat AllocationListSize as an input (capacity request) and
    // will fail or return a 0-sized list if it is left at 0. Request a generous
    // default so allocation tracking can work even when CreateContext did not
    // provide a persistent allocation list.
    uint32_t request_entries = std::max<uint32_t>(
        dev->wddm_context.AllocationListSize ? dev->wddm_context.AllocationListSize : 0u, 4096u);
    // We assign allocation-list slot IDs densely as 0..N-1. Clamp the requested
    // list size to the KMD-advertised max slot ID (+1) so we don't ask the
    // runtime for more entries than we can legally reference.
    if (dev->adapter && dev->adapter->max_allocation_list_slot_id != std::numeric_limits<uint32_t>::max()) {
      request_entries = std::min<uint32_t>(request_entries, dev->adapter->max_allocation_list_slot_id + 1u);
    }
    args.AllocationListSize = request_entries;
  }
  if constexpr (has_member_PatchLocationListSize<Arg>::value) {
    args.PatchLocationListSize = 0;
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    // Ensure the runtime allocates enough DMA private data for the Win7 AeroGPU
    // contract (AEROGPU_DMA_PRIV).
    args.DmaBufferPrivateDataSize = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);
  }

  const HRESULT hr = cb(static_cast<ArgPtr>(&args));

  void* cmd_ptr = nullptr;
  void* dma_ptr = nullptr;
  uint32_t cap = 0;
  bool cap_from_dma_buffer_size = false;

  if constexpr (has_member_pDmaBuffer<Arg>::value) {
    dma_ptr = args.pDmaBuffer;
    cmd_ptr = args.pDmaBuffer;
  }
  if constexpr (has_member_pCommandBuffer<Arg>::value) {
    if (args.pCommandBuffer) {
      cmd_ptr = args.pCommandBuffer;
    }
  }
  if constexpr (has_member_DmaBufferSize<Arg>::value) {
    cap = static_cast<uint32_t>(args.DmaBufferSize);
    cap_from_dma_buffer_size = (cap != 0);
  }
  if constexpr (has_member_CommandBufferSize<Arg>::value) {
    if (cap == 0) {
      cap = static_cast<uint32_t>(args.CommandBufferSize);
    }
  }
  if (!cmd_ptr) {
    cmd_ptr = dma_ptr;
  }
  if (!dma_ptr) {
    dma_ptr = cmd_ptr;
  }
  if (cap_from_dma_buffer_size) {
    cap = AdjustCommandBufferSizeFromDmaBuffer(dma_ptr, cmd_ptr, cap);
  }

  WddmAllocationList* alloc_list = nullptr;
  uint32_t alloc_entries = 0;
  if constexpr (has_member_pAllocationList<Arg>::value) {
    alloc_list = args.pAllocationList;
  }
  if constexpr (has_member_AllocationListSize<Arg>::value) {
    alloc_entries = static_cast<uint32_t>(args.AllocationListSize);
  }

  WddmPatchLocationList* patch_list = nullptr;
  uint32_t patch_entries = 0;
  if constexpr (has_member_pPatchLocationList<Arg>::value) {
    patch_list = args.pPatchLocationList;
  }
  if constexpr (has_member_PatchLocationListSize<Arg>::value) {
    patch_entries = static_cast<uint32_t>(args.PatchLocationListSize);
  }

  void* dma_priv = nullptr;
  uint32_t dma_priv_bytes = 0;
  if constexpr (has_member_pDmaBufferPrivateData<Arg>::value) {
    dma_priv = args.pDmaBufferPrivateData;
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    dma_priv_bytes = static_cast<uint32_t>(args.DmaBufferPrivateDataSize);
  }
  const uint32_t expected_dma_priv_bytes = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);
  if (dma_priv && dma_priv_bytes == 0) {
    dma_priv_bytes = expected_dma_priv_bytes;
  }

  if (FAILED(hr) || !cmd_ptr || cap == 0 || !alloc_list || alloc_entries == 0) {
    if constexpr (has_pfnDeallocateCb<WddmDeviceCallbacks>::value) {
      if (dev->wddm_callbacks.pfnDeallocateCb) {
        if (cmd_ptr || dma_ptr || alloc_list || patch_list || dma_priv) {
          wddm_deallocate_buffers_impl(dev,
                                       dev->wddm_callbacks.pfnDeallocateCb,
                                       dma_ptr,
                                       cmd_ptr,
                                       alloc_list,
                                       patch_list,
                                       dma_priv,
                                       dma_priv_bytes);
        }
      }
    }
    return FAILED(hr) ? hr : E_OUTOFMEMORY;
  }

  dev->wddm_context.buffers_need_deallocate = true;
  dev->wddm_context.allocated_pDmaBuffer = dma_ptr;
  dev->wddm_context.allocated_pCommandBuffer = cmd_ptr;
  dev->wddm_context.allocated_pAllocationList = alloc_list;
  dev->wddm_context.allocated_pPatchLocationList = patch_list;
  dev->wddm_context.allocated_pDmaBufferPrivateData = dma_priv;
  dev->wddm_context.allocated_DmaBufferPrivateDataSize = dma_priv_bytes;

  dev->wddm_context.pDmaBuffer = static_cast<uint8_t*>(dma_ptr ? dma_ptr : cmd_ptr);
  dev->wddm_context.pCommandBuffer = static_cast<uint8_t*>(cmd_ptr);
  dev->wddm_context.CommandBufferSize = cap;
  dev->wddm_context.pAllocationList = alloc_list;
  dev->wddm_context.AllocationListSize = alloc_entries;
  dev->wddm_context.pPatchLocationList = patch_list;
  dev->wddm_context.PatchLocationListSize = patch_entries;

  // Prefer the per-buffer DMA private data returned by AllocateCb when it is
  // available. Some runtimes associate this blob with the allocated DMA buffer
  // and may rotate it alongside the command buffer.
  if (dma_priv && dma_priv_bytes >= expected_dma_priv_bytes) {
    dev->wddm_context.pDmaBufferPrivateData = dma_priv;
    dev->wddm_context.DmaBufferPrivateDataSize = dma_priv_bytes;
    dev->wddm_context.dma_priv_from_allocate = true;
  } else {
    dev->wddm_context.dma_priv_from_allocate = false;
  }

  dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
  dev->wddm_context.reset_submission_buffers();
  dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                 dev->wddm_context.AllocationListSize,
                                 dev->adapter->max_allocation_list_slot_id);
  return S_OK;
}

template <typename CallbackFn>
HRESULT wddm_acquire_submit_buffers_get_command_buffer_impl(Device* dev, CallbackFn cb) {
  if (!dev || !dev->adapter || !cb) {
    return E_INVALIDARG;
  }

  const uint32_t expected_dma_priv_bytes = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);

  using ArgPtr = typename fn_first_param<CallbackFn>::type;
  using Arg = std::remove_const_t<std::remove_pointer_t<ArgPtr>>;

  Arg args;
  std::memset(&args, 0, sizeof(args));
  if constexpr (has_member_hContext<Arg>::value) {
    args.hContext = dev->wddm_context.hContext;
  }
  if constexpr (has_member_hDevice<Arg>::value) {
    args.hDevice = dev->wddm_device;
  }

  const HRESULT hr = cb(static_cast<ArgPtr>(&args));
  if (FAILED(hr)) {
    return hr;
  }

  void* cmd_ptr = nullptr;
  void* dma_ptr = nullptr;
  uint32_t cap = 0;
  bool cap_from_dma_buffer_size = false;
  if constexpr (has_member_pDmaBuffer<Arg>::value) {
    dma_ptr = args.pDmaBuffer;
    cmd_ptr = args.pDmaBuffer;
  }
  if constexpr (has_member_pCommandBuffer<Arg>::value) {
    if (args.pCommandBuffer) {
      cmd_ptr = args.pCommandBuffer;
    }
  }
  if constexpr (has_member_CommandBufferSize<Arg>::value) {
    cap = static_cast<uint32_t>(args.CommandBufferSize);
  }
  if constexpr (has_member_DmaBufferSize<Arg>::value) {
    if (cap == 0) {
      cap = static_cast<uint32_t>(args.DmaBufferSize);
      cap_from_dma_buffer_size = (cap != 0);
    }
  }
  if (!cmd_ptr) {
    cmd_ptr = dma_ptr;
  }
  if (!dma_ptr) {
    dma_ptr = cmd_ptr;
  }
  if (cap_from_dma_buffer_size) {
    cap = AdjustCommandBufferSizeFromDmaBuffer(dma_ptr, cmd_ptr, cap);
  }

  // Some runtimes only return the new command buffer via GetCommandBufferCb and
  // keep the allocation/patch lists stable from CreateContext. Start from the
  // current context pointers and override with any callback-provided values.
  WddmAllocationList* alloc_list = dev->wddm_context.pAllocationList;
  uint32_t alloc_entries = dev->wddm_context.AllocationListSize;
  if constexpr (has_member_pAllocationList<Arg>::value) {
    if (args.pAllocationList) {
      alloc_list = args.pAllocationList;
    }
  }
  if constexpr (has_member_AllocationListSize<Arg>::value) {
    if (args.AllocationListSize) {
      alloc_entries = static_cast<uint32_t>(args.AllocationListSize);
    }
  }

  WddmPatchLocationList* patch_list = dev->wddm_context.pPatchLocationList;
  uint32_t patch_entries = dev->wddm_context.PatchLocationListSize;
  if constexpr (has_member_pPatchLocationList<Arg>::value) {
    if (args.pPatchLocationList) {
      patch_list = args.pPatchLocationList;
    }
  }
  if constexpr (has_member_PatchLocationListSize<Arg>::value) {
    if (args.PatchLocationListSize) {
      patch_entries = static_cast<uint32_t>(args.PatchLocationListSize);
    }
  }

  void* dma_priv = dev->wddm_context.pDmaBufferPrivateData;
  uint32_t dma_priv_bytes = dev->wddm_context.DmaBufferPrivateDataSize;
  if constexpr (has_member_pDmaBufferPrivateData<Arg>::value) {
    if (args.pDmaBufferPrivateData) {
      dma_priv = args.pDmaBufferPrivateData;
    }
  }
  if constexpr (has_member_DmaBufferPrivateDataSize<Arg>::value) {
    if (args.DmaBufferPrivateDataSize) {
      dma_priv_bytes = static_cast<uint32_t>(args.DmaBufferPrivateDataSize);
    }
  }
  if (dma_priv && dma_priv_bytes == 0) {
    dma_priv_bytes = expected_dma_priv_bytes;
  }

  // Validate the required submission contract. If GetCommandBufferCb cannot
  // provide it, return a failure so callers can fall back to AllocateCb.
  if (!cmd_ptr || cap == 0 || !alloc_list || alloc_entries == 0) {
    return E_OUTOFMEMORY;
  }
  if (!dma_priv || dma_priv_bytes < expected_dma_priv_bytes) {
    return E_OUTOFMEMORY;
  }

  dev->wddm_context.buffers_need_deallocate = false;
  dev->wddm_context.allocated_pDmaBuffer = nullptr;
  dev->wddm_context.allocated_pCommandBuffer = nullptr;
  dev->wddm_context.allocated_pAllocationList = nullptr;
  dev->wddm_context.allocated_pPatchLocationList = nullptr;
  dev->wddm_context.allocated_pDmaBufferPrivateData = nullptr;
  dev->wddm_context.allocated_DmaBufferPrivateDataSize = 0;

  dev->wddm_context.pDmaBuffer = static_cast<uint8_t*>(dma_ptr ? dma_ptr : cmd_ptr);
  dev->wddm_context.pCommandBuffer = static_cast<uint8_t*>(cmd_ptr);
  dev->wddm_context.CommandBufferSize = cap;
  dev->wddm_context.pAllocationList = alloc_list;
  dev->wddm_context.AllocationListSize = alloc_entries;
  dev->wddm_context.pPatchLocationList = patch_list;
  dev->wddm_context.PatchLocationListSize = patch_entries;

  // Treat DMA private data as an in/out pointer: GetCommandBufferCb may rotate it
  // alongside the command buffer.
  dev->wddm_context.pDmaBufferPrivateData = dma_priv;
  dev->wddm_context.DmaBufferPrivateDataSize = dma_priv_bytes;
  dev->wddm_context.dma_priv_from_allocate = false;

  dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
  dev->wddm_context.reset_submission_buffers();
  dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                 dev->wddm_context.AllocationListSize,
                                 dev->adapter->max_allocation_list_slot_id);
  return S_OK;
}

bool wddm_ensure_recording_buffers(Device* dev, size_t bytes_needed) {
  if (!dev || !dev->adapter) {
    return false;
  }
  if (dev->wddm_context.hContext == 0) {
    return true;
  }

  const uint32_t expected_dma_priv_bytes = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);
  // All command packets are 4-byte aligned and must at minimum contain a packet
  // header. Ensure the DMA buffer is large enough for the stream header plus at
  // least one packet header (or the caller's requested packet size).
  const size_t min_packet = align_up(sizeof(aerogpu_cmd_hdr), 4);
  const size_t packet_bytes = std::max(bytes_needed, min_packet);
  const size_t min_buffer_bytes_sz = sizeof(aerogpu_cmd_stream_header) + packet_bytes;
  if (min_buffer_bytes_sz > std::numeric_limits<uint32_t>::max()) {
    return false;
  }
  const uint32_t min_buffer_bytes = static_cast<uint32_t>(min_buffer_bytes_sz);
  const bool have_persistent_buffers =
      dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= min_buffer_bytes &&
      dev->wddm_context.pAllocationList &&
      dev->wddm_context.AllocationListSize != 0 &&
      dev->wddm_context.pDmaBufferPrivateData &&
      dev->wddm_context.DmaBufferPrivateDataSize >= expected_dma_priv_bytes;

  if (have_persistent_buffers) {
    // Ensure the writer + allocation list tracker are bound to the active runtime
    // buffers (the runtime is allowed to rotate pointers after a submit).
    if (!dev->wddm_context.pDmaBuffer) {
      dev->wddm_context.pDmaBuffer = dev->wddm_context.pCommandBuffer;
    }
    if (dev->cmd.data() != dev->wddm_context.pCommandBuffer) {
      dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
    }

    const bool needs_alloc_list_rebind =
        dev->alloc_list_tracker.list_base() != reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList) ||
        dev->alloc_list_tracker.list_capacity() != dev->wddm_context.AllocationListSize;
    if (needs_alloc_list_rebind) {
      std::vector<AllocationListTracker::TrackedAllocation> preserved_allocs;
      if (dev->cmd.empty() && dev->alloc_list_tracker.list_len() != 0) {
        preserved_allocs = dev->alloc_list_tracker.snapshot_tracked_allocations();
        if (preserved_allocs.empty()) {
          logf("aerogpu-d3d9: failed to snapshot tracked allocations (OOM)\n");
          return false;
        }
      }
      dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                     dev->wddm_context.AllocationListSize,
                                     dev->adapter->max_allocation_list_slot_id);
      if (!preserved_allocs.empty() && !dev->alloc_list_tracker.replay_tracked_allocations(preserved_allocs)) {
        logf("aerogpu-d3d9: failed to replay tracked allocations after allocation-list pointer rotation (count=%llu)\n",
             static_cast<unsigned long long>(preserved_allocs.size()));
        return false;
      }
    }
    return true;
  }

  // If we already have a valid submission contract (DMA buffer + allocation list
  // + private data), but the current DMA buffer is too small for the next packet,
  // do *not* reacquire submit buffers here when the command stream is non-empty.
  //
  // `ensure_cmd_space()` will notice the insufficient remaining space, submit the
  // in-flight DMA buffer, and call back into `wddm_ensure_recording_buffers()` for
  // a fresh buffer. Reacquiring here would discard already-recorded packets.
  const bool have_submit_contract =
      dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= sizeof(aerogpu_cmd_stream_header) &&
      dev->wddm_context.pAllocationList &&
      dev->wddm_context.AllocationListSize != 0 &&
      dev->wddm_context.pDmaBufferPrivateData &&
      dev->wddm_context.DmaBufferPrivateDataSize >= expected_dma_priv_bytes;
  if (have_submit_contract && !dev->cmd.empty() && dev->wddm_context.CommandBufferSize < min_buffer_bytes) {
    return true;
  }

  // If the caller tracked allocations for a packet but hasn't emitted any command
  // buffer bytes yet (cmd stream still empty), a submit-buffer re-acquire would
  // rebind/reset the allocation-list tracker and drop those tracked alloc_ids.
  //
  // Preserve and replay the tracked allocations across the re-acquire so
  // subsequent packet emission still has a correct allocation table.
  std::vector<AllocationListTracker::TrackedAllocation> preserved_allocs;
  if (dev->cmd.empty() && dev->alloc_list_tracker.list_len() != 0) {
    preserved_allocs = dev->alloc_list_tracker.snapshot_tracked_allocations();
    if (preserved_allocs.empty()) {
      logf("aerogpu-d3d9: failed to snapshot tracked allocations (OOM)\n");
      return false;
    }
  }

  // If AllocateCb handed us buffers but we never emitted anything, return them
  // before acquiring a new set.
  if (dev->wddm_context.buffers_need_deallocate && dev->cmd.empty()) {
    wddm_deallocate_active_buffers(dev);
  }

  // AllocateCb/DeallocateCb acquisition uses the requested buffer sizes as a hint
  // (and some runtimes treat them as a hard requirement). Request at least one
  // page so the returned command buffer can accommodate common fixed-size packets
  // after an allocation-list split without immediately forcing a re-acquire
  // (which would rebind/reset the allocation list tracker and drop tracked
  // alloc_ids).
  const uint32_t request_bytes = std::max<uint32_t>(min_buffer_bytes, 4096u);

  // Prefer GetCommandBufferCb when available; fall back to AllocateCb for older
  // runtimes that require explicit per-submit allocation + DeallocateCb.
  bool tried_get_command_buffer = false;
  HRESULT get_command_buffer_hr = E_NOTIMPL;
  HRESULT hr = E_NOTIMPL;
  if constexpr (has_pfnGetCommandBufferCb<WddmDeviceCallbacks>::value) {
    if (dev->wddm_callbacks.pfnGetCommandBufferCb) {
      tried_get_command_buffer = true;
      get_command_buffer_hr = wddm_acquire_submit_buffers_get_command_buffer_impl(dev, dev->wddm_callbacks.pfnGetCommandBufferCb);
      hr = get_command_buffer_hr;
    }
  }
  // If GetCommandBufferCb succeeds but returns an undersized buffer for the
  // current packet, allow AllocateCb to satisfy the minimum size.
  if (SUCCEEDED(hr)) {
    const bool have_required =
        dev->wddm_context.pCommandBuffer &&
        dev->wddm_context.CommandBufferSize >= min_buffer_bytes &&
        dev->wddm_context.pAllocationList &&
        dev->wddm_context.AllocationListSize != 0 &&
        dev->wddm_context.pDmaBufferPrivateData &&
        dev->wddm_context.DmaBufferPrivateDataSize >= expected_dma_priv_bytes;
    if (!have_required) {
      if (tried_get_command_buffer) {
        static std::once_flag log_once;
        const void* cmd_ptr = dev->wddm_context.pCommandBuffer;
        const uint32_t cmd_bytes = dev->wddm_context.CommandBufferSize;
        const void* alloc_ptr = dev->wddm_context.pAllocationList;
        const uint32_t alloc_entries = dev->wddm_context.AllocationListSize;
        const void* dma_priv_ptr = dev->wddm_context.pDmaBufferPrivateData;
        const uint32_t dma_priv_bytes = dev->wddm_context.DmaBufferPrivateDataSize;
        std::call_once(log_once,
                       [cmd_ptr, cmd_bytes, min_buffer_bytes, alloc_ptr, alloc_entries, dma_priv_ptr, dma_priv_bytes, expected_dma_priv_bytes] {
          aerogpu::logf("aerogpu-d3d9: GetCommandBufferCb returned incomplete/undersized buffers; "
                        "falling back to AllocateCb (cmd=%p bytes=%u need=%u alloc=%p entries=%u dma_priv=%p bytes=%u need>=%u)\n",
                        cmd_ptr,
                        static_cast<unsigned>(cmd_bytes),
                        static_cast<unsigned>(min_buffer_bytes),
                        alloc_ptr,
                        static_cast<unsigned>(alloc_entries),
                        dma_priv_ptr,
                        static_cast<unsigned>(dma_priv_bytes),
                        static_cast<unsigned>(expected_dma_priv_bytes));
        });
      }
      hr = E_FAIL;
    }
  }
  if (FAILED(hr)) {
    if (tried_get_command_buffer && FAILED(get_command_buffer_hr)) {
      static std::once_flag log_once;
      const unsigned hr_code = static_cast<unsigned>(get_command_buffer_hr);
      std::call_once(log_once, [hr_code] {
        aerogpu::logf("aerogpu-d3d9: GetCommandBufferCb failed hr=0x%08x; falling back to AllocateCb\n", hr_code);
      });
    }
    HRESULT allocate_hr = E_NOTIMPL;
    if constexpr (has_pfnAllocateCb<WddmDeviceCallbacks>::value && has_pfnDeallocateCb<WddmDeviceCallbacks>::value) {
      if (dev->wddm_callbacks.pfnAllocateCb && dev->wddm_callbacks.pfnDeallocateCb) {
        allocate_hr = wddm_acquire_submit_buffers_allocate_impl(dev, dev->wddm_callbacks.pfnAllocateCb, request_bytes);
        hr = allocate_hr;
      }
    }
    if (FAILED(hr)) {
      static std::once_flag log_once;
      const unsigned get_hr_code = static_cast<unsigned>(get_command_buffer_hr);
      const unsigned alloc_hr_code = static_cast<unsigned>(allocate_hr);
      std::call_once(log_once, [get_hr_code, alloc_hr_code] {
        aerogpu::logf("aerogpu-d3d9: failed to acquire WDDM submit buffers (GetCommandBufferCb hr=0x%08x AllocateCb hr=0x%08x)\n",
                      get_hr_code,
                      alloc_hr_code);
      });
    }
  }
  if (FAILED(hr)) {
    return false;
  }

  // Re-check required buffers.
  const bool have_required =
      dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= min_buffer_bytes &&
      dev->wddm_context.pAllocationList &&
      dev->wddm_context.AllocationListSize != 0 &&
      dev->wddm_context.pDmaBufferPrivateData &&
      dev->wddm_context.DmaBufferPrivateDataSize >= expected_dma_priv_bytes;
  if (!have_required && dev->wddm_context.buffers_need_deallocate) {
    // Prevent leaking AllocateCb-owned buffers if the runtime did not return the
    // full submission contract (e.g. missing DMA private data).
    wddm_deallocate_active_buffers(dev);
  }
  if (have_required && !preserved_allocs.empty()) {
    if (!dev->alloc_list_tracker.replay_tracked_allocations(preserved_allocs)) {
      logf("aerogpu-d3d9: failed to replay tracked allocations after submit-buffer re-acquire (count=%llu)\n",
           static_cast<unsigned long long>(preserved_allocs.size()));
      if (dev->wddm_context.buffers_need_deallocate) {
        wddm_deallocate_active_buffers(dev);
      }
      return false;
    }
  }
  return have_required;
}
#endif // _WIN32

static void resolve_pending_event_queries(Device* dev, uint64_t fence_value) {
  if (!dev) {
    return;
  }
  if (dev->pending_event_queries.empty()) {
    return;
  }

  for (Query* q : dev->pending_event_queries) {
    if (!q) {
      continue;
    }
    // Some call sites may pre-populate the fence value (e.g. when Issue(END)
    // submits work but we intentionally defer making the query "ready" until a
    // later boundary). Only stamp when still unset.
    if (q->fence_value.load(std::memory_order_relaxed) == 0) {
      q->fence_value.store(fence_value, std::memory_order_release);
    }
    q->submitted.store(true, std::memory_order_release);
  }
  dev->pending_event_queries.clear();
}

uint64_t submit(Device* dev, bool is_present) {
  if (!dev) {
    return 0;
  }

  Adapter* adapter = dev->adapter;
  if (!adapter) {
    return 0;
  }
  if (device_is_lost(dev)) {
    // Device is lost: discard any queued work and keep returning the last known
    // good fence. Key DDIs will translate the lost state into a stable HRESULT
    // (D3DERR_DEVICELOST) rather than returning fence==0 and pretending the GPU
    // made forward progress.
    //
    // Callers must hold `Device::mutex`.
#if defined(_WIN32)
    if (dev->wddm_context.buffers_need_deallocate) {
      wddm_deallocate_active_buffers(dev);
    }
#endif
    dev->cmd.rewind();
    dev->alloc_list_tracker.reset();
    dev->wddm_context.reset_submission_buffers();
    return dev->last_submission_fence;
  }

  if (dev->cmd.empty()) {
    // Even if there's nothing to submit, callers may use submit() as a "split"
    // point when the per-submit allocation list is full. Reset submission-local
    // tracking state so subsequent commands start with a fresh allocation list
    // without issuing an empty DMA buffer to the kernel.
#if defined(_WIN32)
    if (dev->wddm_context.buffers_need_deallocate) {
      wddm_deallocate_active_buffers(dev);
    }
#endif
    const uint64_t fence = dev->last_submission_fence;
    resolve_pending_event_queries(dev, fence);
    dev->cmd.rewind();
    dev->alloc_list_tracker.reset();
    dev->wddm_context.reset_submission_buffers();
    return fence;
  }

  dev->cmd.finalize();
  const uint64_t cmd_bytes = static_cast<uint64_t>(dev->cmd.size());

  bool submitted_to_kmd = false;
  uint64_t submission_fence = 0;
  bool did_submit = false;
#if defined(_WIN32)
  // WDDM submission path: hand the runtime-provided DMA/alloc list buffers back
  // to dxgkrnl via the device callbacks captured at CreateDevice time.
  //
  // The patch-location list is intentionally kept empty; guest-backed memory is
  // referenced via stable `alloc_id` values and resolved by the KMD's per-submit
  // allocation table.
  if (dev->wddm_context.hContext != 0 && dev->wddm_context.pCommandBuffer && dev->wddm_context.CommandBufferSize) {
    if (cmd_bytes <= dev->wddm_context.CommandBufferSize) {
      // CmdStreamWriter can be span-backed and write directly into the runtime
      // DMA buffer. Avoid memcpy on identical ranges (overlap is UB for memcpy).
      if (dev->cmd.data() != dev->wddm_context.pCommandBuffer) {
        std::memcpy(dev->wddm_context.pCommandBuffer, dev->cmd.data(), static_cast<size_t>(cmd_bytes));
      }
      dev->wddm_context.command_buffer_bytes_used = static_cast<uint32_t>(cmd_bytes);
      dev->wddm_context.allocation_list_entries_used = dev->alloc_list_tracker.list_len();
      dev->wddm_context.patch_location_entries_used = 0;
      const uint32_t allocs_used = dev->wddm_context.allocation_list_entries_used;
      const bool needs_allocation_table = (allocs_used != 0);

      // Keep the DMA-private-data pointer/size used for this submission so we can
      // validate the KMD-filled AEROGPU_DMA_PRIV even if the runtime rotates
      // pointers in the callback out-params.
      void* submit_priv_ptr = dev->wddm_context.pDmaBufferPrivateData;
      const uint32_t submit_priv_size = dev->wddm_context.DmaBufferPrivateDataSize;

      HRESULT submit_hr = E_NOTIMPL;
      // Tracks the last attempted submission callback (and therefore the
      // callback that returned `submit_hr`).
      WddmSubmitCallbackKind submit_kind = WddmSubmitCallbackKind::None;
      const uint32_t cmd_len = static_cast<uint32_t>(cmd_bytes);
      // Win7 D3D9 runtimes expose several possible submission callbacks. Prefer
      // Render/Present so dxgkrnl routes through DxgkDdiRender/DxgkDdiPresent and
      // the KMD can stamp AEROGPU_DMA_PRIV + per-submit allocation-table metadata
      // before DxgkDdiSubmitCommand.
      if (is_present) {
        if constexpr (has_pfnPresentCb<WddmDeviceCallbacks>::value) {
          if (dev->wddm_callbacks.pfnPresentCb) {
            submission_fence = 0;
            submit_kind = WddmSubmitCallbackKind::PresentCb;
            submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnPresentCb, cmd_len, /*is_present=*/true,
                                               &submission_fence);
          }
        }

        if (!SUCCEEDED(submit_hr)) {
          if constexpr (has_pfnRenderCb<WddmDeviceCallbacks>::value) {
            if (dev->wddm_callbacks.pfnRenderCb) {
              using RenderCbT = decltype(dev->wddm_callbacks.pfnRenderCb);
              if constexpr (submit_callback_can_signal_present<RenderCbT>()) {
                // Some callback-table variants expose only RenderCb for both render
                // and present submissions (with an explicit Present flag in the
                // args). Prefer that path over SubmitCommandCb so the KMD can
                // attach a MetaHandle in DxgkDdiPresent.
                submission_fence = 0;
                submit_kind = WddmSubmitCallbackKind::RenderCb;
                submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnRenderCb, cmd_len, /*is_present=*/true,
                                                   &submission_fence);
              }
            }
          }
        }

        if (!SUCCEEDED(submit_hr)) {
          // Next preference: SubmitCommandCb. This can bypass DxgkDdiPresent, so
          // the KMD may not have stamped MetaHandle, but it can still build the
          // allocation-table metadata on-demand from the submit args.
          if constexpr (has_pfnSubmitCommandCb<WddmDeviceCallbacks>::value) {
            if (dev->wddm_callbacks.pfnSubmitCommandCb) {
              submission_fence = 0;
              submit_kind = WddmSubmitCallbackKind::SubmitCommandCb;
              submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnSubmitCommandCb, cmd_len, /*is_present=*/true,
                                                 &submission_fence);
            }
          }
        }

        // Last resort: RenderCb even if it cannot explicitly signal "present".
        // This may misclassify the submission, but is still preferable to
        // failing outright in callback-table variants that lack PresentCb and
        // SubmitCommandCb.
        if (!SUCCEEDED(submit_hr)) {
          if constexpr (has_pfnRenderCb<WddmDeviceCallbacks>::value) {
            if (dev->wddm_callbacks.pfnRenderCb) {
              using RenderCbT = decltype(dev->wddm_callbacks.pfnRenderCb);
              if constexpr (!submit_callback_can_signal_present<RenderCbT>()) {
                submission_fence = 0;
                submit_kind = WddmSubmitCallbackKind::RenderCb;
                submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnRenderCb, cmd_len, /*is_present=*/true,
                                                   &submission_fence);
              }
            }
          }
        }
      } else {
        if constexpr (has_pfnRenderCb<WddmDeviceCallbacks>::value) {
          if (dev->wddm_callbacks.pfnRenderCb) {
            submission_fence = 0;
            submit_kind = WddmSubmitCallbackKind::RenderCb;
            submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnRenderCb, cmd_len, /*is_present=*/false,
                                               &submission_fence);
          }
        }

        if (!SUCCEEDED(submit_hr)) {
          // Fallback: SubmitCommandCb (bypasses DxgkDdiRender). This is less
          // desirable than RenderCb, but still allows the KMD to build per-submit
          // allocation metadata on-demand.
          if constexpr (has_pfnSubmitCommandCb<WddmDeviceCallbacks>::value) {
            if (dev->wddm_callbacks.pfnSubmitCommandCb) {
              submission_fence = 0;
              submit_kind = WddmSubmitCallbackKind::SubmitCommandCb;
              submit_hr = invoke_submit_callback(dev, dev->wddm_callbacks.pfnSubmitCommandCb, cmd_len, /*is_present=*/false,
                                                 &submission_fence);
            }
          }
        }
      }

      if (SUCCEEDED(submit_hr)) {
        if (needs_allocation_table && submit_kind != WddmSubmitCallbackKind::SubmitCommandCb && submit_priv_ptr &&
            submit_priv_size >= AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES) {
          AEROGPU_DMA_PRIV priv{};
          std::memcpy(&priv, submit_priv_ptr, sizeof(priv));
          if (priv.MetaHandle == 0) {
            static std::atomic<uint32_t> g_missing_meta_logs{0};
            const uint32_t n = g_missing_meta_logs.fetch_add(1, std::memory_order_relaxed);
            if ((n < 8) || ((n & 1023u) == 0)) {
              logf("aerogpu-d3d9: submit missing MetaHandle (allocs=%u present=%u type=%u)\n",
                   static_cast<unsigned>(allocs_used),
                   is_present ? 1u : 0u,
                   static_cast<unsigned>(priv.Type));
            }
          }
        }
        submitted_to_kmd = true;
        did_submit = true;
        if (dev->wddm_context.buffers_need_deallocate) {
          // AllocateCb/DeallocateCb model: return the per-submit buffers after
          // the submission callback completes.
          wddm_deallocate_active_buffers(dev);
        } else {
          dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                         dev->wddm_context.AllocationListSize,
                                         adapter->max_allocation_list_slot_id);
        }
      } else {
        if (dev->wddm_context.buffers_need_deallocate) {
          // The runtime can still require DeallocateCb even if the submit call
          // fails (best-effort; prevents leaking callback-owned buffers).
          wddm_deallocate_active_buffers(dev);
        }
        mark_device_lost_from_submit(dev, submit_hr, is_present, submit_kind);
      }
    } else {
      logf("aerogpu-d3d9: submit command buffer too large (cmd=%llu cap=%u)\n",
           static_cast<unsigned long long>(cmd_bytes),
           static_cast<unsigned>(dev->wddm_context.CommandBufferSize));
    }
  }
#endif

  // If submission failed and the device was marked as lost, stop here. Do not
  // update fence state or stamp queries with fence==0.
  if (device_is_lost(dev)) {
    dev->cmd.rewind();
    dev->alloc_list_tracker.reset();
    dev->wddm_context.reset_submission_buffers();
    return dev->last_submission_fence;
  }

  uint64_t fence = 0;
  // Fence value associated with this specific submission (as returned by the
  // runtime callback, or (rarely) the KMD query fallback). Keep this separate
  // from adapter-wide tracking so concurrent submissions cannot cause us to
  // return a "too-new" fence.
  uint64_t per_submission_fence = 0;
  bool updated = false;
#if defined(_WIN32)
  if (submitted_to_kmd) {
    // Critical: capture the exact per-submission fence returned by the runtime
    // callback for *this* submission (SubmissionFenceId/NewFenceValue).
    fence = submission_fence;

    // Some WDK header vintages do not expose the callback fence outputs. In
    // that case, fall back to querying the KMD's fence counters via DxgkDdiEscape
    // (D3DKMTEscape) so we still return a real fence value and never "fake
    // complete" fences in-process.
    uint64_t kmd_submitted = 0;
    uint64_t kmd_completed = 0;
    bool kmd_ok = false;
    if (fence == 0 && adapter->kmd_query_available.load(std::memory_order_acquire)) {
      kmd_ok = adapter->kmd_query.QueryFence(&kmd_submitted, &kmd_completed);
      if (!kmd_ok) {
        adapter->kmd_query_available.store(false, std::memory_order_release);
      } else {
        fence = kmd_submitted;
      }
    }

    per_submission_fence = fence;

    if (kmd_ok) {
      std::lock_guard<std::mutex> lock(adapter->fence_mutex);
      const uint64_t prev_submitted = adapter->last_submitted_fence;
      const uint64_t prev_completed = adapter->completed_fence;
      adapter->last_submitted_fence = std::max(adapter->last_submitted_fence, kmd_submitted);
      adapter->completed_fence = std::max(adapter->completed_fence, kmd_completed);
      adapter->next_fence = std::max(adapter->next_fence, adapter->last_submitted_fence + 1);
      adapter->last_kmd_fence_query_ms = monotonic_ms();
      updated = (adapter->last_submitted_fence != prev_submitted) || (adapter->completed_fence != prev_completed);
    }

    if (per_submission_fence) {
      std::lock_guard<std::mutex> lock(adapter->fence_mutex);
      const uint64_t prev_submitted = adapter->last_submitted_fence;
      adapter->last_submitted_fence = std::max(adapter->last_submitted_fence, per_submission_fence);
      adapter->next_fence = std::max(adapter->next_fence, adapter->last_submitted_fence + 1);
      updated = updated || (adapter->last_submitted_fence != prev_submitted);
    }
  }
#endif

#if !(defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI)
  if (fence == 0) {
    {
      std::lock_guard<std::mutex> lock(adapter->fence_mutex);
      if (adapter->next_fence <= adapter->last_submitted_fence) {
        adapter->next_fence = adapter->last_submitted_fence + 1;
      }

      const uint64_t stub_fence = adapter->next_fence++;
      const uint64_t prev_submitted = adapter->last_submitted_fence;
      const uint64_t prev_completed = adapter->completed_fence;
      // Never allow the cached fence values to go backwards: they may be advanced
      // by the KMD query path (or, in a real WDDM build, by runtime-provided fence
      // callbacks).
      adapter->last_submitted_fence = std::max(adapter->last_submitted_fence, stub_fence);
      adapter->completed_fence = std::max(adapter->completed_fence, stub_fence);
      fence = stub_fence;
      updated = updated || (adapter->last_submitted_fence != prev_submitted) || (adapter->completed_fence != prev_completed);
    }
    did_submit = true;
  }
  per_submission_fence = fence;
#endif

  if (per_submission_fence == 0) {
    per_submission_fence = fence;
  }

  if (updated) {
    adapter->fence_cv.notify_all();
  }

  if (did_submit) {
    std::lock_guard<std::mutex> lock(adapter->fence_mutex);
    if (is_present) {
      adapter->present_submit_count++;
    } else {
      adapter->render_submit_count++;
    }
  }

  if (submit_log_enabled()) {
    logf("aerogpu-d3d9: submit cmd_bytes=%llu fence=%llu present=%u\n",
         static_cast<unsigned long long>(cmd_bytes),
         static_cast<unsigned long long>(per_submission_fence),
         is_present ? 1u : 0u);
  }

  dev->last_submission_fence = per_submission_fence;
  resolve_pending_event_queries(dev, per_submission_fence);
  dev->cmd.rewind();
  dev->alloc_list_tracker.reset();
  dev->wddm_context.reset_submission_buffers();
  return per_submission_fence;
}

HRESULT flush_locked(Device* dev) {
  // Flushing an empty command buffer should be a no-op. This matters for
  // D3DGETDATA_FLUSH polling loops (e.g. DWM EVENT queries): if we submit an
  // empty buffer every poll we can flood the KMD/emulator with redundant
  // submissions and increase CPU usage.
  if (!dev) {
    return S_OK;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }
  if (dev->cmd.empty()) {
    // If we have pending EVENT queries waiting for a submission fence, allow
    // this flush call to "resolve" them without forcing an empty DMA buffer to
    // the kernel. `submit()`'s empty-path stamps queries with
    // `last_submission_fence`.
    if (!dev->pending_event_queries.empty()) {
      (void)submit(dev);
      if (device_is_lost(dev)) {
        return device_lost_hresult(dev);
      }
    }
    return S_OK;
  }
  // If we cannot fit an explicit FLUSH marker into the remaining space, just
  // submit the current buffer; the submission boundary is already a flush point.
  const size_t flush_bytes = align_up(sizeof(aerogpu_cmd_flush), 4);
  if (dev->cmd.bytes_remaining() < flush_bytes) {
    submit(dev);
    return device_lost_override(dev, S_OK);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_flush>(dev, AEROGPU_CMD_FLUSH);
  if (cmd) {
    cmd->reserved0 = 0;
    cmd->reserved1 = 0;
  }
  submit(dev);
  return device_lost_override(dev, S_OK);
}

HRESULT copy_surface_bytes(Device* dev, const Resource* src, Resource* dst) {
  if (!dev || !src || !dst) {
    return E_INVALIDARG;
  }
  if (src->width != dst->width || src->height != dst->height) {
    return E_INVALIDARG;
  }
  if (src->format != dst->format) {
    return E_INVALIDARG;
  }

  const bool bc = is_block_compressed_format(src->format);
  uint32_t row_copy_bytes = 0;
  uint32_t rows = 0;
  if (bc) {
    // For BC formats the resource layout is in 4x4 blocks. `row_pitch` already
    // represents the bytes-per-row of blocks; copy whole rows.
    row_copy_bytes = src->row_pitch;
    rows = std::max(1u, (src->height + 3u) / 4u);
  } else {
    const uint32_t bpp = bytes_per_pixel(src->format);
    row_copy_bytes = src->width * bpp;
    rows = src->height;
  }
  if (src->row_pitch < row_copy_bytes || dst->row_pitch < row_copy_bytes) {
    return E_FAIL;
  }

  struct Map {
    void* ptr = nullptr;
    bool wddm_locked = false;
  };

  Map src_map{};
  Map dst_map{};
  const uint8_t* src_base = nullptr;
  uint8_t* dst_base = nullptr;

  const uint64_t bytes_needed = static_cast<uint64_t>(src->row_pitch) * rows;
  if (bytes_needed == 0 || bytes_needed > src->size_bytes || bytes_needed > dst->size_bytes) {
    return E_FAIL;
  }

  bool use_src_storage = src->storage.size() >= bytes_needed;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  // Guest-backed resources may still allocate a CPU shadow buffer (e.g. shared
  // resources opened via OpenResource). On real WDDM builds the authoritative
  // bytes live in the WDDM allocation, so prefer mapping it directly.
  if (src->backing_alloc_id != 0) {
    use_src_storage = false;
  }
#endif
  if (use_src_storage) {
    src_base = src->storage.data();
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (src->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              src->wddm_hAllocation,
                                              0,
                                              bytes_needed,
                                              kD3DLOCK_READONLY,
                                              &src_map.ptr,
                                              dev->wddm_context.hContext);
      if (FAILED(hr) || !src_map.ptr) {
        return FAILED(hr) ? hr : E_FAIL;
      }
      src_map.wddm_locked = true;
      src_base = static_cast<const uint8_t*>(src_map.ptr);
    } else
#endif
    {
      return E_FAIL;
    }
  }

  bool use_dst_storage = dst->storage.size() >= bytes_needed;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dst->backing_alloc_id != 0) {
    use_dst_storage = false;
  }
#endif
  if (use_dst_storage) {
    dst_base = dst->storage.data();
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (dst->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              dst->wddm_hAllocation,
                                              0,
                                              bytes_needed,
                                              &dst_map.ptr,
                                              dev->wddm_context.hContext);
      if (FAILED(hr) || !dst_map.ptr) {
        if (src_map.wddm_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                       dev->wddm_device,
                                       src->wddm_hAllocation,
                                       dev->wddm_context.hContext);
        }
        return FAILED(hr) ? hr : E_FAIL;
      }
      dst_map.wddm_locked = true;
      dst_base = static_cast<uint8_t*>(dst_map.ptr);
    } else
#endif
    {
      if (src_map.wddm_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                     dev->wddm_device,
                                     src->wddm_hAllocation,
                                     dev->wddm_context.hContext);
      }
      return E_FAIL;
    }
  }
  for (uint32_t y = 0; y < rows; y++) {
    std::memcpy(dst_base + static_cast<size_t>(y) * dst->row_pitch,
                src_base + static_cast<size_t>(y) * src->row_pitch,
                row_copy_bytes);
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dst_map.wddm_locked) {
    (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                 dev->wddm_device,
                                 dst->wddm_hAllocation,
                                 dev->wddm_context.hContext);
  }
  if (src_map.wddm_locked) {
    (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                 dev->wddm_device,
                                 src->wddm_hAllocation,
                                 dev->wddm_context.hContext);
  }
#endif
  return S_OK;
}

// -----------------------------------------------------------------------------
// Adapter DDIs
// -----------------------------------------------------------------------------

uint64_t luid_to_u64(const LUID& luid) {
  const uint64_t hi = static_cast<uint64_t>(static_cast<uint32_t>(luid.HighPart));
  const uint64_t lo = static_cast<uint64_t>(luid.LowPart);
  return (hi << 32) | lo;
}

LUID default_luid() {
  LUID luid{};
  luid.LowPart = 0;
  luid.HighPart = 0;
  return luid;
}

std::mutex g_adapter_cache_mutex;
std::unordered_map<uint64_t, Adapter*> g_adapter_cache;

Adapter* acquire_adapter(const LUID& luid,
                         UINT interface_version,
                         UINT umd_version,
                         D3DDDI_ADAPTERCALLBACKS* callbacks,
                         D3DDDI_ADAPTERCALLBACKS2* callbacks2) {
  std::lock_guard<std::mutex> lock(g_adapter_cache_mutex);

  const uint64_t key = luid_to_u64(luid);
  auto it = g_adapter_cache.find(key);
  if (it != g_adapter_cache.end()) {
    Adapter* adapter = it->second;
    adapter->open_count.fetch_add(1);
    adapter->interface_version = interface_version;
    adapter->umd_version = umd_version;
    adapter->adapter_callbacks = callbacks;
    adapter->adapter_callbacks2 = callbacks2;
    adapter->share_token_allocator.set_adapter_luid(luid);
    if (callbacks) {
      adapter->adapter_callbacks_copy = *callbacks;
      adapter->adapter_callbacks_valid = true;
    } else {
      adapter->adapter_callbacks_copy = {};
      adapter->adapter_callbacks_valid = false;
    }
    if (callbacks2) {
      adapter->adapter_callbacks2_copy = *callbacks2;
      adapter->adapter_callbacks2_valid = true;
    } else {
      adapter->adapter_callbacks2_copy = {};
      adapter->adapter_callbacks2_valid = false;
    }
    return adapter;
  }

  auto adapter = make_unique_nothrow<Adapter>();
  if (!adapter) {
    return nullptr;
  }
  adapter->luid = luid;
  adapter->share_token_allocator.set_adapter_luid(luid);
  adapter->open_count.store(1);
  adapter->interface_version = interface_version;
  adapter->umd_version = umd_version;
  adapter->adapter_callbacks = callbacks;
  adapter->adapter_callbacks2 = callbacks2;
  if (callbacks) {
    adapter->adapter_callbacks_copy = *callbacks;
    adapter->adapter_callbacks_valid = true;
  } else {
    adapter->adapter_callbacks_copy = {};
    adapter->adapter_callbacks_valid = false;
  }
  if (callbacks2) {
    adapter->adapter_callbacks2_copy = *callbacks2;
    adapter->adapter_callbacks2_valid = true;
  } else {
    adapter->adapter_callbacks2_copy = {};
    adapter->adapter_callbacks2_valid = false;
  }

#if defined(_WIN32)
  // Initialize a best-effort primary display mode so GetDisplayModeEx returns a
  // stable value even when the runtime opens the adapter via the LUID path (as
  // DWM commonly does).
  const int w = GetSystemMetrics(SM_CXSCREEN);
  const int h = GetSystemMetrics(SM_CYSCREEN);
  if (w > 0) {
    adapter->primary_width = static_cast<uint32_t>(w);
  }
  if (h > 0) {
    adapter->primary_height = static_cast<uint32_t>(h);
  }

  DEVMODEA dm{};
  dm.dmSize = sizeof(dm);
  if (EnumDisplaySettingsA(nullptr, ENUM_CURRENT_SETTINGS, &dm)) {
    if (dm.dmPelsWidth > 0) {
      adapter->primary_width = static_cast<uint32_t>(dm.dmPelsWidth);
    }
    if (dm.dmPelsHeight > 0) {
      adapter->primary_height = static_cast<uint32_t>(dm.dmPelsHeight);
    }
    if (dm.dmDisplayFrequency > 0) {
      adapter->primary_refresh_hz = static_cast<uint32_t>(dm.dmDisplayFrequency);
    }
  }
#endif

  try {
    g_adapter_cache.emplace(key, adapter.get());
  } catch (...) {
    return nullptr;
  }
  return adapter.release();
}

void release_adapter(Adapter* adapter) {
  if (!adapter) {
    return;
  }

  std::lock_guard<std::mutex> lock(g_adapter_cache_mutex);
  const uint32_t remaining = adapter->open_count.fetch_sub(1) - 1;
  if (remaining != 0) {
    return;
  }

  g_adapter_cache.erase(luid_to_u64(adapter->luid));

#if defined(_WIN32)
  // Release cross-process alloc_id token allocator state.
  {
    std::lock_guard<std::mutex> share_lock(adapter->share_token_mutex);
    if (adapter->share_token_view) {
      UnmapViewOfFile(adapter->share_token_view);
      adapter->share_token_view = nullptr;
    }
    if (adapter->share_token_mapping) {
      CloseHandle(adapter->share_token_mapping);
      adapter->share_token_mapping = nullptr;
    }
  }
#endif
  delete adapter;
}

HRESULT AEROGPU_D3D9_CALL adapter_close(D3DDDI_HADAPTER hAdapter) {
  D3d9TraceCall trace(D3d9TraceFunc::AdapterClose, d3d9_trace_arg_ptr(hAdapter.pDrvPrivate), 0, 0, 0);
  release_adapter(as_adapter(hAdapter));
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL adapter_get_caps(
    D3DDDI_HADAPTER hAdapter,
    const D3D9DDIARG_GETCAPS* pGetCaps) {
  D3d9TraceCall trace(D3d9TraceFunc::AdapterGetCaps,
                      d3d9_trace_arg_ptr(hAdapter.pDrvPrivate),
                      pGetCaps ? static_cast<uint64_t>(pGetCaps->Type) : 0,
                      pGetCaps ? static_cast<uint64_t>(pGetCaps->DataSize) : 0,
                      pGetCaps ? d3d9_trace_arg_ptr(pGetCaps->pData) : 0);
  auto* adapter = as_adapter(hAdapter);
  if (!adapter || !pGetCaps) {
    return trace.ret(E_INVALIDARG);
  }
  return trace.ret(aerogpu::get_caps(adapter, pGetCaps));
}

HRESULT AEROGPU_D3D9_CALL adapter_query_adapter_info(
    D3DDDI_HADAPTER hAdapter,
    const D3D9DDIARG_QUERYADAPTERINFO* pQueryAdapterInfo) {
  uint64_t data_ptr = 0;
  uint32_t size = 0;
  if (pQueryAdapterInfo) {
    data_ptr = d3d9_trace_arg_ptr(pQueryAdapterInfo->pPrivateDriverData);
    size = pQueryAdapterInfo->PrivateDriverDataSize;
  }

  D3d9TraceCall trace(D3d9TraceFunc::AdapterQueryAdapterInfo,
                      d3d9_trace_arg_ptr(hAdapter.pDrvPrivate),
                      pQueryAdapterInfo ? static_cast<uint64_t>(pQueryAdapterInfo->Type) : 0,
                      static_cast<uint64_t>(size),
                      data_ptr);

  auto* adapter = as_adapter(hAdapter);
  if (!adapter || !pQueryAdapterInfo) {
    return trace.ret(E_INVALIDARG);
  }
  void* data = pQueryAdapterInfo->pPrivateDriverData;
  size = pQueryAdapterInfo->PrivateDriverDataSize;

  return trace.ret(aerogpu::query_adapter_info(adapter, pQueryAdapterInfo));
}

HRESULT AEROGPU_D3D9_CALL adapter_create_device(
    D3D9DDIARG_CREATEDEVICE* pCreateDevice,
    D3D9DDI_DEVICEFUNCS* pDeviceFuncs);

// -----------------------------------------------------------------------------
// Device DDIs
// -----------------------------------------------------------------------------

HRESULT AEROGPU_D3D9_CALL device_destroy(D3DDDI_HDEVICE hDevice) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroy, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), 0, 0, 0);
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(S_OK);
  }

  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    if (dev->recording_state_block) {
      delete dev->recording_state_block;
      dev->recording_state_block = nullptr;
    }
    // Ensure we are not holding on to a DMA buffer that references allocations we
    // are about to destroy (e.g. swapchain backbuffers created but never
    // submitted). This matches the per-resource destroy path, but we do it once
    // for the whole device teardown.
    (void)submit(dev);

    // Tear down internal objects that the runtime does not know about.
    for (size_t i = 0; i < static_cast<size_t>(FixedFuncVariant::COUNT); ++i) {
      auto& pipe = dev->fixedfunc_pipelines[i];
      if (pipe.vertex_decl) {
        (void)emit_destroy_input_layout_locked(dev, pipe.vertex_decl->handle);
        delete pipe.vertex_decl;
        pipe.vertex_decl = nullptr;
      }
      if (pipe.vs) {
        (void)emit_destroy_shader_locked(dev, pipe.vs->handle);
        delete pipe.vs;
        pipe.vs = nullptr;
      }
      if (pipe.vs_lit) {
        (void)emit_destroy_shader_locked(dev, pipe.vs_lit->handle);
        delete pipe.vs_lit;
        pipe.vs_lit = nullptr;
      }
      if (pipe.vs_fog) {
        (void)emit_destroy_shader_locked(dev, pipe.vs_fog->handle);
        delete pipe.vs_fog;
        pipe.vs_fog = nullptr;
      }
      if (pipe.vs_lit_fog) {
        (void)emit_destroy_shader_locked(dev, pipe.vs_lit_fog->handle);
        delete pipe.vs_lit_fog;
        pipe.vs_lit_fog = nullptr;
      }
      // Pixel shaders are cached in `fixedfunc_ps_variants` and may be
      // shared across multiple fixed-function variants. Individual pipeline
      // slots only store a pointer to the currently-selected variant and do not
      // own it.
      pipe.ps = nullptr;
    }
    // Additional internal FVF-derived declarations for the programmable pipeline.
    for (auto& it : dev->fvf_vertex_decl_cache) {
      VertexDecl* decl = it.second;
      if (!decl) {
        continue;
      }
      (void)emit_destroy_input_layout_locked(dev, decl->handle);
      delete decl;
    }
    dev->fvf_vertex_decl_cache.clear();
    // Fixed-function PS cache entries can alias the same Shader* (e.g. if two
    // different keys map to the same fallback bytecode). Deduplicate deletes so
    // device teardown can't double-free.
    Shader* destroyed_fixedfunc_ps[sizeof(dev->fixedfunc_ps_variants) / sizeof(dev->fixedfunc_ps_variants[0])] = {};
    size_t destroyed_fixedfunc_ps_len = 0;
    for (Shader*& ps : dev->fixedfunc_ps_variants) {
      if (!ps) {
        continue;
      }
      bool already_destroyed = false;
      for (size_t i = 0; i < destroyed_fixedfunc_ps_len; ++i) {
        if (destroyed_fixedfunc_ps[i] == ps) {
          already_destroyed = true;
          break;
        }
      }
      if (!already_destroyed) {
        destroyed_fixedfunc_ps[destroyed_fixedfunc_ps_len++] = ps;
        (void)emit_destroy_shader_locked(dev, ps->handle);
        delete ps;
      }
      ps = nullptr;
    }
    dev->fixedfunc_ps_variant_cache.clear();
    dev->fixedfunc_ps_interop = nullptr;
    if (dev->up_vertex_buffer) {
      (void)emit_destroy_resource_locked(dev, dev->up_vertex_buffer->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dev->up_vertex_buffer->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      dev->up_vertex_buffer->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        dev->up_vertex_buffer->wddm_hAllocation = 0;
      }
#endif
      delete dev->up_vertex_buffer;
      dev->up_vertex_buffer = nullptr;
    }
    for (uint32_t s = 0; s < 16; ++s) {
      Resource* vb = dev->instancing_vertex_buffers[s];
      if (!vb) {
        continue;
      }
      (void)emit_destroy_resource_locked(dev, vb->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (vb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      vb->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        vb->wddm_hAllocation = 0;
      }
#endif
      delete vb;
      dev->instancing_vertex_buffers[s] = nullptr;
    }
    if (dev->up_index_buffer) {
      (void)emit_destroy_resource_locked(dev, dev->up_index_buffer->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (dev->up_index_buffer->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      dev->up_index_buffer->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        dev->up_index_buffer->wddm_hAllocation = 0;
      }
#endif
      delete dev->up_index_buffer;
      dev->up_index_buffer = nullptr;
    }
    destroy_blit_objects_locked(dev);
    for (SwapChain* sc : dev->swapchains) {
      if (!sc) {
        continue;
      }
      for (Resource* bb : sc->backbuffers) {
        if (!bb) {
          continue;
        }
        (void)emit_destroy_resource_locked(dev, bb->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (bb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
          (void)wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, bb->wddm_hAllocation, dev->wddm_context.hContext);
          bb->wddm_hAllocation = 0;
        }
#endif
        delete bb;
      }
      delete sc;
    }
    dev->swapchains.clear();
    dev->current_swapchain = nullptr;
    flush_locked(dev);
  }

#if defined(_WIN32)
  // Ensure we return any AllocateCb-owned per-submit buffers before destroying
  // the context/device. Some runtimes allocate these even if we never end up
  // submitting (e.g. device teardown during initialization failures).
  if (dev->wddm_context.buffers_need_deallocate) {
    wddm_deallocate_active_buffers(dev);
  }
  dev->wddm_context.destroy(dev->wddm_callbacks);
  wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
  dev->wddm_device = 0;
#endif
  // The explicit teardown above already destroyed all internal objects; make the
  // `Device` destructor a no-op to avoid double-free.
  dev->adapter = nullptr;
  delete dev;
  return trace.ret(S_OK);
}

static void consume_wddm_alloc_priv(Resource* res,
                                   const void* priv_data,
                                   uint32_t priv_data_size,
                                   bool is_shared_resource) {
  if (!res || !priv_data || priv_data_size < sizeof(aerogpu_wddm_alloc_priv)) {
    return;
  }

  aerogpu_wddm_alloc_priv priv{};
  std::memcpy(&priv, priv_data, sizeof(priv));

  if (priv.magic != AEROGPU_WDDM_ALLOC_PRIV_MAGIC ||
      (priv.version != AEROGPU_WDDM_ALLOC_PRIV_VERSION && priv.version != AEROGPU_WDDM_ALLOC_PRIV_VERSION_2)) {
    return;
  }

  res->backing_alloc_id = priv.alloc_id;
  res->share_token = priv.share_token;
  if (res->size_bytes == 0 && priv.size_bytes != 0 && priv.size_bytes <= 0xFFFFFFFFull) {
    res->size_bytes = static_cast<uint32_t>(priv.size_bytes);
  }
  if (priv.flags & AEROGPU_WDDM_ALLOC_PRIV_FLAG_IS_SHARED) {
    res->is_shared = true;
  }
  (void)is_shared_resource;
}

static aerogpu_wddm_u64 encode_wddm_alloc_priv_desc(uint32_t format, uint32_t width, uint32_t height) {
  if (format == 0 || width == 0 || height == 0) {
    return 0;
  }
  width = std::min<uint32_t>(width, static_cast<uint32_t>(AEROGPU_WDDM_ALLOC_PRIV_DESC_MAX_WIDTH));
  height = std::min<uint32_t>(height, static_cast<uint32_t>(AEROGPU_WDDM_ALLOC_PRIV_DESC_MAX_HEIGHT));
  if (width == 0 || height == 0) {
    return 0;
  }
  return AEROGPU_WDDM_ALLOC_PRIV_DESC_PACK(format, width, height);
}

static bool decode_wddm_alloc_priv_desc(aerogpu_wddm_u64 desc, uint32_t* format_out, uint32_t* width_out, uint32_t* height_out) {
  if (!format_out || !width_out || !height_out) {
    return false;
  }
  if (!AEROGPU_WDDM_ALLOC_PRIV_DESC_PRESENT(desc)) {
    return false;
  }
  const uint32_t format = static_cast<uint32_t>(AEROGPU_WDDM_ALLOC_PRIV_DESC_FORMAT(desc));
  const uint32_t width = static_cast<uint32_t>(AEROGPU_WDDM_ALLOC_PRIV_DESC_WIDTH(desc));
  const uint32_t height = static_cast<uint32_t>(AEROGPU_WDDM_ALLOC_PRIV_DESC_HEIGHT(desc));
  if (format == 0 || width == 0 || height == 0) {
    return false;
  }
  *format_out = format;
  *width_out = width;
  *height_out = height;
  return true;
}

HRESULT create_backbuffer_locked(Device* dev, Resource* res, uint32_t format, uint32_t width, uint32_t height) {
  if (!dev || !dev->adapter || !res) {
    return E_INVALIDARG;
  }
  if (!is_supported_backbuffer_format(static_cast<D3DDDIFORMAT>(format))) {
    return E_INVALIDARG;
  }

  const uint32_t bpp = bytes_per_pixel(format);
  width = std::max(1u, width);
  height = std::max(1u, height);

  res->handle = allocate_global_handle(dev->adapter);
  res->kind = ResourceKind::Surface;
  res->type = 0;
  res->format = format;
  res->width = width;
  res->height = height;
  res->depth = 1;
  res->mip_levels = 1;
  res->usage = kD3DUsageRenderTarget;
  res->pool = kD3DPOOL_DEFAULT;
  res->backing_alloc_id = 0;
  res->backing_offset_bytes = 0;
  res->share_token = 0;
  res->is_shared = false;
  res->is_shared_alias = false;
  res->wddm_hAllocation = 0;
  res->row_pitch = width * bpp;
  res->slice_pitch = res->row_pitch * height;
  res->locked = false;
  res->locked_offset = 0;
  res->locked_size = 0;
  res->locked_flags = 0;
  res->locked_ptr = nullptr;

  uint64_t total = static_cast<uint64_t>(res->slice_pitch);
  if (total > 0x7FFFFFFFu) {
    return E_OUTOFMEMORY;
  }
  res->size_bytes = static_cast<uint32_t>(total);

  bool has_wddm_allocation = false;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dev->wddm_device != 0) {
    const uint32_t alloc_id = allocate_umd_alloc_id(dev->adapter);
    if (alloc_id == 0) {
      return E_OUTOFMEMORY;
    }
    res->backing_alloc_id = alloc_id;

    aerogpu_wddm_alloc_priv priv{};
    priv.magic = AEROGPU_WDDM_ALLOC_PRIV_MAGIC;
    priv.version = AEROGPU_WDDM_ALLOC_PRIV_VERSION;
    priv.alloc_id = alloc_id;
    priv.flags = AEROGPU_WDDM_ALLOC_PRIV_FLAG_NONE;
    priv.share_token = 0;
    priv.size_bytes = static_cast<aerogpu_wddm_u64>(res->size_bytes);
    priv.reserved0 = encode_wddm_alloc_priv_desc(res->format, res->width, res->height);

    const HRESULT hr = wddm_create_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              res->size_bytes,
                                              &priv,
                                              sizeof(priv),
                                              &res->wddm_hAllocation,
                                              dev->wddm_context.hContext);
    if (FAILED(hr) || res->wddm_hAllocation == 0) {
      return FAILED(hr) ? hr : E_FAIL;
    }

    has_wddm_allocation = true;
  }
#endif

  if (!has_wddm_allocation) {
    // Fallback (non-WDDM builds): allocate CPU shadow storage and treat the host
    // object as "host allocated" (backing_alloc_id remains 0).
    try {
      res->storage.resize(res->size_bytes);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
    res->wddm_hAllocation = 0;
    res->backing_alloc_id = 0;
  }

  if (!emit_create_resource_locked(dev, res)) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      (void)wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
      res->wddm_hAllocation = 0;
    }
#endif
    return E_OUTOFMEMORY;
  }
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_create_resource(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_CREATERESOURCE* pCreateResource) {
  const uint64_t type_format =
      pCreateResource
          ? d3d9_trace_pack_u32_u32(d3d9_resource_type(*pCreateResource), d3d9_resource_format(*pCreateResource))
          : 0;
  const uint64_t wh =
      pCreateResource
          ? d3d9_trace_pack_u32_u32(d3d9_resource_width(*pCreateResource), d3d9_resource_height(*pCreateResource))
          : 0;
  const uint64_t usage_pool =
      pCreateResource ? d3d9_trace_pack_u32_u32(d3d9_resource_usage(*pCreateResource), d3d9_resource_pool(*pCreateResource)) : 0;
  D3d9TraceCall trace(
      D3d9TraceFunc::DeviceCreateResource, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), type_format, wh, usage_pool);
  if (!hDevice.pDrvPrivate || !pCreateResource) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  const bool wants_shared = (pCreateResource->pSharedHandle != nullptr);
  const bool open_existing_shared = wants_shared && (*pCreateResource->pSharedHandle != nullptr);
  const uint32_t requested_mip_levels = d3d9_resource_mip_levels(*pCreateResource);

  const uint32_t create_type_u32 = d3d9_resource_type(*pCreateResource);
  const uint32_t create_size_bytes = d3d9_resource_size(*pCreateResource);
  const uint32_t create_width = d3d9_resource_width(*pCreateResource);
  const uint32_t create_height = d3d9_resource_height(*pCreateResource);
  uint32_t create_depth = std::max(1u, d3d9_resource_depth(*pCreateResource));
  if (create_size_bytes == 0 && create_type_u32 == kD3dRTypeCubeTexture) {
    // D3DRTYPE_CUBETEXTURE always has 6 faces. Some runtime/header combinations
    // do not provide a meaningful Depth field for cube resources, so normalize
    // it here to the array-layer count expected by the AeroGPU host executor.
    create_depth = 6;
  }

  uint32_t mip_levels = std::max(1u, requested_mip_levels);
  if (!wants_shared &&
      requested_mip_levels == 0 &&
      create_size_bytes == 0 &&
      create_width != 0 &&
      create_height != 0) {
    // D3D9 CreateTexture semantics: MipLevels=0 means "create the full mip chain".
    mip_levels = calc_full_mip_chain_levels_2d(create_width, create_height);
  }
  if (wants_shared && (requested_mip_levels != 1 || create_depth != 1)) {
    // MVP: shared surfaces must be single-allocation (no mip chains/arrays).
    return trace.ret(D3DERR_INVALIDCALL);
  }

  // The AeroGPU host executor currently interprets `array_layers==6` as a cube
  // texture (via a cube view). Cube textures must be square; reject invalid
  // descriptors early so we don't emit a CREATE_TEXTURE2D packet that the host
  // will later reject during command execution.
  if (create_size_bytes == 0 &&
      create_depth == 6 &&
      create_width != 0 &&
      create_height != 0 &&
      create_width != create_height) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  // AeroGPU only supports 2D textures/surfaces (CREATE_TEXTURE2D) plus buffers.
  //
  // Compatibility: some D3D9 runtimes/headers appear to use Type=0 for surface
  // resources. Accept this encoding (treat as a non-array surface) rather than
  // rejecting it as an unknown type.
  if (create_size_bytes == 0 &&
      !d3d9_validate_nonbuffer_type(create_type_u32,
                                    create_depth,
                                    D3d9NonBufferTypeValidation::AllowUnknownAsSurface)) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  auto res = make_unique_nothrow<Resource>();
  if (!res) {
    return trace.ret(E_OUTOFMEMORY);
  }
  res->handle = allocate_global_handle(dev->adapter);
  res->type = create_type_u32;
  res->format = d3d9_resource_format(*pCreateResource);
  res->width = create_width;
  res->height = create_height;
  res->depth = create_depth;
  res->mip_levels = mip_levels;
  res->usage = d3d9_resource_usage(*pCreateResource);
  res->pool = d3d9_resource_pool(*pCreateResource);
  res->wddm_hAllocation = get_wddm_allocation_from_create_resource(pCreateResource);
  res->is_shared = wants_shared;
  res->is_shared_alias = open_existing_shared;

  // Keep format support conservative and consistent with GetCaps() format ops.
  // In particular:
  // - Do not accept render-target usage for formats that are only supported as
  //   textures (e.g. BC formats).
  // - Only accept depth/stencil usage for formats the host executor supports.
  if ((res->usage & kD3DUsageRenderTarget) && !is_supported_backbuffer_format(res->format)) {
    return trace.ret(D3DERR_INVALIDCALL);
  }
  if ((res->usage & kD3DUsageDepthStencil) && !is_supported_depth_stencil_format(res->format)) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  /*
   * Only treat KMD allocation private data as an INPUT when opening an existing
   * shared resource.
   *
   * For normal resource creation, `pPrivateDriverData` is an output buffer
   * owned by the runtime; consuming it before we populate it risks picking up
   * stale bytes from a previous call (e.g. reusing an old alloc_id/share_token),
   * which can lead to cross-process collisions and host-side shared-surface
   * table corruption.
   */
  if (open_existing_shared) {
    consume_wddm_alloc_priv(res.get(),
                            pCreateResource->pPrivateDriverData,
                            pCreateResource->PrivateDriverDataSize,
                             /*is_shared_resource=*/true);
  }

  // Heuristic: if size is provided, treat as buffer; otherwise treat as a 2D image.
  if (create_size_bytes) {
    res->kind = ResourceKind::Buffer;
    const uint64_t requested = static_cast<uint64_t>(create_size_bytes);
    const uint64_t aligned = (requested + 3ull) & ~3ull;
    if (aligned == 0 || aligned > 0x7FFFFFFFu) {
      return trace.ret(E_OUTOFMEMORY);
    }
    res->size_bytes = static_cast<uint32_t>(aligned);
    res->row_pitch = 0;
    res->slice_pitch = 0;
  } else if (res->width && res->height) {
    // Surface/Texture2D share the same storage layout for now.
    res->kind = (res->mip_levels > 1 || res->depth > 1) ? ResourceKind::Texture2D : ResourceKind::Surface;

    Texture2dLayout layout{};
    if (!calc_texture2d_layout(res->format, res->width, res->height, res->mip_levels, res->depth, &layout)) {
      return trace.ret(E_OUTOFMEMORY);
    }
    if (layout.total_size_bytes > 0x7FFFFFFFu) {
      return trace.ret(E_OUTOFMEMORY);
    }

    res->row_pitch = layout.row_pitch_bytes;
    res->slice_pitch = layout.slice_pitch_bytes;
    res->size_bytes = static_cast<uint32_t>(layout.total_size_bytes);
  } else {
    return trace.ret(E_INVALIDARG);
  }

  if (res->pool != kD3DPOOL_SYSTEMMEM && res->kind != ResourceKind::Buffer) {
    const uint32_t fmt = static_cast<uint32_t>(res->format);
    const bool wants_rt = (res->usage & kD3DUsageRenderTarget) != 0;
    const bool wants_ds = (res->usage & kD3DUsageDepthStencil) != 0;
    if (wants_rt && wants_ds) {
      return trace.ret(D3DERR_INVALIDCALL);
    }

    // Enforce deterministic format/usage combinations.
    //
    // The AeroGPU D3D9 UMD exposes render-target support for the common 32-bit and
    // packed 16-bit RGB formats.
    if (wants_rt) {
      switch (fmt) {
        // D3DFMT_A8R8G8B8 / D3DFMT_X8R8G8B8 / D3DFMT_A8B8G8R8
        case 21u:
        case 22u:
        // D3DFMT_R5G6B5 / D3DFMT_X1R5G5B5 / D3DFMT_A1R5G5B5
        case 23u:
        case 24u:
        case 25u:
        case 32u:
          break;
        default:
          return trace.ret(D3DERR_INVALIDCALL);
      }
    }
    if (wants_ds) {
      // D3DFMT_D24S8
      if (fmt != 75u) {
        return trace.ret(D3DERR_INVALIDCALL);
      }
    } else {
      // Prevent depth formats from being created without depth-stencil usage.
      if (fmt == 75u) {
        return trace.ret(D3DERR_INVALIDCALL);
      }
    }

    const uint32_t agpu_format = d3d9_format_to_aerogpu(res->format);
    if (agpu_format == AEROGPU_FORMAT_INVALID) {
      return trace.ret(D3DERR_INVALIDCALL);
    }

    // BC formats were introduced in the guesthost ABI in minor version 2.
    // Older emulators will treat these as invalid; gate them so the UMD can run
    // against older hosts.
    if (is_block_compressed_format(res->format) && !SupportsBcFormats(dev)) {
      return trace.ret(D3DERR_INVALIDCALL);
    }
  }

  // System-memory pool resources (e.g. CreateOffscreenPlainSurface with
  // D3DPOOL_SYSTEMMEM) are used by the D3D9 runtime for readback
  // (GetRenderTargetData). In WDDM builds we back these with a guest allocation
  // so the host can write pixels directly into guest memory
  // (AEROGPU_COPY_FLAG_WRITEBACK_DST) and the CPU can lock the allocation to
  // read them.
  if (res->pool == kD3DPOOL_SYSTEMMEM) {
    if (wants_shared) {
      return trace.ret(D3DERR_INVALIDCALL);
    }
    // In non-WDDM/portable builds there is no allocation-table plumbing, so keep
    // systemmem resources CPU-only (no host object).
    //
    // NOTE: Some portable tests set `wddm_context.hContext` to a non-zero value to
    // exercise allocation-list tracking logic without a real WDDM runtime. Only
    // the WDK build provides allocation lock callbacks, so keep systemmem resources
    // CPU-only unless we're built for WDDM and have a real WDDM device.
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    const bool allow_wddm_systemmem = (dev->wddm_device != 0);
#else
    const bool allow_wddm_systemmem = false;
#endif
    if (!allow_wddm_systemmem) {
      try {
        res->storage.resize(res->size_bytes);
      } catch (...) {
        return trace.ret(E_OUTOFMEMORY);
      }
      res->handle = 0;
      res->backing_alloc_id = 0;
      res->backing_offset_bytes = 0;
      res->share_token = 0;
      res->wddm_hAllocation = 0;
      pCreateResource->hResource.pDrvPrivate = res.release();
      return trace.ret(S_OK);
    }

    // WDDM path: back the systemmem surface with a guest allocation so the host
    // can write pixels back into guest memory (WRITEBACK_DST) and the CPU can
    // lock/map the allocation to read them.
    const bool have_runtime_priv =
        (pCreateResource->pPrivateDriverData &&
         pCreateResource->PrivateDriverDataSize >= sizeof(aerogpu_wddm_alloc_priv));
    if (res->wddm_hAllocation != 0 && !have_runtime_priv) {
      // If the runtime already attached a kernel allocation handle, we need a
      // private-driver-data buffer to communicate the alloc_id to the KMD.
      logf("aerogpu-d3d9: Create systemmem resource missing private data buffer for existing hAllocation (have=%u need=%u)\n",
           pCreateResource->PrivateDriverDataSize,
           static_cast<unsigned>(sizeof(aerogpu_wddm_alloc_priv)));
      return trace.ret(D3DERR_INVALIDCALL);
    }

    // WRITEBACK_DST requires the destination to have a host resource.
    if (d3d9_format_to_aerogpu(res->format) == AEROGPU_FORMAT_INVALID) {
      return trace.ret(D3DERR_INVALIDCALL);
    }
    if (is_block_compressed_format(res->format) && !SupportsBcFormats(dev)) {
      return trace.ret(D3DERR_INVALIDCALL);
    }

    const uint32_t alloc_id = allocate_umd_alloc_id(dev->adapter);
    if (!alloc_id) {
      logf("aerogpu-d3d9: Failed to allocate systemmem alloc_id (handle=%u)\n",
           static_cast<unsigned>(res->handle));
      return trace.ret(E_FAIL);
    }

    aerogpu_wddm_alloc_priv priv{};
    priv.magic = AEROGPU_WDDM_ALLOC_PRIV_MAGIC;
    priv.version = AEROGPU_WDDM_ALLOC_PRIV_VERSION;
    priv.alloc_id = alloc_id;
    priv.flags = AEROGPU_WDDM_ALLOC_PRIV_FLAG_NONE;
    priv.share_token = 0;
    priv.size_bytes = static_cast<aerogpu_wddm_u64>(res->size_bytes);
    priv.reserved0 = encode_wddm_alloc_priv_desc(res->format, res->width, res->height);
    if (have_runtime_priv) {
      std::memcpy(pCreateResource->pPrivateDriverData, &priv, sizeof(priv));
    }

    res->backing_alloc_id = alloc_id;
    res->backing_offset_bytes = 0;
    res->share_token = 0;
    res->is_shared = false;
    res->is_shared_alias = false;

    bool allocation_created = false;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    // Some D3D9 runtimes do not attach a WDDM allocation handle to systemmem pool
    // resources. For AeroGPU we still want a real guest-backed allocation so the
    // host can write pixels directly into guest memory (WRITEBACK_DST) and the
    // CPU can map it via LockRect. Create a system-memory segment allocation if
    // the runtime did not supply one.
    if (res->wddm_hAllocation == 0 && dev->wddm_device != 0) {
      const HRESULT hr = wddm_create_allocation(dev->wddm_callbacks,
                                                dev->wddm_device,
                                                res->size_bytes,
                                                &priv,
                                                sizeof(priv),
                                                &res->wddm_hAllocation,
                                                dev->wddm_context.hContext);
      if (FAILED(hr) || res->wddm_hAllocation == 0) {
        logf("aerogpu-d3d9: AllocateCb failed for systemmem resource hr=0x%08lx handle=%u alloc_id=%u\n",
             static_cast<unsigned long>(hr),
             static_cast<unsigned>(res->handle),
             static_cast<unsigned>(res->backing_alloc_id));
        return trace.ret(FAILED(hr) ? hr : E_FAIL);
      }
      allocation_created = true;
    }
#endif

    if (res->wddm_hAllocation == 0) {
      // Without a WDDM allocation handle we cannot participate in the alloc-table
      // protocol, so WRITEBACK_DST readback is not supported.
      logf("aerogpu-d3d9: systemmem resource missing WDDM hAllocation (handle=%u alloc_id=%u)\n",
           static_cast<unsigned>(res->handle),
           static_cast<unsigned>(res->backing_alloc_id));
      return trace.ret(E_FAIL);
    }

    // Ensure CPU copies/locks map the allocation rather than reading stale
    // `storage` bytes.
    res->storage.clear();

    if (!emit_create_resource_locked(dev, res.get())) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (allocation_created && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      res->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        res->wddm_hAllocation = 0;
      }
#endif
      return trace.ret(E_OUTOFMEMORY);
    }
    pCreateResource->hResource.pDrvPrivate = res.release();
    return trace.ret(S_OK);
  }

  bool force_host_backing = false;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  // Guest-backed textures currently only support a single subresource
  // (mip 0 / array layer 0). Many real D3D9 apps create mipmapped textures in the
  // default pool; rather than failing creation, fall back to host-backed storage
  // so the host can allocate the full texture and the UMD can update it via
  // UPLOAD_RESOURCE.
  force_host_backing =
      !wants_shared &&
      res->kind != ResourceKind::Buffer &&
      (res->mip_levels > 1 || res->depth > 1);
  if (force_host_backing) {
    // Ensure the allocation private-driver-data blob is not interpreted by the
    // KMD as an AeroGPU alloc-id contract. The D3D9 runtime owns this buffer and
    // may reuse it across calls without clearing it; leaving stale bytes here
    // can cause the KMD to reject the allocation (e.g. shared flag mismatch) or
    // to reuse a stale alloc_id. For host-backed resources we intentionally do
    // not participate in the alloc-table protocol, so clear the blob.
    if (pCreateResource->pPrivateDriverData && pCreateResource->PrivateDriverDataSize) {
      std::memset(pCreateResource->pPrivateDriverData, 0, pCreateResource->PrivateDriverDataSize);
    }
    res->backing_alloc_id = 0;
    res->backing_offset_bytes = 0;
    res->share_token = 0;
    try {
      res->storage.resize(res->size_bytes);
    } catch (...) {
      return trace.ret(E_OUTOFMEMORY);
    }
  }
#endif

  // On the real WDDM path we want GPU resources to be backed by WDDM allocations
  // and referenced in the command stream via a stable per-allocation `alloc_id`
  // (carried in aerogpu_wddm_alloc_priv and resolved via the per-submit allocation
  // table).
  if (!wants_shared && !force_host_backing && dev->wddm_context.hContext != 0) {
    if (!res->backing_alloc_id) {
      const bool have_runtime_priv =
          (pCreateResource->pPrivateDriverData &&
           pCreateResource->PrivateDriverDataSize >= sizeof(aerogpu_wddm_alloc_priv));
      if (res->wddm_hAllocation != 0 && !have_runtime_priv) {
        // If the runtime already attached an allocation handle, we have no other
        // way to communicate the alloc_id into the KMD allocation record.
        logf("aerogpu-d3d9: CreateResource missing private data buffer for existing hAllocation (have=%u need=%u)\n",
             pCreateResource->PrivateDriverDataSize,
             static_cast<unsigned>(sizeof(aerogpu_wddm_alloc_priv)));
        return trace.ret(D3DERR_INVALIDCALL);
      }

      // Use the same cross-process allocator as shared surfaces so alloc_id values
      // never collide within a submission (DWM can reference shared + non-shared
      // allocations together).
      uint64_t alloc_token = 0;
      uint32_t alloc_id = 0;
      do {
        alloc_token = allocate_shared_alloc_id_token(dev->adapter);
        alloc_id = static_cast<uint32_t>(alloc_token & AEROGPU_WDDM_ALLOC_ID_UMD_MAX);
      } while (alloc_token != 0 && alloc_id == 0);

      if (!alloc_token || !alloc_id) {
        logf("aerogpu-d3d9: Failed to allocate alloc_id for non-shared resource (token=%llu alloc_id=%u)\n",
             static_cast<unsigned long long>(alloc_token),
             static_cast<unsigned>(alloc_id));
        return E_FAIL;
      }

      aerogpu_wddm_alloc_priv priv{};
      priv.magic = AEROGPU_WDDM_ALLOC_PRIV_MAGIC;
      priv.version = AEROGPU_WDDM_ALLOC_PRIV_VERSION;
      priv.alloc_id = alloc_id;
      priv.flags = AEROGPU_WDDM_ALLOC_PRIV_FLAG_NONE;
      priv.share_token = 0;
      priv.size_bytes = static_cast<aerogpu_wddm_u64>(res->size_bytes);
      priv.reserved0 = encode_wddm_alloc_priv_desc(res->format, res->width, res->height);
      if (have_runtime_priv) {
        std::memcpy(pCreateResource->pPrivateDriverData, &priv, sizeof(priv));
      }

      res->backing_alloc_id = alloc_id;
      res->backing_offset_bytes = 0;
      res->share_token = 0;
    }
  }

  if (wants_shared && !open_existing_shared) {
    if (!pCreateResource->pPrivateDriverData ||
        pCreateResource->PrivateDriverDataSize < sizeof(aerogpu_wddm_alloc_priv)) {
      logf("aerogpu-d3d9: Create shared resource missing private data buffer (have=%u need=%u)\n",
           pCreateResource->PrivateDriverDataSize,
           static_cast<unsigned>(sizeof(aerogpu_wddm_alloc_priv)));
      return trace.ret(D3DERR_INVALIDCALL);
    }

    uint64_t share_token = 0;
#if !(defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI)
    share_token = dev->adapter->share_token_allocator.allocate_share_token();
#endif

    // Allocate a stable cross-process alloc_id (31-bit) and persist it in
    // allocation private data so it survives OpenResource/OpenAllocation in
    // another process.
    //
    // The Win7 KMD fills `aerogpu_wddm_alloc_priv.share_token` during
    // DxgkDdiCreateAllocation. For shared allocations, dxgkrnl preserves and
    // replays the private-data blob on cross-process opens so other guest
    // processes observe the same token.
    //
    // NOTE: DWM may compose many shared surfaces from *different* processes in a
    // single submission. alloc_id values must therefore avoid collisions across
    // guest processes (not just within one process).
    uint32_t alloc_id = 0;
    {
      // `allocate_shared_alloc_id_token()` provides a monotonic 64-bit counter shared
      // across guest processes (best effort). Derive a 31-bit alloc_id from it.
      uint64_t alloc_token = 0;
      do {
        alloc_token = allocate_shared_alloc_id_token(dev->adapter);
        alloc_id = static_cast<uint32_t>(alloc_token & AEROGPU_WDDM_ALLOC_ID_UMD_MAX);
      } while (alloc_token != 0 && alloc_id == 0);

      if (!alloc_token || !alloc_id) {
        logf("aerogpu-d3d9: Failed to allocate shared alloc_id (token=%llu alloc_id=%u)\n",
             static_cast<unsigned long long>(alloc_token),
             static_cast<unsigned>(alloc_id));
        return trace.ret(E_FAIL);
      }
    }

    aerogpu_wddm_alloc_priv priv{};
    priv.magic = AEROGPU_WDDM_ALLOC_PRIV_MAGIC;
    priv.version = AEROGPU_WDDM_ALLOC_PRIV_VERSION;
    priv.alloc_id = alloc_id;
    priv.flags = AEROGPU_WDDM_ALLOC_PRIV_FLAG_IS_SHARED;
    priv.share_token = static_cast<aerogpu_wddm_u64>(share_token);
    priv.size_bytes = static_cast<aerogpu_wddm_u64>(res->size_bytes);
    priv.reserved0 = encode_wddm_alloc_priv_desc(res->format, res->width, res->height);
    std::memcpy(pCreateResource->pPrivateDriverData, &priv, sizeof(priv));

    res->backing_alloc_id = alloc_id;
    res->share_token = share_token;
  }

  bool has_wddm_allocation = (res->wddm_hAllocation != 0);
  bool allocation_created = false;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (!force_host_backing && !has_wddm_allocation && !open_existing_shared && dev->wddm_device != 0) {
    uint32_t alloc_id = res->backing_alloc_id;
    if (alloc_id == 0) {
      alloc_id = allocate_umd_alloc_id(dev->adapter);
      if (alloc_id == 0) {
        return E_OUTOFMEMORY;
      }
      res->backing_alloc_id = alloc_id;
    }

    aerogpu_wddm_alloc_priv priv_local{};
    aerogpu_wddm_alloc_priv* priv = &priv_local;

    // Prefer the runtime-provided private-data buffer when available: it avoids
    // passing a pointer to stack memory across the userkernel boundary.
    if (pCreateResource->pPrivateDriverData &&
        pCreateResource->PrivateDriverDataSize >= sizeof(aerogpu_wddm_alloc_priv)) {
      priv = reinterpret_cast<aerogpu_wddm_alloc_priv*>(pCreateResource->pPrivateDriverData);
    }

    // Treat the struct as in/out. Clear it so we never pick up stale bytes from
    // a previous call (which can cause cross-process collisions).
    std::memset(priv, 0, sizeof(*priv));
    priv->magic = AEROGPU_WDDM_ALLOC_PRIV_MAGIC;
    priv->version = AEROGPU_WDDM_ALLOC_PRIV_VERSION;
    priv->alloc_id = alloc_id;
    priv->flags = wants_shared ? AEROGPU_WDDM_ALLOC_PRIV_FLAG_IS_SHARED : AEROGPU_WDDM_ALLOC_PRIV_FLAG_NONE;
    // The Win7 KMD owns share_token generation; provide 0 as a placeholder.
    priv->share_token = 0;
    priv->size_bytes = static_cast<aerogpu_wddm_u64>(res->size_bytes);
    priv->reserved0 = encode_wddm_alloc_priv_desc(res->format, res->width, res->height);

    const HRESULT hr = wddm_create_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              res->size_bytes,
                                              priv,
                                              sizeof(*priv),
                                              &res->wddm_hAllocation,
                                              dev->wddm_context.hContext);
    if (FAILED(hr) || res->wddm_hAllocation == 0) {
      return FAILED(hr) ? hr : E_FAIL;
    }

    consume_wddm_alloc_priv(res.get(), priv, sizeof(*priv), wants_shared);
    if (wants_shared && res->share_token == 0) {
      logf("aerogpu-d3d9: KMD did not return share_token for shared alloc_id=%u\n", res->backing_alloc_id);
      return E_FAIL;
    }

    has_wddm_allocation = true;
    allocation_created = true;
  }
#endif

  if (!has_wddm_allocation) {
    // Fallback (non-WDDM builds): allocate CPU shadow storage.
    //
    // For non-shared resources, treat the host object as "host allocated" and
    // clear `backing_alloc_id` so update paths fall back to inline uploads
    // instead of alloc-table indirections (portable builds have no guest
    // allocation table backing).
    //
    // Shared resources still need a stable alloc_id/share_token contract for
    // EXPORT/IMPORT, so preserve `backing_alloc_id` even in portable builds.
    try {
      res->storage.resize(res->size_bytes);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
    res->wddm_hAllocation = 0;
    res->backing_offset_bytes = 0;
    if (!res->is_shared) {
      res->backing_alloc_id = 0;
    }

    // Portable builds do not have a Win7 KMD to generate a stable share_token for
    // shared allocations. Generate one in user mode and persist it into the
    // allocation private data blob so simulated cross-process opens observe the
    // same token.
    if (res->is_shared && res->share_token == 0 && dev->adapter) {
      res->share_token = dev->adapter->share_token_allocator.allocate_share_token();
      if (pCreateResource->pPrivateDriverData && pCreateResource->PrivateDriverDataSize >= sizeof(aerogpu_wddm_alloc_priv)) {
        auto* priv = reinterpret_cast<aerogpu_wddm_alloc_priv*>(pCreateResource->pPrivateDriverData);
        if (priv->magic == AEROGPU_WDDM_ALLOC_PRIV_MAGIC &&
            (priv->version == AEROGPU_WDDM_ALLOC_PRIV_VERSION || priv->version == AEROGPU_WDDM_ALLOC_PRIV_VERSION_2)) {
          priv->share_token = res->share_token;
        }
      }
    }
  }

  // Host-backed resources (backing_alloc_id==0) need CPU shadow storage for
  // Lock/Unlock and for UPLOAD_RESOURCE updates, even on WDDM builds where the
  // runtime may have attached a WDDM allocation handle (which we treat as
  // non-authoritative backing).
  if (res->backing_alloc_id == 0 && res->size_bytes != 0 && res->storage.size() < res->size_bytes) {
    try {
      res->storage.resize(res->size_bytes);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
  }

  if (open_existing_shared) {
    if (!res->share_token) {
      logf("aerogpu-d3d9: Open shared resource missing share_token (alloc_id=%u)\n", res->backing_alloc_id);
      return trace.ret(E_FAIL);
    }
    // Shared surface open (D3D9Ex): the host already has the original resource,
    // so we only create a new alias handle and IMPORT it.
    if (!emit_import_shared_surface_locked(dev, res.get())) {
      return trace.ret(E_OUTOFMEMORY);
    }
  } else {
    if (!emit_create_resource_locked(dev, res.get())) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (allocation_created && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      res->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        res->wddm_hAllocation = 0;
      }
#endif
      return trace.ret(E_OUTOFMEMORY);
    }

    if (res->is_shared) {
      if (!res->share_token) {
        logf("aerogpu-d3d9: Create shared resource missing share_token (alloc_id=%u)\n", res->backing_alloc_id);
      } else {
        // Shared surface create (D3D9Ex): export exactly once so other guest
        // processes can IMPORT using the same stable share_token.
        if (!emit_export_shared_surface_locked(dev, res.get())) {
          return trace.ret(E_OUTOFMEMORY);
        }

        // Shared surfaces must be importable by other processes immediately
        // after CreateResource returns. Since AeroGPU resource creation is
        // expressed in the command stream, force a submission so the host
        // observes the export.
        submit(dev);

        logf("aerogpu-d3d9: export shared_surface res=%u token=%llu\n",
             res->handle,
             static_cast<unsigned long long>(res->share_token));
      }
    }
  }

  pCreateResource->hResource.pDrvPrivate = res.release();
  return trace.ret(S_OK);
}

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
namespace {
template <typename T, typename = void>
struct aerogpu_has_member_hAllocation : std::false_type {};
template <typename T>
struct aerogpu_has_member_hAllocation<T, std::void_t<decltype(std::declval<T>().hAllocation)>> : std::true_type {};

template <typename T, typename = void>
struct aerogpu_has_member_hAllocations : std::false_type {};
template <typename T>
struct aerogpu_has_member_hAllocations<T, std::void_t<decltype(std::declval<T>().hAllocations)>> : std::true_type {};

template <typename T, typename = void>
struct aerogpu_has_member_phAllocation : std::false_type {};
template <typename T>
struct aerogpu_has_member_phAllocation<T, std::void_t<decltype(std::declval<T>().phAllocation)>> : std::true_type {};

template <typename T, typename = void>
struct aerogpu_has_member_NumAllocations : std::false_type {};
template <typename T>
struct aerogpu_has_member_NumAllocations<T, std::void_t<decltype(std::declval<T>().NumAllocations)>> : std::true_type {};

template <typename T, typename = void>
struct aerogpu_has_member_pOpenAllocationInfo : std::false_type {};
template <typename T>
struct aerogpu_has_member_pOpenAllocationInfo<T, std::void_t<decltype(std::declval<T>().pOpenAllocationInfo)>> : std::true_type {};

template <typename T, typename = void>
struct aerogpu_has_member_pAllocations : std::false_type {};
template <typename T>
struct aerogpu_has_member_pAllocations<T, std::void_t<decltype(std::declval<T>().pAllocations)>> : std::true_type {};
} // namespace

template <typename OpenResourceT>
WddmAllocationHandle get_wddm_allocation_from_openresource(const OpenResourceT* open_resource) {
  if (!open_resource) {
    return 0;
  }

  if constexpr (aerogpu_has_member_hAllocation<OpenResourceT>::value) {
    const auto h = static_cast<WddmAllocationHandle>(open_resource->hAllocation);
    if (h != 0) {
      return h;
    }
  }

  if constexpr (aerogpu_has_member_hAllocations<OpenResourceT>::value) {
    const auto& h_allocations = open_resource->hAllocations;
    using AllocationsT = std::remove_reference_t<decltype(h_allocations)>;
    if constexpr (std::is_array_v<AllocationsT>) {
      const auto h = static_cast<WddmAllocationHandle>(h_allocations[0]);
      if (h != 0) {
        return h;
      }
    } else if constexpr (std::is_pointer_v<AllocationsT>) {
      if (h_allocations) {
        const auto h = static_cast<WddmAllocationHandle>(h_allocations[0]);
        if (h != 0) {
          return h;
        }
      }
    } else if constexpr (std::is_integral_v<AllocationsT>) {
      const auto h = static_cast<WddmAllocationHandle>(h_allocations);
      if (h != 0) {
        return h;
      }
    }
  }

  if constexpr (aerogpu_has_member_phAllocation<OpenResourceT>::value && aerogpu_has_member_NumAllocations<OpenResourceT>::value) {
    if (open_resource->phAllocation && open_resource->NumAllocations) {
      const auto h = static_cast<WddmAllocationHandle>(open_resource->phAllocation[0]);
      if (h != 0) {
        return h;
      }
    }
  }

  if constexpr (aerogpu_has_member_pOpenAllocationInfo<OpenResourceT>::value && aerogpu_has_member_NumAllocations<OpenResourceT>::value) {
    if (open_resource->pOpenAllocationInfo && open_resource->NumAllocations) {
      using InfoT = std::remove_pointer_t<decltype(open_resource->pOpenAllocationInfo)>;
      if constexpr (aerogpu_has_member_hAllocation<InfoT>::value) {
        const auto h = static_cast<WddmAllocationHandle>(open_resource->pOpenAllocationInfo[0].hAllocation);
        if (h != 0) {
          return h;
        }
      }
    }
  }

  if constexpr (aerogpu_has_member_pAllocations<OpenResourceT>::value) {
    const auto* allocs = open_resource->pAllocations;
    if (allocs) {
      using Elem = std::remove_pointer_t<decltype(allocs)>;
      if constexpr (std::is_class_v<Elem> && aerogpu_has_member_hAllocation<Elem>::value) {
        const auto h = static_cast<WddmAllocationHandle>(allocs[0].hAllocation);
        if (h != 0) {
          return h;
        }
      } else if constexpr (!std::is_class_v<Elem> && std::is_convertible_v<Elem, WddmAllocationHandle>) {
        const auto h = static_cast<WddmAllocationHandle>(allocs[0]);
        if (h != 0) {
          return h;
        }
      }
    }
  }

  return 0;
}
#endif

static HRESULT device_open_resource_impl(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_OPENRESOURCE* pOpenResource) {
  if (!hDevice.pDrvPrivate || !pOpenResource) {
    return E_INVALIDARG;
  }

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }

  const void* priv_data = d3d9_private_driver_data_ptr(*pOpenResource);
  const uint32_t priv_data_size = d3d9_private_driver_data_size(*pOpenResource);

  if (!priv_data || priv_data_size < sizeof(aerogpu_wddm_alloc_priv)) {
    return E_INVALIDARG;
  }

  aerogpu_wddm_alloc_priv priv{};
  std::memcpy(&priv, priv_data, sizeof(priv));
  if (priv.magic != AEROGPU_WDDM_ALLOC_PRIV_MAGIC ||
      (priv.version != AEROGPU_WDDM_ALLOC_PRIV_VERSION && priv.version != AEROGPU_WDDM_ALLOC_PRIV_VERSION_2)) {
    return E_INVALIDARG;
  }
  if ((priv.flags & AEROGPU_WDDM_ALLOC_PRIV_FLAG_IS_SHARED) == 0 || priv.share_token == 0 || priv.alloc_id == 0) {
    return E_INVALIDARG;
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  auto res = make_unique_nothrow<Resource>();
  if (!res) {
    return E_OUTOFMEMORY;
  }
  res->handle = allocate_global_handle(dev->adapter);

  res->is_shared = true;
  res->is_shared_alias = true;
  res->share_token = priv.share_token;
  res->backing_alloc_id = priv.alloc_id;
  res->backing_offset_bytes = 0;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  res->wddm_hAllocation = get_wddm_allocation_from_openresource(pOpenResource);
#else
  res->wddm_hAllocation = static_cast<WddmAllocationHandle>(pOpenResource->wddm_hAllocation);
#endif
  if (dev->wddm_context.hContext != 0 && res->backing_alloc_id != 0 && res->wddm_hAllocation == 0) {
    logf("aerogpu-d3d9: OpenResource missing WDDM hAllocation (alloc_id=%u)\n", res->backing_alloc_id);
    return E_FAIL;
  }

  // OpenResource DDI structs vary across WDK header vintages. Some header sets do
  // not include a full resource description, so treat all description fields as
  // optional and fall back to the encoded `priv.reserved0` description when
  // available.
  res->type = d3d9_optional_resource_type(*pOpenResource);
  res->format = static_cast<D3DDDIFORMAT>(d3d9_optional_resource_format(*pOpenResource));
  res->width = d3d9_optional_resource_width(*pOpenResource);
  res->height = d3d9_optional_resource_height(*pOpenResource);
  res->depth = std::max(1u, d3d9_resource_depth(*pOpenResource));
  res->usage = d3d9_resource_usage(*pOpenResource);
  res->pool = d3d9_resource_pool(*pOpenResource);
  const uint32_t open_size_bytes = d3d9_resource_size(*pOpenResource);
  const uint32_t requested_mip_levels = d3d9_resource_mip_levels(*pOpenResource);

  if (open_size_bytes == 0 && res->type == kD3dRTypeCubeTexture) {
    // Mirror CreateResource normalization: treat cube textures as arrays with 6
    // layers, even if the runtime did not populate Depth in the OpenResource DDI
    // struct.
    res->depth = 6;
  }

  uint32_t mip_levels = std::max(1u, requested_mip_levels);
  if (open_size_bytes == 0 &&
      requested_mip_levels == 0 &&
      res->width != 0 &&
      res->height != 0) {
    // D3D9 OpenResource semantics: MipLevels=0 means "allocate the full mip chain"
    // (same as CreateResource), but only when the runtime supplies an explicit
    // width/height (otherwise we cannot infer the chain length here).
    mip_levels = calc_full_mip_chain_levels_2d(res->width, res->height);
  }
  res->mip_levels = mip_levels;

  // AeroGPU cannot represent volume resources in the guesthost protocol. Reject
  // shared allocations that describe them to avoid importing a handle that the
  // host will interpret incorrectly.
  //
  // Cube textures are represented as 2D arrays with Depth==6 and are accepted.
  if (open_size_bytes == 0 &&
      !d3d9_validate_nonbuffer_type(res->type, res->depth, D3d9NonBufferTypeValidation::AllowUnknownAsSurface)) {
    return D3DERR_INVALIDCALL;
  }

  const aerogpu_wddm_alloc_priv_v2* priv2 = nullptr;
  if (priv.version == AEROGPU_WDDM_ALLOC_PRIV_VERSION_2 &&
      priv_data_size >= sizeof(aerogpu_wddm_alloc_priv_v2)) {
    const auto* tmp = reinterpret_cast<const aerogpu_wddm_alloc_priv_v2*>(priv_data);
    if (tmp->magic == AEROGPU_WDDM_ALLOC_PRIV_MAGIC && tmp->version == AEROGPU_WDDM_ALLOC_PRIV_VERSION_2) {
      priv2 = tmp;
    }
  }

  // For non-descriptor reserved0 values, the low 32 bits may carry a pitch hint
  // (used by D3D10/11 shared surfaces). This enables D3D9 consumers (e.g. DWM)
  // to Lock a shared allocation with the correct row pitch even when the runtime
  // does not provide it explicitly in the OpenResource DDI struct.
  uint32_t priv_pitch_bytes = 0;
  if (!AEROGPU_WDDM_ALLOC_PRIV_DESC_PRESENT(priv.reserved0)) {
    priv_pitch_bytes = static_cast<uint32_t>(priv.reserved0 & 0xFFFFFFFFull);
  }
  if (priv_pitch_bytes == 0 && priv2 && priv2->row_pitch_bytes != 0) {
    priv_pitch_bytes = static_cast<uint32_t>(priv2->row_pitch_bytes);
  }

  uint32_t desc_format = 0;
  uint32_t desc_width = 0;
  uint32_t desc_height = 0;
  if (decode_wddm_alloc_priv_desc(priv.reserved0, &desc_format, &desc_width, &desc_height)) {
    if (res->format == 0) {
      res->format = desc_format;
    }
    if (res->width == 0) {
      res->width = desc_width;
    }
    if (res->height == 0) {
      res->height = desc_height;
    }
  }

  // DXGI/D3D10/11 shared resources opened through the D3D9 runtime may not
  // provide a D3D9-format description in reserved0. If the caller did not supply
  // a usable description, fall back to v2 private-data metadata (DXGI format,
  // width/height) and map to the closest D3D9 format so we can construct a
  // compatible surface layout for Lock/CPU-side helpers.
  if (priv2) {
    if (res->width == 0 && priv2->width != 0) {
      res->width = static_cast<uint32_t>(priv2->width);
    }
    if (res->height == 0 && priv2->height != 0) {
      res->height = static_cast<uint32_t>(priv2->height);
    }
    if (res->format == 0 && priv2->format != 0) {
      // DXGI_FORMAT subset (numeric values from dxgiformat.h).
      // Keep this list minimal and self-contained so portable builds do not
      // require the Windows SDK headers.
      //
      // Include common 16-bit formats as well: D3D9Ex consumers (notably DWM)
      // can open DXGI shared resources created by D3D10/11 apps, and those apps
      // may legitimately use B5G6R5/B5G5R5A1 surfaces.
      constexpr uint32_t kDxgiFmtB5G6R5Unorm = 85;
      constexpr uint32_t kDxgiFmtB5G5R5A1Unorm = 86;
      constexpr uint32_t kDxgiFmtR8G8B8A8Typeless = 27;
      constexpr uint32_t kDxgiFmtR8G8B8A8Unorm = 28;
      constexpr uint32_t kDxgiFmtR8G8B8A8UnormSrgb = 29;
      constexpr uint32_t kDxgiFmtB8G8R8A8Unorm = 87;
      constexpr uint32_t kDxgiFmtB8G8R8X8Unorm = 88;
      constexpr uint32_t kDxgiFmtB8G8R8A8Typeless = 90;
      constexpr uint32_t kDxgiFmtB8G8R8A8UnormSrgb = 91;
      constexpr uint32_t kDxgiFmtB8G8R8X8Typeless = 92;
      constexpr uint32_t kDxgiFmtB8G8R8X8UnormSrgb = 93;

      // D3D9 format subset (numeric values from d3d9types.h).
      constexpr uint32_t kD3d9FmtA8R8G8B8 = 21u;
      constexpr uint32_t kD3d9FmtX8R8G8B8 = 22u;
      constexpr uint32_t kD3d9FmtA8B8G8R8 = 32u;
      constexpr uint32_t kD3d9FmtR5G6B5 = 23u;
      constexpr uint32_t kD3d9FmtA1R5G5B5 = 25u;

      const uint32_t dxgi_fmt = static_cast<uint32_t>(priv2->format);
      uint32_t d3d9_fmt = 0;
      switch (dxgi_fmt) {
        case kDxgiFmtB5G6R5Unorm:
          d3d9_fmt = kD3d9FmtR5G6B5;
          break;
        case kDxgiFmtB5G5R5A1Unorm:
          d3d9_fmt = kD3d9FmtA1R5G5B5;
          break;
        case kDxgiFmtB8G8R8A8Unorm:
        case kDxgiFmtB8G8R8A8Typeless:
        case kDxgiFmtB8G8R8A8UnormSrgb:
          d3d9_fmt = kD3d9FmtA8R8G8B8;
          break;
        case kDxgiFmtB8G8R8X8Unorm:
        case kDxgiFmtB8G8R8X8Typeless:
        case kDxgiFmtB8G8R8X8UnormSrgb:
          d3d9_fmt = kD3d9FmtX8R8G8B8;
          break;
        case kDxgiFmtR8G8B8A8Unorm:
        case kDxgiFmtR8G8B8A8Typeless:
        case kDxgiFmtR8G8B8A8UnormSrgb:
          d3d9_fmt = kD3d9FmtA8B8G8R8;
          break;
        default:
          break;
      }
      if (d3d9_fmt != 0) {
        res->format = static_cast<D3DDDIFORMAT>(d3d9_fmt);
      }
    }
  }

  // The AeroGPU host executor currently interprets `array_layers==6` as a cube
  // texture (via a cube view). Cube textures must be square; reject invalid
  // descriptors early so we don't import a shared handle that the host will later
  // reject during view creation/binding.
  if (open_size_bytes == 0 &&
      res->depth == 6 &&
      res->width != 0 &&
      res->height != 0 &&
      res->width != res->height) {
    return D3DERR_INVALIDCALL;
  }

  // Prefer a reconstructed size when the runtime provides a description; fall
  // back to the size_bytes persisted in allocation private data.
  if (open_size_bytes) {
    res->kind = ResourceKind::Buffer;
    res->size_bytes = open_size_bytes;
    res->row_pitch = 0;
    res->slice_pitch = 0;
  } else if (res->width && res->height) {
    // Mirror CreateResource semantics: treat Depth>1 as a texture2D array (used
    // for cube textures with 6 layers), not a single-subresource surface.
    res->kind =
        (res->mip_levels > 1 || res->depth > 1) ? ResourceKind::Texture2D : ResourceKind::Surface;

    Texture2dLayout layout{};
    if (!calc_texture2d_layout(res->format, res->width, res->height, res->mip_levels, res->depth, &layout)) {
      return E_OUTOFMEMORY;
    }
    if (layout.total_size_bytes > 0x7FFFFFFFu) {
      return E_OUTOFMEMORY;
    }

    uint32_t row_pitch = layout.row_pitch_bytes;
    uint32_t slice_pitch = layout.slice_pitch_bytes;
    uint32_t size_bytes = static_cast<uint32_t>(layout.total_size_bytes);

    // If the private driver data provides an explicit row pitch (either via the
    // legacy reserved0 pitch encoding or the v2 row_pitch_bytes field), prefer
    // it for uncompressed single-mip surfaces. This matches the layout the
    // producing runtime/KMD will use for the shared allocation.
    if (res->mip_levels == 1 && res->depth == 1 && !is_block_compressed_format(res->format) &&
        priv_pitch_bytes != 0) {
      const uint64_t slice_u64 = static_cast<uint64_t>(priv_pitch_bytes) * static_cast<uint64_t>(res->height);
      if (priv_pitch_bytes >= row_pitch && slice_u64 <= 0x7FFFFFFFull && slice_u64 <= 0xFFFFFFFFull) {
        row_pitch = priv_pitch_bytes;
        slice_pitch = static_cast<uint32_t>(slice_u64);
        if (priv.size_bytes != 0 && priv.size_bytes <= 0x7FFFFFFFu && priv.size_bytes >= slice_u64) {
          size_bytes = static_cast<uint32_t>(priv.size_bytes);
        } else {
          size_bytes = static_cast<uint32_t>(slice_u64);
        }
      }
    }

    res->row_pitch = row_pitch;
    res->slice_pitch = slice_pitch;
    res->size_bytes = size_bytes;
  } else if (priv.size_bytes != 0 && priv.size_bytes <= 0x7FFFFFFFu) {
    res->kind = ResourceKind::Surface;
    res->size_bytes = static_cast<uint32_t>(priv.size_bytes);
    res->row_pitch = 0;
    res->slice_pitch = 0;
  } else {
    return E_INVALIDARG;
  }

  if (res->kind != ResourceKind::Buffer) {
    const uint32_t agpu_format = d3d9_format_to_aerogpu(res->format);
    if (agpu_format == AEROGPU_FORMAT_INVALID) {
      return E_INVALIDARG;
    }

    if (is_block_compressed_format(res->format) && !SupportsBcFormats(dev)) {
      return E_INVALIDARG;
    }
  }

  if (!res->size_bytes) {
    return E_INVALIDARG;
  }

  bool want_cpu_shadow = true;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  // In real WDDM builds, shared/OpenResource allocations are backed by a WDDM
  // system-memory allocation and are CPU-accessible via LockCb. Avoid allocating
  // an additional full-size CPU shadow buffer for these resources: DWM can open
  // many redirected surfaces, and double-buffering them in user mode wastes
  // significant memory.
  if (dev->wddm_device != 0 && res->wddm_hAllocation != 0 && res->backing_alloc_id != 0) {
    want_cpu_shadow = false;
  }
#endif

  if (want_cpu_shadow) {
    try {
      res->storage.resize(res->size_bytes);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
  }

  if (!emit_import_shared_surface_locked(dev, res.get())) {
    return E_OUTOFMEMORY;
  }

  logf("aerogpu-d3d9: import shared_surface out_res=%u token=%llu alloc_id=%u hAllocation=0x%08x\n",
       res->handle,
        static_cast<unsigned long long>(res->share_token),
        static_cast<unsigned>(res->backing_alloc_id),
        static_cast<unsigned>(res->wddm_hAllocation));

  pOpenResource->hResource.pDrvPrivate = res.release();
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_open_resource(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_OPENRESOURCE* pOpenResource) {
  uint64_t arg0 = d3d9_trace_arg_ptr(hDevice.pDrvPrivate);
  uint64_t arg1 = d3d9_trace_arg_ptr(pOpenResource);
  uint64_t arg2 = 0;
  uint64_t arg3 = 0;
  if constexpr (aerogpu_d3d9_has_member_Type<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_type<D3D9DDIARG_OPENRESOURCE>::value ||
                aerogpu_d3d9_has_member_Width<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_width<D3D9DDIARG_OPENRESOURCE>::value ||
                aerogpu_d3d9_has_member_Height<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_height<D3D9DDIARG_OPENRESOURCE>::value) {
    arg1 = pOpenResource
               ? d3d9_trace_pack_u32_u32(d3d9_optional_resource_type(*pOpenResource), d3d9_optional_resource_format(*pOpenResource))
               : 0;
    arg2 = pOpenResource
               ? d3d9_trace_pack_u32_u32(d3d9_optional_resource_width(*pOpenResource), d3d9_optional_resource_height(*pOpenResource))
               : 0;
    arg3 = pOpenResource ? d3d9_trace_pack_u32_u32(d3d9_resource_usage(*pOpenResource), d3d9_private_driver_data_size(*pOpenResource)) : 0;
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceOpenResource, arg0, arg1, arg2, arg3);
  return trace.ret(device_open_resource_impl(hDevice, pOpenResource));
}

HRESULT AEROGPU_D3D9_CALL device_open_resource2(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_OPENRESOURCE* pOpenResource) {
  uint64_t arg0 = d3d9_trace_arg_ptr(hDevice.pDrvPrivate);
  uint64_t arg1 = d3d9_trace_arg_ptr(pOpenResource);
  uint64_t arg2 = 0;
  uint64_t arg3 = 0;
  if constexpr (aerogpu_d3d9_has_member_Type<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_type<D3D9DDIARG_OPENRESOURCE>::value ||
                aerogpu_d3d9_has_member_Width<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_width<D3D9DDIARG_OPENRESOURCE>::value ||
                aerogpu_d3d9_has_member_Height<D3D9DDIARG_OPENRESOURCE>::value || aerogpu_d3d9_has_member_height<D3D9DDIARG_OPENRESOURCE>::value) {
    arg1 = pOpenResource
               ? d3d9_trace_pack_u32_u32(d3d9_optional_resource_type(*pOpenResource), d3d9_optional_resource_format(*pOpenResource))
               : 0;
    arg2 = pOpenResource
               ? d3d9_trace_pack_u32_u32(d3d9_optional_resource_width(*pOpenResource), d3d9_optional_resource_height(*pOpenResource))
               : 0;
    arg3 = pOpenResource ? d3d9_trace_pack_u32_u32(d3d9_resource_usage(*pOpenResource), d3d9_private_driver_data_size(*pOpenResource)) : 0;
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceOpenResource2, arg0, arg1, arg2, arg3);
  return trace.ret(device_open_resource_impl(hDevice, pOpenResource));
}

HRESULT AEROGPU_D3D9_CALL device_destroy_resource(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hResource) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroyResource,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hResource.pDrvPrivate),
                      0,
                      0);
  auto* dev = as_device(hDevice);
  auto* res = as_resource(hResource);
  if (!dev || !res) {
    delete res;
    return trace.ret(S_OK);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  // Ensure any queued commands referencing this allocation are submitted before
  // we release the kernel allocation handle.
  (void)submit(dev);

  for (SwapChain* sc : dev->swapchains) {
    if (!sc) {
      continue;
    }
    auto& bbs = sc->backbuffers;
    bbs.erase(std::remove(bbs.begin(), bbs.end(), res), bbs.end());
  }

  // Defensive: DWM and other D3D9Ex clients can destroy resources while they are
  // still bound. Clear any cached bindings that point at the resource before we
  // delete it so subsequent command emission does not dereference a dangling
  // pointer.
  bool rt_changed = false;
  for (uint32_t i = 0; i < 4; ++i) {
    if (dev->render_targets[i] == res) {
      dev->render_targets[i] = nullptr;
      rt_changed = true;
    }
  }
  if (dev->depth_stencil == res) {
    dev->depth_stencil = nullptr;
    rt_changed = true;
  }

  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (dev->textures[stage] != res) {
      continue;
    }
    dev->textures[stage] = nullptr;
    if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE)) {
      cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
      cmd->slot = stage;
      cmd->texture = 0;
      cmd->reserved0 = 0;
    }
  }

  if (dev->cursor_bitmap == res) {
    const bool was_hw_active = dev->cursor_hw_active;
    dev->cursor_bitmap = nullptr;
    dev->cursor_visible = FALSE;
    dev->cursor_hw_active = false;
    // Invalidate any in-flight SetCursorProperties call that may still be
    // programming hardware cursor state after releasing the device mutex.
    dev->cursor_bitmap_serial++;
    if (was_hw_active && dev->adapter) {
      aerogpu_escape_set_cursor_visibility_in vis{};
      vis.hdr.version = AEROGPU_ESCAPE_VERSION;
      vis.hdr.op = AEROGPU_ESCAPE_OP_SET_CURSOR_VISIBILITY;
      vis.hdr.size = sizeof(vis);
      vis.hdr.reserved0 = 0;
      vis.visible = 0;
      vis.reserved0 = 0;
      (void)dev->adapter->kmd_query.SendEscape(&vis, sizeof(vis));
    }
  }

  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (dev->streams[stream].vb != res) {
      continue;
    }
    dev->streams[stream] = {};

    aerogpu_vertex_buffer_binding binding{};
    binding.buffer = 0;
    binding.stride_bytes = 0;
    binding.offset_bytes = 0;
    binding.reserved0 = 0;

    if (auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
            dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding))) {
      cmd->start_slot = stream;
      cmd->buffer_count = 1;
    }
  }

  if (dev->index_buffer == res) {
    dev->index_buffer = nullptr;
    dev->index_offset_bytes = 0;
    dev->index_format = kD3dFmtIndex16;

    if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER)) {
      cmd->buffer = 0;
      cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
      cmd->offset_bytes = 0;
      cmd->reserved0 = 0;
    }
  }

  if (rt_changed) {
    (void)emit_set_render_targets_locked(dev);
  }
  // Shared surfaces are refcounted host-side: DESTROY_RESOURCE releases a single
  // handle (original or alias) and the underlying surface is freed once the last
  // reference is gone.
  (void)emit_destroy_resource_locked(dev, res->handle);

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
    // Ensure the allocation handle is no longer referenced by the current DMA
    // buffer before we destroy it.
    (void)submit(dev);
    const HRESULT hr =
        wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(hr)) {
      logf("aerogpu-d3d9: DestroyAllocation failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
           static_cast<unsigned long>(hr),
           static_cast<unsigned>(res->backing_alloc_id),
           static_cast<unsigned long long>(res->wddm_hAllocation));
    }
    res->wddm_hAllocation = 0;
  }
#endif
  delete res;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_create_swap_chain(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_CREATESWAPCHAIN* pCreateSwapChain) {
  const D3D9DDI_PRESENT_PARAMETERS* trace_pp = pCreateSwapChain ? d3d9_get_present_params(*pCreateSwapChain) : nullptr;
  const uint64_t bb_wh =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_width(*trace_pp), d3d9_pp_backbuffer_height(*trace_pp)) : 0;
  const uint64_t fmt_count =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_format(*trace_pp), d3d9_pp_backbuffer_count(*trace_pp)) : 0;
  const uint64_t interval_flags =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_presentation_interval(*trace_pp), d3d9_pp_flags(*trace_pp)) : 0;
  D3d9TraceCall trace(
      D3d9TraceFunc::DeviceCreateSwapChain, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), bb_wh, fmt_count, interval_flags);
  if (!hDevice.pDrvPrivate || !pCreateSwapChain) {
    return trace.ret(E_INVALIDARG);
  }

  // Default to null outputs so error paths never leak stale runtime handles.
  pCreateSwapChain->hBackBuffer.pDrvPrivate = nullptr;
  pCreateSwapChain->hSwapChain.pDrvPrivate = nullptr;

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  const D3D9DDI_PRESENT_PARAMETERS* pp = d3d9_get_present_params(*pCreateSwapChain);
  if (!pp) {
    return trace.ret(E_INVALIDARG);
  }
  const D3DDDIFORMAT bb_fmt = static_cast<D3DDDIFORMAT>(d3d9_pp_backbuffer_format(*pp));
  if (!is_supported_backbuffer_format(bb_fmt)) {
    return trace.ret(E_INVALIDARG);
  }
  if (d3d9_format_to_aerogpu(d3d9_pp_backbuffer_format(*pp)) == AEROGPU_FORMAT_INVALID) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t width = d3d9_pp_backbuffer_width(*pp) ? d3d9_pp_backbuffer_width(*pp) : 1u;
  const uint32_t height = d3d9_pp_backbuffer_height(*pp) ? d3d9_pp_backbuffer_height(*pp) : 1u;
  const uint32_t backbuffer_count = std::max(1u, d3d9_pp_backbuffer_count(*pp));

  std::lock_guard<std::mutex> lock(dev->mutex);

  auto sc = make_unique_nothrow<SwapChain>();
  if (!sc) {
    return trace.ret(E_OUTOFMEMORY);
  }
  sc->handle = allocate_global_handle(dev->adapter);
  sc->hwnd = pp->hDeviceWindow;
  sc->width = width;
  sc->height = height;
  sc->format = d3d9_pp_backbuffer_format(*pp);
  sc->sync_interval = d3d9_pp_presentation_interval(*pp);
  sc->swap_effect = d3d9_pp_swap_effect(*pp);
  sc->flags = d3d9_pp_flags(*pp);

  auto destroy_swapchain_backbuffers_locked = [&](SwapChain* sc_to_destroy) {
    if (!sc_to_destroy) {
      return;
    }
    // Best-effort cleanup: emit host-side destroys for any already-created
    // backbuffers, submit so the runtime sees a consistent alloc list, then
    // destroy the per-process WDDM allocations.
    for (Resource* created : sc_to_destroy->backbuffers) {
      if (!created) {
        continue;
      }
      (void)emit_destroy_resource_locked(dev, created->handle);
    }
    (void)submit(dev);

    for (Resource* created : sc_to_destroy->backbuffers) {
      if (!created) {
        continue;
      }
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (created->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      created->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        created->wddm_hAllocation = 0;
      }
#endif
      delete created;
    }
    sc_to_destroy->backbuffers.clear();
  };

  try {
    sc->backbuffers.reserve(backbuffer_count);
  } catch (...) {
    return trace.ret(E_OUTOFMEMORY);
  }
  for (uint32_t i = 0; i < backbuffer_count; i++) {
    auto bb = make_unique_nothrow<Resource>();
    if (!bb) {
      destroy_swapchain_backbuffers_locked(sc.get());
      return trace.ret(E_OUTOFMEMORY);
    }
    HRESULT hr = create_backbuffer_locked(dev, bb.get(), sc->format, sc->width, sc->height);
    if (hr < 0) {
      destroy_swapchain_backbuffers_locked(sc.get());
      return trace.ret(hr);
    }
    Resource* bb_raw = bb.get();
    try {
      sc->backbuffers.push_back(bb_raw);
    } catch (...) {
      Resource* extra = bb.release();
      (void)emit_destroy_resource_locked(dev, extra->handle);
      destroy_swapchain_backbuffers_locked(sc.get());
      (void)submit(dev);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (extra->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      extra->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        extra->wddm_hAllocation = 0;
      }
#endif
      delete extra;
      return trace.ret(E_OUTOFMEMORY);
    }
    bb.release();
  }

  Resource* first_backbuffer = sc->backbuffers.empty() ? nullptr : sc->backbuffers[0];

  // Default D3D9 behavior: the first backbuffer is bound as render target 0.
  bool bound_rt0 = false;
  if (!dev->render_targets[0] && first_backbuffer) {
    dev->render_targets[0] = first_backbuffer;
    bound_rt0 = true;
    if (!emit_set_render_targets_locked(dev)) {
      // Keep driver state consistent with the host by rolling back the implicit
      // binding and tearing down the partially-created swapchain.
      dev->render_targets[0] = nullptr;
      destroy_swapchain_backbuffers_locked(sc.get());
      return trace.ret(E_OUTOFMEMORY);
    }
  }

  SwapChain* sc_raw = sc.get();
  try {
    dev->swapchains.push_back(sc_raw);
  } catch (...) {
    if (bound_rt0) {
      dev->render_targets[0] = nullptr;
      (void)emit_set_render_targets_locked(dev);
    }
    destroy_swapchain_backbuffers_locked(sc.get());
    return trace.ret(E_OUTOFMEMORY);
  }
  (void)sc.release();
  if (!dev->current_swapchain) {
    dev->current_swapchain = dev->swapchains.back();
  }

  pCreateSwapChain->hBackBuffer.pDrvPrivate = first_backbuffer;
  pCreateSwapChain->hSwapChain.pDrvPrivate = sc_raw;

  return trace.ret(S_OK);
}

HRESULT copy_surface_rects(Device* dev, const Resource* src, Resource* dst, const RECT* rects, uint32_t rect_count) {
  if (!rects || rect_count == 0) {
    return copy_surface_bytes(dev, src, dst);
  }
  if (!dev || !src || !dst) {
    return E_INVALIDARG;
  }
  if (src->format != dst->format) {
    return E_INVALIDARG;
  }
  if (is_block_compressed_format(src->format)) {
    // Rect-based copies operate in pixels and do not support BC formats.
    return E_INVALIDARG;
  }

  const uint32_t bpp = bytes_per_pixel(src->format);

  struct Map {
    void* ptr = nullptr;
    bool wddm_locked = false;
  };

  Map src_map{};
  Map dst_map{};
  const uint8_t* src_base = nullptr;
  uint8_t* dst_base = nullptr;

  const uint64_t src_bytes = src->slice_pitch;
  const uint64_t dst_bytes = dst->slice_pitch;
  if (src_bytes == 0 || dst_bytes == 0 || src_bytes > src->size_bytes || dst_bytes > dst->size_bytes) {
    return E_FAIL;
  }

  bool use_src_storage = src->storage.size() >= src_bytes;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (src->backing_alloc_id != 0) {
    use_src_storage = false;
  }
#endif
  if (use_src_storage) {
    src_base = src->storage.data();
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (src->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              src->wddm_hAllocation,
                                              0,
                                              src_bytes,
                                              kD3DLOCK_READONLY,
                                              &src_map.ptr,
                                              dev->wddm_context.hContext);
      if (FAILED(hr) || !src_map.ptr) {
        return FAILED(hr) ? hr : E_FAIL;
      }
      src_map.wddm_locked = true;
      src_base = static_cast<const uint8_t*>(src_map.ptr);
    } else
#endif
    {
      return E_FAIL;
    }
  }

  bool use_dst_storage = dst->storage.size() >= dst_bytes;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dst->backing_alloc_id != 0) {
    use_dst_storage = false;
  }
#endif
  if (use_dst_storage) {
    dst_base = dst->storage.data();
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (dst->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                              dev->wddm_device,
                                              dst->wddm_hAllocation,
                                              0,
                                              dst_bytes,
                                              &dst_map.ptr,
                                              dev->wddm_context.hContext);
      if (FAILED(hr) || !dst_map.ptr) {
        if (src_map.wddm_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                       dev->wddm_device,
                                       src->wddm_hAllocation,
                                       dev->wddm_context.hContext);
        }
        return FAILED(hr) ? hr : E_FAIL;
      }
      dst_map.wddm_locked = true;
      dst_base = static_cast<uint8_t*>(dst_map.ptr);
    } else
#endif
    {
      if (src_map.wddm_locked) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                     dev->wddm_device,
                                     src->wddm_hAllocation,
                                     dev->wddm_context.hContext);
      }
      return E_FAIL;
    }
  }

  for (uint32_t i = 0; i < rect_count; ++i) {
    const RECT& r = rects[i];
    if (r.right <= r.left || r.bottom <= r.top) {
      continue;
    }

    const uint32_t left = static_cast<uint32_t>(std::max<long>(0, r.left));
    const uint32_t top = static_cast<uint32_t>(std::max<long>(0, r.top));
    const uint32_t right = static_cast<uint32_t>(std::max<long>(0, r.right));
    const uint32_t bottom = static_cast<uint32_t>(std::max<long>(0, r.bottom));

    const uint32_t clamped_right = std::min<uint32_t>({right, src->width, dst->width});
    const uint32_t clamped_bottom = std::min<uint32_t>({bottom, src->height, dst->height});

    if (left >= clamped_right || top >= clamped_bottom) {
      continue;
    }

    const uint32_t row_bytes = (clamped_right - left) * bpp;
    for (uint32_t y = top; y < clamped_bottom; ++y) {
      const size_t src_off = static_cast<size_t>(y) * src->row_pitch + static_cast<size_t>(left) * bpp;
      const size_t dst_off = static_cast<size_t>(y) * dst->row_pitch + static_cast<size_t>(left) * bpp;
      if (src_off + row_bytes > src_bytes || dst_off + row_bytes > dst_bytes) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (dst_map.wddm_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                       dev->wddm_device,
                                       dst->wddm_hAllocation,
                                       dev->wddm_context.hContext);
        }
        if (src_map.wddm_locked) {
          (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                       dev->wddm_device,
                                       src->wddm_hAllocation,
                                       dev->wddm_context.hContext);
        }
#endif
        return E_INVALIDARG;
      }
      std::memcpy(dst_base + dst_off, src_base + src_off, row_bytes);
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (dst_map.wddm_locked) {
    (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                 dev->wddm_device,
                                 dst->wddm_hAllocation,
                                 dev->wddm_context.hContext);
  }
  if (src_map.wddm_locked) {
    (void)wddm_unlock_allocation(dev->wddm_callbacks,
                                 dev->wddm_device,
                                 src->wddm_hAllocation,
                                 dev->wddm_context.hContext);
  }
#endif

  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_destroy_swap_chain(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HSWAPCHAIN hSwapChain) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroySwapChain,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hSwapChain.pDrvPrivate),
                      0,
                      0);
  auto* dev = as_device(hDevice);
  auto* sc = as_swapchain(hSwapChain);
  if (!dev || !sc) {
    delete sc;
    return trace.ret(S_OK);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  // Ensure we are not about to destroy an allocation handle that is still
  // referenced by the current DMA buffer.
  (void)submit(dev);

  auto it = std::find(dev->swapchains.begin(), dev->swapchains.end(), sc);
  if (it != dev->swapchains.end()) {
    dev->swapchains.erase(it);
  }
  if (dev->current_swapchain == sc) {
    dev->current_swapchain = dev->swapchains.empty() ? nullptr : dev->swapchains[0];
  }

  bool rt_changed = false;
  for (Resource* bb : sc->backbuffers) {
    if (!bb) {
      continue;
    }
    for (uint32_t i = 0; i < 4; ++i) {
      if (dev->render_targets[i] == bb) {
        dev->render_targets[i] = nullptr;
        rt_changed = true;
      }
    }
    if (dev->depth_stencil == bb) {
      dev->depth_stencil = nullptr;
      rt_changed = true;
    }

    for (uint32_t stage = 0; stage < 16; ++stage) {
      if (dev->textures[stage] != bb) {
        continue;
      }
      dev->textures[stage] = nullptr;
      if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE)) {
        cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
        cmd->slot = stage;
        cmd->texture = 0;
        cmd->reserved0 = 0;
      }
    }

    for (uint32_t stream = 0; stream < 16; ++stream) {
      if (dev->streams[stream].vb != bb) {
        continue;
      }
      dev->streams[stream] = {};

      aerogpu_vertex_buffer_binding binding{};
      binding.buffer = 0;
      binding.stride_bytes = 0;
      binding.offset_bytes = 0;
      binding.reserved0 = 0;

      if (auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
              dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding))) {
        cmd->start_slot = stream;
        cmd->buffer_count = 1;
      }
    }

    if (dev->index_buffer == bb) {
      dev->index_buffer = nullptr;
      dev->index_offset_bytes = 0;
      dev->index_format = kD3dFmtIndex16;

      if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER)) {
        cmd->buffer = 0;
        cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
        cmd->offset_bytes = 0;
        cmd->reserved0 = 0;
      }
    }
  }

  if (rt_changed) {
    (void)emit_set_render_targets_locked(dev);
  }

  for (Resource* bb : sc->backbuffers) {
    if (!bb) {
      continue;
    }
    (void)emit_destroy_resource_locked(dev, bb->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (bb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      (void)wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, bb->wddm_hAllocation, dev->wddm_context.hContext);
      bb->wddm_hAllocation = 0;
    }
#endif
    delete bb;
  }

  delete sc;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_swap_chain(
    D3DDDI_HDEVICE hDevice,
    uint32_t index,
    D3D9DDI_HSWAPCHAIN* phSwapChain) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetSwapChain,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(index),
                      d3d9_trace_arg_ptr(phSwapChain),
                      0);
  if (!hDevice.pDrvPrivate || !phSwapChain) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (index >= dev->swapchains.size()) {
    phSwapChain->pDrvPrivate = nullptr;
    return trace.ret(E_INVALIDARG);
  }
  phSwapChain->pDrvPrivate = dev->swapchains[index];
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_swap_chain(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HSWAPCHAIN hSwapChain) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetSwapChain,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hSwapChain.pDrvPrivate),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  auto* sc = as_swapchain(hSwapChain);

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (sc) {
    auto it = std::find(dev->swapchains.begin(), dev->swapchains.end(), sc);
    if (it == dev->swapchains.end()) {
      return trace.ret(E_INVALIDARG);
    }
  }
  dev->current_swapchain = sc;
  return trace.ret(S_OK);
}

HRESULT reset_swap_chain_locked(Device* dev, SwapChain* sc, const D3D9DDI_PRESENT_PARAMETERS& pp) {
  if (!dev || !dev->adapter || !sc) {
    return E_INVALIDARG;
  }

  // Reset/backbuffer recreation destroys WDDM allocation handles. Ensure pending
  // command buffers are flushed first so we don't hand dxgkrnl stale handles in
  // a later submission.
  (void)submit(dev);

  const D3DDDIFORMAT bb_fmt = static_cast<D3DDDIFORMAT>(d3d9_pp_backbuffer_format(pp));
  if (!is_supported_backbuffer_format(bb_fmt)) {
    return E_INVALIDARG;
  }
  if (d3d9_format_to_aerogpu(d3d9_pp_backbuffer_format(pp)) == AEROGPU_FORMAT_INVALID) {
    return E_INVALIDARG;
  }

  const uint32_t new_width = d3d9_pp_backbuffer_width(pp) ? d3d9_pp_backbuffer_width(pp) : sc->width;
  const uint32_t new_height = d3d9_pp_backbuffer_height(pp) ? d3d9_pp_backbuffer_height(pp) : sc->height;
  const uint32_t new_count = std::max(1u, d3d9_pp_backbuffer_count(pp));

  sc->hwnd = pp.hDeviceWindow ? pp.hDeviceWindow : sc->hwnd;
  sc->width = new_width;
  sc->height = new_height;
  sc->format = d3d9_pp_backbuffer_format(pp);
  sc->sync_interval = d3d9_pp_presentation_interval(pp);
  sc->swap_effect = d3d9_pp_swap_effect(pp);
  sc->flags = d3d9_pp_flags(pp);

  // Reset destroys/recreates backbuffers. Flush any queued commands first so we
  // don't destroy allocations still referenced by an unsubmitted command buffer.
  (void)submit(dev);

  while (sc->backbuffers.size() > new_count) {
    Resource* bb = sc->backbuffers.back();
    sc->backbuffers.pop_back();
    if (!bb) {
      continue;
    }

    bool rt_changed = false;
    for (uint32_t i = 0; i < 4; ++i) {
      if (dev->render_targets[i] == bb) {
        dev->render_targets[i] = nullptr;
        rt_changed = true;
      }
    }
    if (dev->depth_stencil == bb) {
      dev->depth_stencil = nullptr;
      rt_changed = true;
    }

    for (uint32_t stage = 0; stage < 16; ++stage) {
      if (dev->textures[stage] != bb) {
        continue;
      }
      dev->textures[stage] = nullptr;
      if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE)) {
        cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
        cmd->slot = stage;
        cmd->texture = 0;
        cmd->reserved0 = 0;
      }
    }

    for (uint32_t stream = 0; stream < 16; ++stream) {
      if (dev->streams[stream].vb != bb) {
        continue;
      }
      dev->streams[stream] = {};

      aerogpu_vertex_buffer_binding binding{};
      binding.buffer = 0;
      binding.stride_bytes = 0;
      binding.offset_bytes = 0;
      binding.reserved0 = 0;

      if (auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
              dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding))) {
        cmd->start_slot = stream;
        cmd->buffer_count = 1;
      }
    }

    if (dev->index_buffer == bb) {
      dev->index_buffer = nullptr;
      dev->index_offset_bytes = 0;
      dev->index_format = kD3dFmtIndex16;

      if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER)) {
        cmd->buffer = 0;
        cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
        cmd->offset_bytes = 0;
        cmd->reserved0 = 0;
      }
    }

    if (rt_changed) {
      (void)emit_set_render_targets_locked(dev);
    }

    (void)emit_destroy_resource_locked(dev, bb->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (bb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      (void)wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, bb->wddm_hAllocation, dev->wddm_context.hContext);
      bb->wddm_hAllocation = 0;
    }
#endif
    delete bb;
  }

  // Ensure backbuffer vector growth does not throw later in the reset path.
  if (sc->backbuffers.capacity() < new_count) {
    try {
      sc->backbuffers.reserve(new_count);
    } catch (...) {
      return E_OUTOFMEMORY;
    }
  }
  while (sc->backbuffers.size() < new_count) {
    auto bb = make_unique_nothrow<Resource>();
    if (!bb) {
      return E_OUTOFMEMORY;
    }
    HRESULT hr = create_backbuffer_locked(dev, bb.get(), sc->format, sc->width, sc->height);
    if (hr < 0) {
      return hr;
    }
    Resource* bb_raw = bb.get();
    try {
      sc->backbuffers.push_back(bb_raw);
    } catch (...) {
      // Backbuffer vector was pre-reserved; this should not happen, but keep driver cleanup safe.
      (void)emit_destroy_resource_locked(dev, bb_raw->handle);
      (void)submit(dev);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (bb_raw->wddm_hAllocation != 0 && dev->wddm_device != 0) {
        (void)wddm_destroy_allocation(dev->wddm_callbacks,
                                      dev->wddm_device,
                                      bb_raw->wddm_hAllocation,
                                      dev->wddm_context.hContext);
        bb_raw->wddm_hAllocation = 0;
      }
#endif
      return E_OUTOFMEMORY;
    }
    bb.release();
  }

  // Recreate backbuffer storage/handles.
  for (Resource* bb : sc->backbuffers) {
    if (!bb) {
      continue;
    }
    (void)emit_destroy_resource_locked(dev, bb->handle);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (bb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      (void)wddm_destroy_allocation(dev->wddm_callbacks, dev->wddm_device, bb->wddm_hAllocation, dev->wddm_context.hContext);
      bb->wddm_hAllocation = 0;
    }
#endif
    HRESULT hr = create_backbuffer_locked(dev, bb, sc->format, sc->width, sc->height);
    if (hr < 0) {
      return hr;
    }
  }

  auto is_backbuffer = [sc](const Resource* res) -> bool {
    if (!sc || !res) {
      return false;
    }
    return std::find(sc->backbuffers.begin(), sc->backbuffers.end(), res) != sc->backbuffers.end();
  };

  // Reset recreates swapchain backbuffer handles. If any of the backbuffers are
  // currently bound via other state (textures / IA bindings), re-emit the bind
  // commands so the host uses the updated handles.
  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (!is_backbuffer(dev->textures[stage])) {
      continue;
    }
    if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE)) {
      cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
      cmd->slot = stage;
      cmd->texture = dev->textures[stage] ? dev->textures[stage]->handle : 0;
      cmd->reserved0 = 0;
    }
  }

  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (!is_backbuffer(dev->streams[stream].vb)) {
      continue;
    }

    aerogpu_vertex_buffer_binding binding{};
    binding.buffer = dev->streams[stream].vb ? dev->streams[stream].vb->handle : 0;
    binding.stride_bytes = dev->streams[stream].stride_bytes;
    binding.offset_bytes = dev->streams[stream].offset_bytes;
    binding.reserved0 = 0;

    if (auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
            dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding))) {
      cmd->start_slot = stream;
      cmd->buffer_count = 1;
    }
  }

  if (is_backbuffer(dev->index_buffer)) {
    if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER)) {
      cmd->buffer = dev->index_buffer ? dev->index_buffer->handle : 0;
      cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
      cmd->offset_bytes = dev->index_offset_bytes;
      cmd->reserved0 = 0;
    }
  }

  if (!dev->render_targets[0] && !sc->backbuffers.empty()) {
    dev->render_targets[0] = sc->backbuffers[0];
  }
  if (!emit_set_render_targets_locked(dev)) {
    return E_OUTOFMEMORY;
  }
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_reset(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_RESET* pReset) {
  const D3D9DDI_PRESENT_PARAMETERS* trace_pp = pReset ? d3d9_get_present_params(*pReset) : nullptr;
  const uint64_t bb_wh =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_width(*trace_pp), d3d9_pp_backbuffer_height(*trace_pp)) : 0;
  const uint64_t fmt_count =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_format(*trace_pp), d3d9_pp_backbuffer_count(*trace_pp)) : 0;
  const uint64_t interval_flags =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_presentation_interval(*trace_pp), d3d9_pp_flags(*trace_pp)) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceReset, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), bb_wh, fmt_count, interval_flags);
  if (!hDevice.pDrvPrivate || !pReset) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  // Reset implies a new frame queue; drop any in-flight present fences so
  // max-frame-latency throttling doesn't block the first presents after a reset.
  dev->inflight_present_fences.clear();
  SwapChain* sc = dev->current_swapchain;
  if (!sc && !dev->swapchains.empty()) {
    sc = dev->swapchains[0];
  }
  if (!sc) {
    return trace.ret(S_OK);
  }

  const D3D9DDI_PRESENT_PARAMETERS* pp = d3d9_get_present_params(*pReset);
  if (!pp) {
    return trace.ret(E_INVALIDARG);
  }
  return trace.ret(reset_swap_chain_locked(dev, sc, *pp));
}

HRESULT AEROGPU_D3D9_CALL device_reset_ex(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_RESET* pReset) {
  const D3D9DDI_PRESENT_PARAMETERS* trace_pp = pReset ? d3d9_get_present_params(*pReset) : nullptr;
  const uint64_t bb_wh =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_width(*trace_pp), d3d9_pp_backbuffer_height(*trace_pp)) : 0;
  const uint64_t fmt_count =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_backbuffer_format(*trace_pp), d3d9_pp_backbuffer_count(*trace_pp)) : 0;
  const uint64_t interval_flags =
      trace_pp ? d3d9_trace_pack_u32_u32(d3d9_pp_presentation_interval(*trace_pp), d3d9_pp_flags(*trace_pp)) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceResetEx, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), bb_wh, fmt_count, interval_flags);
  return trace.ret(device_reset(hDevice, pReset));
}

HRESULT AEROGPU_D3D9_CALL device_check_device_state(
    D3DDDI_HDEVICE hDevice,
    HWND hWnd) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCheckDeviceState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hWnd),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  if (hwnd_is_occluded(hWnd)) {
    return trace.ret(kSPresentOccluded);
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_rotate_resource_identities(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE* pResources,
    uint32_t resource_count) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceRotateResourceIdentities,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(resource_count),
                      d3d9_trace_arg_ptr(pResources),
                      0);
  if (!hDevice.pDrvPrivate || !pResources || resource_count < 2) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  std::vector<Resource*> resources;
  try {
    resources.reserve(resource_count);
  } catch (...) {
    return trace.ret(E_OUTOFMEMORY);
  }
  for (uint32_t i = 0; i < resource_count; ++i) {
    auto* res = as_resource(pResources[i]);
    if (!res) {
      return trace.ret(E_INVALIDARG);
    }
    if (std::find(resources.begin(), resources.end(), res) != resources.end()) {
      // Reject duplicates: D3D9 expects a set of distinct resources.
      return trace.ret(E_INVALIDARG);
    }
    resources.push_back(res);
  }

  auto matches_desc = [&resources](const Resource* res) -> bool {
    const Resource* base = resources.empty() ? nullptr : resources[0];
    if (!base || !res) {
      return false;
    }
    return res->kind == base->kind &&
           res->type == base->type &&
           res->format == base->format &&
           res->width == base->width &&
           res->height == base->height &&
           res->depth == base->depth &&
           res->mip_levels == base->mip_levels &&
           res->usage == base->usage &&
           res->pool == base->pool &&
           res->size_bytes == base->size_bytes &&
           res->row_pitch == base->row_pitch &&
           res->slice_pitch == base->slice_pitch;
  };

  for (Resource* res : resources) {
    if (!matches_desc(res)) {
      return trace.ret(kD3DErrInvalidCall);
    }
    if (res->locked) {
      return trace.ret(kD3DErrInvalidCall);
    }
    // Shared resources have stable identities (`share_token`); rotating them is
    // likely to break EXPORT/IMPORT semantics across processes.
    if (res->is_shared || res->is_shared_alias || res->share_token != 0) {
      return trace.ret(kD3DErrInvalidCall);
    }
  }

  auto is_rotated = [&resources](const Resource* res) -> bool {
    if (!res) {
      return false;
    }
    return std::find(resources.begin(), resources.end(), res) != resources.end();
  };

  // Rotating resource identities swaps the host handles/backing allocations
  // attached to the affected Resource objects. If any of those resources are
  // currently bound via device state, we must re-emit the corresponding binds
  // using the *new* handles so the host does not keep referencing the old
  // handles.
  size_t needed_bytes = align_up(sizeof(aerogpu_cmd_set_render_targets), 4);
  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (is_rotated(dev->textures[stage])) {
      needed_bytes += align_up(sizeof(aerogpu_cmd_set_texture), 4);
    }
  }
  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (is_rotated(dev->streams[stream].vb)) {
      needed_bytes += align_up(sizeof(aerogpu_cmd_set_vertex_buffers) + sizeof(aerogpu_vertex_buffer_binding), 4);
    }
  }
  if (is_rotated(dev->index_buffer)) {
    needed_bytes += align_up(sizeof(aerogpu_cmd_set_index_buffer), 4);
  }

  // Ensure the DMA buffer has enough space for all rebinding packets before we
  // rotate identities and track allocations; tracking may force a submission
  // split, and command-buffer splits must not occur after tracking or the
  // allocation list would be out of sync.
  if (!ensure_cmd_space(dev, needed_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  struct ResourceIdentity {
    aerogpu_handle_t handle = 0;
    uint32_t backing_alloc_id = 0;
    uint32_t backing_offset_bytes = 0;
    uint64_t share_token = 0;
    bool is_shared = false;
    bool is_shared_alias = false;
    bool locked = false;
    uint32_t locked_offset = 0;
    uint32_t locked_size = 0;
    uint32_t locked_flags = 0;
    WddmAllocationHandle wddm_hAllocation = 0;
    std::vector<uint8_t> storage;
    std::vector<uint8_t> shared_private_driver_data;
  };

  auto take_identity = [](Resource* res) -> ResourceIdentity {
    ResourceIdentity id{};
    id.handle = res->handle;
    id.backing_alloc_id = res->backing_alloc_id;
    id.backing_offset_bytes = res->backing_offset_bytes;
    id.share_token = res->share_token;
    id.is_shared = res->is_shared;
    id.is_shared_alias = res->is_shared_alias;
    id.locked = res->locked;
    id.locked_offset = res->locked_offset;
    id.locked_size = res->locked_size;
    id.locked_flags = res->locked_flags;
    id.wddm_hAllocation = res->wddm_hAllocation;
    id.storage = std::move(res->storage);
    id.shared_private_driver_data = std::move(res->shared_private_driver_data);
    return id;
  };

  auto put_identity = [](Resource* res, ResourceIdentity&& id) {
    res->handle = id.handle;
    res->backing_alloc_id = id.backing_alloc_id;
    res->backing_offset_bytes = id.backing_offset_bytes;
    res->share_token = id.share_token;
    res->is_shared = id.is_shared;
    res->is_shared_alias = id.is_shared_alias;
    res->locked = id.locked;
    res->locked_offset = id.locked_offset;
    res->locked_size = id.locked_size;
    res->locked_flags = id.locked_flags;
    res->wddm_hAllocation = id.wddm_hAllocation;
    res->storage = std::move(id.storage);
    res->shared_private_driver_data = std::move(id.shared_private_driver_data);
  };

  auto undo_rotation = [&resources, resource_count, &take_identity, &put_identity]() {
    // Undo the rotation (rotate right by one).
    ResourceIdentity undo_saved = take_identity(resources[resource_count - 1]);
    for (uint32_t i = resource_count - 1; i > 0; --i) {
      put_identity(resources[i], take_identity(resources[i - 1]));
    }
    put_identity(resources[0], std::move(undo_saved));
  };

  // Perform the identity rotation (rotate left by one).
  ResourceIdentity saved = take_identity(resources[0]);
  for (uint32_t i = 0; i + 1 < resource_count; ++i) {
    put_identity(resources[i], take_identity(resources[i + 1]));
  }
  put_identity(resources[resource_count - 1], std::move(saved));

  if (dev->wddm_context.hContext != 0 &&
      dev->alloc_list_tracker.list_base() != nullptr &&
      dev->alloc_list_tracker.list_capacity_effective() != 0) {
    // The rebinding packets reference multiple resources. `track_resource_allocation_locked`
    // can internally split the submission (submit+retry) when the allocation list
    // is full. If that happens mid-sequence, earlier tracked allocations would be
    // dropped and the submission would be missing required alloc-table entries.
    //
    // Pre-scan all allocations referenced by the rebinding commands and split once
    // up front when the remaining allocation-list capacity is insufficient.
    std::array<UINT, 4 + 1 + 16 + 16 + 1> unique_allocs{};
    size_t unique_alloc_len = 0;
    auto add_alloc = [&unique_allocs, &unique_alloc_len](const Resource* res) {
      if (!res) {
        return;
      }
      if (res->backing_alloc_id == 0) {
        return;
      }
      if (res->wddm_hAllocation == 0) {
        return;
      }
      const UINT alloc_id = res->backing_alloc_id;
      for (size_t i = 0; i < unique_alloc_len; ++i) {
        if (unique_allocs[i] == alloc_id) {
          return;
        }
      }
      unique_allocs[unique_alloc_len++] = alloc_id;
    };

    for (uint32_t i = 0; i < 4; ++i) {
      add_alloc(dev->render_targets[i]);
    }
    add_alloc(dev->depth_stencil);
    for (uint32_t stage = 0; stage < 16; ++stage) {
      if (is_rotated(dev->textures[stage])) {
        add_alloc(dev->textures[stage]);
      }
    }
    for (uint32_t stream = 0; stream < 16; ++stream) {
      if (is_rotated(dev->streams[stream].vb)) {
        add_alloc(dev->streams[stream].vb);
      }
    }
    if (is_rotated(dev->index_buffer)) {
      add_alloc(dev->index_buffer);
    }

    const UINT needed_total = static_cast<UINT>(unique_alloc_len);
    if (needed_total != 0) {
      const UINT cap = dev->alloc_list_tracker.list_capacity_effective();
      if (needed_total > cap) {
        logf("aerogpu-d3d9: rotate identities requires %u allocations but allocation list capacity is %u\n",
             static_cast<unsigned>(needed_total),
             static_cast<unsigned>(cap));
        undo_rotation();
        return trace.ret(E_FAIL);
      }

      UINT needed_new = 0;
      for (size_t i = 0; i < unique_alloc_len; ++i) {
        if (!dev->alloc_list_tracker.contains_alloc_id(unique_allocs[i])) {
          needed_new++;
        }
      }
      const UINT existing = dev->alloc_list_tracker.list_len();
      if (existing > cap || needed_new > cap - existing) {
        (void)submit(dev);
      }
    }

    // If the allocation-list pre-scan split the submission, re-check command space
    // so we don't end up splitting the command buffer after allocation tracking.
    if (!ensure_cmd_space(dev, needed_bytes)) {
      undo_rotation();
      return trace.ret(E_OUTOFMEMORY);
    }
  }

  // Track allocations referenced by the rebinding commands so the KMD/emulator
  // can resolve alloc_id -> GPA even if the submission contains only state
  // updates (no draw).
  HRESULT hr = track_render_targets_locked(dev);
  if (FAILED(hr)) {
    undo_rotation();
    return trace.ret(hr);
  }
  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (!is_rotated(dev->textures[stage])) {
      continue;
    }
    const HRESULT track_hr = track_resource_allocation_locked(dev, dev->textures[stage], /*write=*/false);
    if (FAILED(track_hr)) {
      undo_rotation();
      return trace.ret(track_hr);
    }
  }
  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (!is_rotated(dev->streams[stream].vb)) {
      continue;
    }
    const HRESULT track_hr = track_resource_allocation_locked(dev, dev->streams[stream].vb, /*write=*/false);
    if (FAILED(track_hr)) {
      undo_rotation();
      return trace.ret(track_hr);
    }
  }
  if (is_rotated(dev->index_buffer)) {
    const HRESULT track_hr = track_resource_allocation_locked(dev, dev->index_buffer, /*write=*/false);
    if (FAILED(track_hr)) {
      undo_rotation();
      return trace.ret(track_hr);
    }
  }

  // Re-emit binds so the host observes the updated handles.
  bool ok = emit_set_render_targets_locked(dev);
  for (uint32_t stage = 0; ok && stage < 16; ++stage) {
    if (!is_rotated(dev->textures[stage])) {
      continue;
    }
    auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE);
    if (!cmd) {
      ok = false;
      break;
    }
    cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
    cmd->slot = stage;
    cmd->texture = dev->textures[stage] ? dev->textures[stage]->handle : 0;
    cmd->reserved0 = 0;
  }

  for (uint32_t stream = 0; ok && stream < 16; ++stream) {
    if (!is_rotated(dev->streams[stream].vb)) {
      continue;
    }

    aerogpu_vertex_buffer_binding binding{};
    binding.buffer = dev->streams[stream].vb ? dev->streams[stream].vb->handle : 0;
    binding.stride_bytes = dev->streams[stream].stride_bytes;
    binding.offset_bytes = dev->streams[stream].offset_bytes;
    binding.reserved0 = 0;

    auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
        dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding));
    if (!cmd) {
      ok = false;
      break;
    }
    cmd->start_slot = stream;
    cmd->buffer_count = 1;
  }

  if (ok && is_rotated(dev->index_buffer)) {
    auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (!cmd) {
      ok = false;
    } else {
      cmd->buffer = dev->index_buffer ? dev->index_buffer->handle : 0;
      cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
      cmd->offset_bytes = dev->index_offset_bytes;
      cmd->reserved0 = 0;
    }
  }

  if (!ok) {
    // Preserve device/host state consistency: if we cannot emit the rebinding
    // commands (command buffer too small), undo the rotation so future draws
    // still target the host's current bindings.
    undo_rotation();
    return trace.ret(E_OUTOFMEMORY);
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_lock(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_LOCK* pLock,
    D3DDDI_LOCKEDBOX* pLockedBox) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceLock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pLock ? d3d9_trace_arg_ptr(pLock->hResource.pDrvPrivate) : 0,
                      pLock ? d3d9_trace_pack_u32_u32(d3d9_lock_offset(*pLock), d3d9_lock_size(*pLock)) : 0,
                       pLock ? static_cast<uint64_t>(d3d9_lock_flags(*pLock)) : 0);
  if (!hDevice.pDrvPrivate || !pLock || !pLockedBox) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* res = as_resource(pLock->hResource);
  if (!dev || !res) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  if (res->locked) {
    return trace.ret(E_FAIL);
  }

  const uint32_t offset = d3d9_lock_offset(*pLock);
  const uint32_t requested_size = d3d9_lock_size(*pLock);
  if (offset > res->size_bytes) {
    return trace.ret(E_INVALIDARG);
  }

  Texture2dSubresourceLayout sub{};
  const bool have_subresource_layout =
      (res->kind != ResourceKind::Buffer) &&
      (res->mip_levels > 1 || res->depth > 1) &&
      calc_texture2d_subresource_layout_for_offset(res->format,
                                                   res->width,
                                                   res->height,
                                                   res->mip_levels,
                                                   res->depth,
                                                   offset,
                                                   &sub);

  uint32_t size = requested_size;
  if (size == 0) {
    if (res->kind == ResourceKind::Buffer) {
      size = res->size_bytes - offset;
    } else if (have_subresource_layout && sub.subresource_end_bytes >= offset) {
      const uint64_t end = std::min<uint64_t>(sub.subresource_end_bytes, res->size_bytes);
      const uint64_t span = end - offset;
      if (span > 0xFFFFFFFFull) {
        return trace.ret(E_INVALIDARG);
      }
      size = static_cast<uint32_t>(span);
    } else {
      // Legacy fallback: lock the remaining bytes. This is correct for
      // single-subresource surfaces/textures where offset==0.
      size = res->size_bytes - offset;
    }
  }

  if (size == 0 || size > res->size_bytes - offset) {
    return trace.ret(E_INVALIDARG);
  }

  uint32_t row_pitch = res->row_pitch;
  uint32_t slice_pitch = res->slice_pitch;
  if (have_subresource_layout) {
    row_pitch = sub.row_pitch_bytes;
    slice_pitch = sub.slice_pitch_bytes;
  }

  res->locked = true;
  res->locked_offset = offset;
  res->locked_size = size;
  res->locked_flags = d3d9_lock_flags(*pLock);
  res->locked_ptr = nullptr;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (res->backing_alloc_id != 0 && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
    void* ptr = nullptr;
    const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                           dev->wddm_device,
                                           res->wddm_hAllocation,
                                           offset,
                                           size,
                                           res->locked_flags,
                                           &ptr,
                                           dev->wddm_context.hContext);
    if (FAILED(hr) || !ptr) {
      res->locked = false;
      res->locked_flags = 0;
      return trace.ret(FAILED(hr) ? hr : E_FAIL);
    }
    res->locked_ptr = ptr;
    d3d9_locked_box_set_ptr(pLockedBox, ptr);
  } else
#endif
  {
    if (res->storage.size() < res->size_bytes) {
      res->locked = false;
      res->locked_flags = 0;
      return trace.ret(E_FAIL);
    }
    res->locked_ptr = res->storage.data() + offset;
    d3d9_locked_box_set_ptr(pLockedBox, res->locked_ptr);
  }

  d3d9_locked_box_set_row_pitch(pLockedBox, row_pitch);
  d3d9_locked_box_set_slice_pitch(pLockedBox, slice_pitch);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_unlock(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_UNLOCK* pUnlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceUnlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pUnlock ? d3d9_trace_arg_ptr(pUnlock->hResource.pDrvPrivate) : 0,
                      pUnlock ? d3d9_trace_pack_u32_u32(d3d9_unlock_offset(*pUnlock), d3d9_unlock_size(*pUnlock)) : 0,
                      0);
  if (!hDevice.pDrvPrivate || !pUnlock) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* res = as_resource(pUnlock->hResource);
  if (!dev || !res) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  if (!res->locked) {
    return trace.ret(E_FAIL);
  }

  const uint32_t unlock_offset = d3d9_unlock_offset(*pUnlock);
  const uint32_t unlock_size = d3d9_unlock_size(*pUnlock);
  const uint32_t offset = unlock_offset ? unlock_offset : res->locked_offset;
  const uint32_t size = unlock_size ? unlock_size : res->locked_size;
  if (offset > res->size_bytes || size > res->size_bytes - offset) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t locked_flags = res->locked_flags;
  void* locked_ptr = res->locked_ptr;
  const uint32_t locked_offset = res->locked_offset;
  const uint32_t locked_size = res->locked_size;

  res->locked = false;
  res->locked_ptr = nullptr;
  res->locked_flags = 0;

  // D3DFMT_X1R5G5B5: treat alpha as 1. If the app wrote texels with the MSB
  // cleared, fix them up in-place before the host observes the backing bytes.
  if ((locked_flags & kD3DLOCK_READONLY) == 0 && size != 0 && locked_ptr != nullptr &&
      static_cast<uint32_t>(res->format) == 24u /*D3DFMT_X1R5G5B5*/) {
    force_x1r5g5b5_alpha1_locked_range(locked_ptr, locked_offset, locked_size, offset, size);
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (res->backing_alloc_id != 0 && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
    const HRESULT hr =
        wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(hr)) {
      logf("aerogpu-d3d9: UnlockCb failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
           static_cast<unsigned long>(hr),
           static_cast<unsigned>(res->backing_alloc_id),
           static_cast<unsigned long long>(res->wddm_hAllocation));
      return trace.ret(hr);
    }
  }
#endif

  // CPU writes into allocation-backed resources are observed by the host via the
  // guest physical memory. Notify the host that the backing bytes changed so it
  // can re-upload on demand.
  if (res->handle != 0 && res->backing_alloc_id != 0 && (locked_flags & kD3DLOCK_READONLY) == 0 && size) {
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
      return trace.ret(E_OUTOFMEMORY);
    }

    const HRESULT hr = track_resource_allocation_locked(dev, res, /*write=*/false);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
    if (!cmd) {
      return trace.ret(E_OUTOFMEMORY);
    }
    cmd->resource_handle = res->handle;
    cmd->reserved0 = 0;
    cmd->offset_bytes = static_cast<uint64_t>(offset);
    cmd->size_bytes = static_cast<uint64_t>(size);
    return trace.ret(S_OK);
  }

  // Fallback: host-allocated resources are updated by embedding raw bytes in the
  // command stream.
  if (res->handle != 0 && (locked_flags & kD3DLOCK_READONLY) == 0 && size) {
    const HRESULT upload_hr = emit_upload_resource_range_locked(dev, res, offset, size);
    if (FAILED(upload_hr)) {
      return trace.ret(upload_hr);
    }
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_process_vertices(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_PROCESSVERTICES* pProcessVertices) {
  uint64_t arg0 = d3d9_trace_arg_ptr(hDevice.pDrvPrivate);
  uint64_t arg1 = pProcessVertices ? d3d9_trace_arg_ptr(pProcessVertices->hDestBuffer.pDrvPrivate) : 0;
  uint64_t arg2 = 0;
  uint64_t arg3 = 0;
  if (pProcessVertices) {
    arg2 = d3d9_trace_pack_u32_u32(pProcessVertices->SrcStartIndex, pProcessVertices->DestIndex);
    uint32_t dest_stride = 0;
    if constexpr (aerogpu_d3d9_has_member_DestStride<D3DDDIARG_PROCESSVERTICES>::value) {
      dest_stride = pProcessVertices->DestStride;
    }
    arg3 = d3d9_trace_pack_u32_u32(pProcessVertices->VertexCount, dest_stride);
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceProcessVertices, arg0, arg1, arg2, arg3);
  if (!hDevice.pDrvPrivate || !pProcessVertices) {
    return trace.ret(E_INVALIDARG);
  }

  // Fixed-function fallback: attempt a minimal CPU transform for common FVF paths.
  // If not handled, fall back to a memcpy-style implementation below.
  const HRESULT fixedfunc_hr = device_process_vertices_internal(hDevice, pProcessVertices);
  if (fixedfunc_hr != D3DERR_NOTAVAILABLE) {
    return trace.ret(fixedfunc_hr);
  }

  auto* dev = as_device(hDevice);
  auto* dst_res = as_resource(pProcessVertices->hDestBuffer);
  if (!dev || !dst_res) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  const uint32_t vertex_count = pProcessVertices->VertexCount;
  if (vertex_count == 0) {
    return trace.ret(S_OK);
  }

  const DeviceStateStream& stream0 = dev->streams[0];
  Resource* src_res = stream0.vb;
  if (!src_res) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (src_res->kind != ResourceKind::Buffer || dst_res->kind != ResourceKind::Buffer) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const uint32_t src_stride = stream0.stride_bytes;
  if (src_stride == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }

  // D3DPV_* flags passed through from IDirect3DDevice9::ProcessVertices.
  constexpr uint32_t kD3dPvDoNotCopyData = 0x1u;
  const bool do_not_copy_data = (pProcessVertices->Flags & kD3dPvDoNotCopyData) != 0;

  uint32_t dst_stride = 0;
  if constexpr (aerogpu_d3d9_has_member_DestStride<D3DDDIARG_PROCESSVERTICES>::value) {
    dst_stride = pProcessVertices->DestStride;
  }
  if (dst_stride == 0) {
    // Some runtimes may pass DestStride=0 and expect the driver to infer it from
    // the destination vertex declaration (matching the fixed-function bring-up
    // path and the D3D9 API behavior).
    VertexDecl* dst_decl = as_vertex_decl(pProcessVertices->hVertexDecl);
    ProcessVerticesDeclInfo layout{};
    if (dst_decl && parse_process_vertices_dest_decl(dst_decl, &layout)) {
      dst_stride = layout.stride_bytes;
    } else {
      dst_stride = src_stride;
    }
  }
  if (dst_stride == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }

  uint32_t copy_stride = std::min(src_stride, dst_stride);
  // For pre-transformed XYZRHW inputs, position is the first float4 (16 bytes).
  // Honor D3DPV_DONOTCOPYDATA by copying only POSITIONT and leaving the rest of
  // the destination vertex untouched.
  if (do_not_copy_data && (dev->fvf & kD3dFvfXyzRhw) != 0 && copy_stride > 16u) {
    copy_stride = 16u;
  }
  if (copy_stride == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const uint64_t src_start_offset = static_cast<uint64_t>(stream0.offset_bytes) +
                                    static_cast<uint64_t>(pProcessVertices->SrcStartIndex) * static_cast<uint64_t>(src_stride);
  const uint64_t dst_start_offset =
      static_cast<uint64_t>(pProcessVertices->DestIndex) * static_cast<uint64_t>(dst_stride);

  // Total touched ranges are strided, but use the bounding range for validation
  // and for DIRTY_RANGE/UPLOAD notifications.
  const uint64_t src_end_offset = src_start_offset +
                                  static_cast<uint64_t>(vertex_count - 1) * static_cast<uint64_t>(src_stride) +
                                  static_cast<uint64_t>(copy_stride);
  const uint64_t dst_end_offset = dst_start_offset +
                                  static_cast<uint64_t>(vertex_count - 1) * static_cast<uint64_t>(dst_stride) +
                                  static_cast<uint64_t>(copy_stride);
  if (src_end_offset < src_start_offset || dst_end_offset < dst_start_offset) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (src_end_offset > static_cast<uint64_t>(src_res->size_bytes) ||
      dst_end_offset > static_cast<uint64_t>(dst_res->size_bytes)) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const bool same_resource = (src_res == dst_res);

  // When ProcessVertices reads and writes the same buffer, the touched byte ranges
  // are strided, not contiguous. A "copy direction" heuristic based only on the
  // starting offsets is insufficient when `src_stride != dst_stride`: destination
  // writes from earlier/later vertices may clobber source bytes that have not yet
  // been read.
  //
  // Match "read all source first, then write all destinations" semantics by
  // staging source bytes into a temporary buffer when the strided regions overlap.
  const auto strided_ranges_overlap = [&](uint64_t a_start,
                                          uint32_t a_stride,
                                          uint64_t b_start,
                                          uint32_t b_stride,
                                          uint32_t chunk_size,
                                          uint32_t count) -> bool {
    if (count == 0 || chunk_size == 0) {
      return false;
    }

    const uint64_t a_stride_u64 = static_cast<uint64_t>(a_stride);
    const uint64_t b_stride_u64 = static_cast<uint64_t>(b_stride);
    const uint64_t chunk_u64 = static_cast<uint64_t>(chunk_size);

    uint32_t i = 0;
    uint32_t j = 0;
    while (i < count && j < count) {
      const uint64_t a0 = a_start + static_cast<uint64_t>(i) * a_stride_u64;
      const uint64_t a1 = a0 + chunk_u64;
      const uint64_t b0 = b_start + static_cast<uint64_t>(j) * b_stride_u64;
      const uint64_t b1 = b0 + chunk_u64;

      if (a0 < b1 && b0 < a1) {
        return true;
      }

      // Advance whichever interval ends first (intervals are sorted by start).
      if (a1 <= b0) {
        ++i;
      } else {
        ++j;
      }
    }
    return false;
  };

  bool needs_staging = false;
  if (same_resource && src_stride != dst_stride) {
    // Fast reject: bounding ranges don't overlap.
    const bool bounding_overlap = (src_start_offset < dst_end_offset) && (dst_start_offset < src_end_offset);
    if (bounding_overlap) {
      needs_staging = strided_ranges_overlap(src_start_offset, src_stride, dst_start_offset, dst_stride, copy_stride, vertex_count);
    }
  }

  const uint8_t* src_base = nullptr;
  uint8_t* dst_base = nullptr;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  struct WddmScopedLock {
    Device* dev = nullptr;
    Resource* res = nullptr;
    void* ptr = nullptr;
    bool locked = false;

    HRESULT lock_allocation(Device* d, Resource* r, uint64_t offset_bytes, uint64_t size_bytes, uint32_t lock_flags) {
      if (!d || !r || size_bytes == 0) {
        return E_INVALIDARG;
      }
      if (r->wddm_hAllocation == 0 || d->wddm_device == 0) {
        return E_FAIL;
      }
      void* out = nullptr;
      const HRESULT hr = wddm_lock_allocation(d->wddm_callbacks,
                                             d->wddm_device,
                                             r->wddm_hAllocation,
                                             offset_bytes,
                                             size_bytes,
                                             lock_flags,
                                             &out,
                                             d->wddm_context.hContext);
      if (FAILED(hr) || !out) {
        return FAILED(hr) ? hr : E_FAIL;
      }
      dev = d;
      res = r;
      ptr = out;
      locked = true;
      return S_OK;
    }

    HRESULT unlock_allocation() {
      if (!locked) {
        return S_OK;
      }
      locked = false;
      return wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
    }

    ~WddmScopedLock() {
      if (locked && dev && res) {
        (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
      }
    }
  };

  WddmScopedLock src_lock{};
  WddmScopedLock dst_lock{};
#endif

  // Map source and destination bytes.
  if (same_resource) {
    if (src_res->storage.size() >= src_res->size_bytes) {
      src_base = src_res->storage.data() + static_cast<size_t>(src_start_offset);
      dst_base = src_res->storage.data() + static_cast<size_t>(dst_start_offset);
    }
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    else if (src_res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      // Avoid locking the same allocation twice. Lock a single union range and
      // derive separate pointers for src/dst.
      const uint64_t lock_start = std::min(src_start_offset, dst_start_offset);
      const uint64_t lock_end = std::max(src_end_offset, dst_end_offset);
      const uint64_t lock_size = lock_end - lock_start;
      const HRESULT hr = src_lock.lock_allocation(dev, src_res, lock_start, lock_size, /*lock_flags=*/0);
      if (FAILED(hr)) {
        return trace.ret(hr);
      }
      uint8_t* base = reinterpret_cast<uint8_t*>(src_lock.ptr);
      src_base = base + static_cast<size_t>(src_start_offset - lock_start);
      dst_base = base + static_cast<size_t>(dst_start_offset - lock_start);
    }
#endif
    else {
      return trace.ret(kD3DErrInvalidCall);
    }
  } else {
    if (src_res->storage.size() >= src_res->size_bytes) {
      src_base = src_res->storage.data() + static_cast<size_t>(src_start_offset);
    }
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    else if (src_res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const uint64_t lock_size = src_end_offset - src_start_offset;
      const HRESULT hr = src_lock.lock_allocation(dev, src_res, src_start_offset, lock_size, kD3DLOCK_READONLY);
      if (FAILED(hr)) {
        return trace.ret(hr);
      }
      src_base = reinterpret_cast<const uint8_t*>(src_lock.ptr);
    }
#endif
    else {
      return trace.ret(kD3DErrInvalidCall);
    }

    if (dst_res->storage.size() >= dst_res->size_bytes) {
      dst_base = dst_res->storage.data() + static_cast<size_t>(dst_start_offset);
    }
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    else if (dst_res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const uint64_t lock_size = dst_end_offset - dst_start_offset;
      const HRESULT hr = dst_lock.lock_allocation(dev, dst_res, dst_start_offset, lock_size, /*lock_flags=*/0);
      if (FAILED(hr)) {
        return trace.ret(hr);
      }
      dst_base = reinterpret_cast<uint8_t*>(dst_lock.ptr);
    }
#endif
    else {
      return trace.ret(kD3DErrInvalidCall);
    }
  }

  if (needs_staging) {
    // Stride conversions can overlap even when the first dst byte is higher than
    // the first src byte (or vice versa). Rather than trying to infer a safe
    // iteration direction, stage the source bytes before writing any destination
    // bytes.
    std::vector<uint8_t> staged;
    const uint64_t staged_size_u64 = static_cast<uint64_t>(vertex_count) * static_cast<uint64_t>(copy_stride);
    if (staged_size_u64 > static_cast<uint64_t>(std::numeric_limits<size_t>::max())) {
      return trace.ret(E_OUTOFMEMORY);
    }
    try {
      staged.resize(static_cast<size_t>(staged_size_u64));
    } catch (...) {
      return trace.ret(E_OUTOFMEMORY);
    }

    for (uint32_t i = 0; i < vertex_count; ++i) {
      const uint8_t* src = src_base + static_cast<size_t>(i) * src_stride;
      std::memcpy(staged.data() + static_cast<size_t>(i) * copy_stride, src, copy_stride);
    }
    for (uint32_t i = 0; i < vertex_count; ++i) {
      uint8_t* dst = dst_base + static_cast<size_t>(i) * dst_stride;
      std::memcpy(dst, staged.data() + static_cast<size_t>(i) * copy_stride, copy_stride);
    }
  } else {
    const auto copy_vertex = [&](uint32_t i) {
      const uint8_t* src = src_base + static_cast<size_t>(i) * src_stride;
      uint8_t* dst = dst_base + static_cast<size_t>(i) * dst_stride;
      std::memmove(dst, src, copy_stride);
    };

    // If source and destination alias the same buffer, copy in a direction that
    // matches `memmove`-style semantics for overlapping ranges.
    if (same_resource && dst_start_offset > src_start_offset) {
      for (uint32_t i = vertex_count; i-- > 0;) {
        copy_vertex(i);
      }
    } else {
      for (uint32_t i = 0; i < vertex_count; ++i) {
        copy_vertex(i);
      }
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  const HRESULT unlock_dst_hr = dst_lock.unlock_allocation();
  const HRESULT unlock_src_hr = src_lock.unlock_allocation();
  if (FAILED(unlock_dst_hr)) {
    return trace.ret(unlock_dst_hr);
  }
  if (FAILED(unlock_src_hr)) {
    return trace.ret(unlock_src_hr);
  }
#endif

  const uint32_t write_offset = static_cast<uint32_t>(dst_start_offset);
  const uint32_t write_size = static_cast<uint32_t>(dst_end_offset - dst_start_offset);

  // If the destination is host-visible (systemmem / CPU-only), there is no host
  // resource to update.
  if (dst_res->handle == 0 || write_size == 0) {
    return trace.ret(S_OK);
  }

  if (dst_res->backing_alloc_id != 0) {
    // Guest-backed resources are updated via CPU-visible guest memory; notify
    // the host that a range changed (like Unlock).
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
      return trace.ret(E_OUTOFMEMORY);
    }
    const HRESULT track_hr = track_resource_allocation_locked(dev, dst_res, /*write=*/false);
    if (FAILED(track_hr)) {
      return trace.ret(track_hr);
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
    if (!cmd) {
      return trace.ret(E_OUTOFMEMORY);
    }
    cmd->resource_handle = dst_res->handle;
    cmd->reserved0 = 0;
    cmd->offset_bytes = static_cast<uint64_t>(write_offset);
    cmd->size_bytes = static_cast<uint64_t>(write_size);
    return trace.ret(S_OK);
  }

  // Host-allocated resources are updated by embedding raw bytes in the command
  // stream (like Unlock).
  if (dst_res->storage.size() < dst_res->size_bytes) {
    return trace.ret(E_FAIL);
  }

  uint32_t upload_offset = write_offset;
  uint32_t upload_size = write_size;
  const uint32_t start = upload_offset & ~3u;
  const uint64_t end_u64 = static_cast<uint64_t>(upload_offset) + static_cast<uint64_t>(upload_size);
  const uint32_t end = static_cast<uint32_t>((end_u64 + 3ull) & ~3ull);
  if (end > dst_res->size_bytes || end < start) {
    return trace.ret(E_INVALIDARG);
  }
  upload_offset = start;
  upload_size = end - start;

  const uint8_t* upload_src = dst_res->storage.data() + upload_offset;
  uint32_t remaining = upload_size;
  uint32_t cur_offset = upload_offset;

  while (remaining) {
    const size_t min_payload = 4; // buffer upload requires 4-byte alignment
    const size_t min_needed = align_up(sizeof(aerogpu_cmd_upload_resource) + min_payload, 4);
    if (!ensure_cmd_space(dev, min_needed)) {
      return trace.ret(E_OUTOFMEMORY);
    }

    HRESULT track_hr = track_resource_allocation_locked(dev, dst_res, /*write=*/true);
    if (FAILED(track_hr)) {
      return trace.ret(track_hr);
    }

    if (!ensure_cmd_space(dev, min_needed)) {
      return trace.ret(E_OUTOFMEMORY);
    }

    const size_t avail = dev->cmd.bytes_remaining();
    size_t chunk = 0;
    if (avail > sizeof(aerogpu_cmd_upload_resource)) {
      chunk = std::min<size_t>(remaining, avail - sizeof(aerogpu_cmd_upload_resource));
    }

    chunk &= ~static_cast<size_t>(3);
    if (!chunk) {
      submit(dev);
      continue;
    }

    auto* cmd = append_with_payload_locked<aerogpu_cmd_upload_resource>(
        dev, AEROGPU_CMD_UPLOAD_RESOURCE, upload_src, chunk);
    if (!cmd) {
      return trace.ret(E_OUTOFMEMORY);
    }

    cmd->resource_handle = dst_res->handle;
    cmd->reserved0 = 0;
    cmd->offset_bytes = cur_offset;
    cmd->size_bytes = chunk;

    upload_src += chunk;
    cur_offset += static_cast<uint32_t>(chunk);
    remaining -= static_cast<uint32_t>(chunk);
  }

  return trace.ret(S_OK);
}

static bool SupportsTransfer(const Device* dev) {
  if (!dev || !dev->adapter || !dev->adapter->umd_private_valid) {
    return false;
  }
  const aerogpu_umd_private_v1& blob = dev->adapter->umd_private;
  if ((blob.device_features & AEROGPU_UMDPRIV_FEATURE_TRANSFER) == 0) {
    return false;
  }
  const uint32_t major = blob.device_abi_version_u32 >> 16;
  const uint32_t minor = blob.device_abi_version_u32 & 0xFFFFu;
  return (major == AEROGPU_ABI_MAJOR) && (minor >= 1);
}

HRESULT AEROGPU_D3D9_CALL device_get_render_target_data(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_GETRENDERTARGETDATA* pGetRenderTargetData) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetRenderTargetData,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pGetRenderTargetData ? d3d9_trace_arg_ptr(pGetRenderTargetData->hSrcResource.pDrvPrivate) : 0,
                      pGetRenderTargetData ? d3d9_trace_arg_ptr(pGetRenderTargetData->hDstResource.pDrvPrivate) : 0,
                      0);
  if (!hDevice.pDrvPrivate || !pGetRenderTargetData) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* src = as_resource(pGetRenderTargetData->hSrcResource);
  auto* dst = as_resource(pGetRenderTargetData->hDstResource);
  if (!dev || !src || !dst) {
    return trace.ret(E_INVALIDARG);
  }

  // GetRenderTargetData copies from a GPU render target/backbuffer into a
  // system-memory surface.
  if (dst->pool != kD3DPOOL_SYSTEMMEM) {
    return trace.ret(E_INVALIDARG);
  }
  if (dst->locked) {
    return trace.ret(E_FAIL);
  }

  if (src->width != dst->width || src->height != dst->height || src->format != dst->format) {
    return trace.ret(kD3DErrInvalidCall);
  }
  const uint32_t bpp = bytes_per_pixel(src->format);
  if (bpp != 2 && bpp != 4) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const bool transfer_supported = SupportsTransfer(dev);

  if (!transfer_supported) {
    // Fallback: when the device does not advertise transfer/copy support, avoid
    // emitting COPY_TEXTURE2D. Instead, submit any pending GPU work and copy via
    // CPU-visible storage/allocation mappings.
    uint64_t fence = 0;
    {
      std::lock_guard<std::mutex> lock(dev->mutex);
      fence = submit(dev);
    }

    const FenceWaitResult wait_res = wait_for_fence(dev, fence, /*timeout_ms=*/2000);
    if (wait_res == FenceWaitResult::Failed) {
      return trace.ret(E_FAIL);
    }
    if (wait_res == FenceWaitResult::NotReady) {
      return trace.ret(kD3dErrWasStillDrawing);
    }

    const HRESULT hr = copy_surface_rects(dev, src, dst, /*rects=*/nullptr, /*rect_count=*/0);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }

    // If the destination is allocation-backed, the host only observes CPU writes
    // when we mark the allocation dirty.
    if (dst->handle != 0 && dst->size_bytes) {
      std::lock_guard<std::mutex> lock(dev->mutex);

      if (dst->backing_alloc_id != 0) {
        if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
          return trace.ret(E_OUTOFMEMORY);
        }
        const HRESULT track_hr = track_resource_allocation_locked(dev, dst, /*write=*/false);
        if (FAILED(track_hr)) {
          return trace.ret(track_hr);
        }
        auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
        if (!cmd) {
          return trace.ret(E_OUTOFMEMORY);
        }
        cmd->resource_handle = dst->handle;
        cmd->reserved0 = 0;
        cmd->offset_bytes = 0;
        cmd->size_bytes = dst->size_bytes;
      } else {
        // Host-allocated resources must be updated by embedding raw bytes in the
        // command stream (UPLOAD_RESOURCE), mirroring the Lock/Unlock writeback
        // path.
        if (dst->storage.size() < dst->size_bytes) {
          return trace.ret(E_FAIL);
        }

        const uint8_t* src_bytes = dst->storage.data();
        uint32_t remaining = dst->size_bytes;
        uint32_t cur_offset = 0;
        while (remaining) {
          const size_t min_needed = align_up(sizeof(aerogpu_cmd_upload_resource) + 1, 4);
          if (!ensure_cmd_space(dev, min_needed)) {
            return trace.ret(E_OUTOFMEMORY);
          }

          // Track allocations so the host can resolve the destination. For
          // host-allocated surfaces this is a no-op.
          const HRESULT track_hr = track_resource_allocation_locked(dev, dst, /*write=*/true);
          if (FAILED(track_hr)) {
            return trace.ret(track_hr);
          }

          const size_t avail = dev->cmd.bytes_remaining();
          size_t chunk = 0;
          if (avail > sizeof(aerogpu_cmd_upload_resource)) {
            chunk = std::min<size_t>(remaining, avail - sizeof(aerogpu_cmd_upload_resource));
          }
          while (chunk && align_up(sizeof(aerogpu_cmd_upload_resource) + chunk, 4) > avail) {
            chunk--;
          }
          if (!chunk) {
            submit(dev);
            continue;
          }

          auto* cmd = append_with_payload_locked<aerogpu_cmd_upload_resource>(
              dev, AEROGPU_CMD_UPLOAD_RESOURCE, src_bytes + cur_offset, chunk);
          if (!cmd) {
            return trace.ret(E_OUTOFMEMORY);
          }
          cmd->resource_handle = dst->handle;
          cmd->reserved0 = 0;
          cmd->offset_bytes = cur_offset;
          cmd->size_bytes = chunk;

          cur_offset += static_cast<uint32_t>(chunk);
          remaining -= static_cast<uint32_t>(chunk);
        }
      }
    }

    return trace.ret(S_OK);
  }

  // Transfer-supported path: issue a host copy with WRITEBACK_DST so the GPU
  // texture bytes land in the systemmem surface's backing allocation.
  if (!src->handle || !dst->handle) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (dst->backing_alloc_id == 0) {
    // WRITEBACK_DST requires a guest allocation backing the destination so the
    // host can populate the systemmem surface bytes. When transfer support is
    // enabled, render target bytes are not guaranteed to be CPU-visible without
    // an explicit writeback, so do not silently fall back to a CPU copy.
    static std::once_flag log_once;
    std::call_once(log_once, [] {
      logf("aerogpu-d3d9: GetRenderTargetData requires allocation-backed systemmem surface when transfer is enabled\n");
    });
    return trace.ret(kD3DErrInvalidCall);
  }

  uint64_t fence = 0;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);

    // Ensure we can fit the copy packet before tracking allocations: allocation
    // tracking can force a submission split, and we must not split after
    // populating the allocation list for this command.
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_copy_texture2d), 4))) {
      return trace.ret(E_OUTOFMEMORY);
    }

    if (track_resource_allocation_locked(dev, dst, /*write=*/true) < 0) {
      return trace.ret(E_FAIL);
    }
    if (track_resource_allocation_locked(dev, src, /*write=*/false) < 0) {
      return trace.ret(E_FAIL);
    }
    // Allocation tracking can flush/split the current submission if the runtime
    // allocation list is full. If tracking `src` forced a split, the allocation
    // list has been reset and we must re-track `dst` so the final submission
    // references both allocations.
    if (track_resource_allocation_locked(dev, dst, /*write=*/true) < 0) {
      return trace.ret(E_FAIL);
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_copy_texture2d>(dev, AEROGPU_CMD_COPY_TEXTURE2D);
    if (!cmd) {
      return trace.ret(E_OUTOFMEMORY);
    }
    cmd->dst_texture = dst->handle;
    cmd->src_texture = src->handle;
    cmd->dst_mip_level = 0;
    cmd->dst_array_layer = 0;
    cmd->src_mip_level = 0;
    cmd->src_array_layer = 0;
    cmd->dst_x = 0;
    cmd->dst_y = 0;
    cmd->src_x = 0;
    cmd->src_y = 0;
    cmd->width = dst->width;
    cmd->height = dst->height;
    cmd->flags = AEROGPU_COPY_FLAG_WRITEBACK_DST;
    cmd->reserved0 = 0;

    fence = submit(dev);
  }

  // Wait for completion so the CPU sees final pixels.
  const FenceWaitResult wait_res = wait_for_fence(dev, fence, /*timeout_ms=*/2000);
  if (wait_res == FenceWaitResult::Failed) {
    return trace.ret(E_FAIL);
  }
  if (wait_res == FenceWaitResult::NotReady) {
    return trace.ret(kD3dErrWasStillDrawing);
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_copy_rects(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_COPYRECTS* pCopyRects) {
  const uint64_t src_ptr = pCopyRects ? d3d9_trace_arg_ptr(pCopyRects->hSrcResource.pDrvPrivate) : 0;
  const uint64_t dst_ptr = pCopyRects ? d3d9_trace_arg_ptr(pCopyRects->hDstResource.pDrvPrivate) : 0;
  const RECT* rect_list = pCopyRects ? d3d9_copy_rects_rects(*pCopyRects) : nullptr;
  const uint32_t rect_count = pCopyRects ? d3d9_copy_rects_count(*pCopyRects) : 0;
  const uint64_t rects =
      pCopyRects ? d3d9_trace_pack_u32_u32(rect_count, rect_list != nullptr ? 1u : 0u) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCopyRects, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), src_ptr, dst_ptr, rects);
  if (!hDevice.pDrvPrivate || !pCopyRects) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* src = as_resource(pCopyRects->hSrcResource);
  auto* dst = as_resource(pCopyRects->hDstResource);
  if (!dev || !src || !dst) {
    return trace.ret(E_INVALIDARG);
  }

  // Fast path: GPU -> systemmem copy (readback). If the destination is a
  // systemmem surface backed by a guest allocation, emit a host copy with
  // WRITEBACK_DST so the bytes land in guest memory for CPU LockRect.
  if (dst->pool == kD3DPOOL_SYSTEMMEM &&
      dst->backing_alloc_id != 0 &&
      SupportsTransfer(dev) &&
      src->handle != 0 &&
      dst->handle != 0 &&
      src->format == dst->format &&
      (!pCopyRects->pSrcRects || pCopyRects->rect_count == 0)) {
    const uint32_t width = std::min<uint32_t>(src->width, dst->width);
    const uint32_t height = std::min<uint32_t>(src->height, dst->height);
    if (width == 0 || height == 0) {
      return trace.ret(S_OK);
    }

    uint64_t fence = 0;
    {
      std::lock_guard<std::mutex> lock(dev->mutex);

      if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_copy_texture2d), 4))) {
        return trace.ret(E_OUTOFMEMORY);
      }

      if (track_resource_allocation_locked(dev, dst, /*write=*/true) < 0) {
        return trace.ret(E_FAIL);
      }
      if (track_resource_allocation_locked(dev, src, /*write=*/false) < 0) {
        return trace.ret(E_FAIL);
      }
      if (track_resource_allocation_locked(dev, dst, /*write=*/true) < 0) {
        return trace.ret(E_FAIL);
      }

      auto* cmd = append_fixed_locked<aerogpu_cmd_copy_texture2d>(dev, AEROGPU_CMD_COPY_TEXTURE2D);
      if (!cmd) {
        return trace.ret(E_OUTOFMEMORY);
      }
      cmd->dst_texture = dst->handle;
      cmd->src_texture = src->handle;
      cmd->dst_mip_level = 0;
      cmd->dst_array_layer = 0;
      cmd->src_mip_level = 0;
      cmd->src_array_layer = 0;
      cmd->dst_x = 0;
      cmd->dst_y = 0;
      cmd->src_x = 0;
      cmd->src_y = 0;
      cmd->width = width;
      cmd->height = height;
      cmd->flags = AEROGPU_COPY_FLAG_WRITEBACK_DST;
      cmd->reserved0 = 0;

      fence = submit(dev);
    }

    const FenceWaitResult wait_res = wait_for_fence(dev, fence, /*timeout_ms=*/2000);
    if (wait_res == FenceWaitResult::Failed) {
      return trace.ret(E_FAIL);
    }
    if (wait_res == FenceWaitResult::NotReady) {
      return trace.ret(kD3dErrWasStillDrawing);
    }
    return trace.ret(S_OK);
  }

  uint64_t fence = 0;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    fence = submit(dev);
  }
  const FenceWaitResult wait_res = wait_for_fence(dev, fence, /*timeout_ms=*/2000);
  if (wait_res == FenceWaitResult::Failed) {
    return trace.ret(E_FAIL);
  }
  if (wait_res == FenceWaitResult::NotReady) {
    return trace.ret(kD3dErrWasStillDrawing);
  }

  const HRESULT hr = copy_surface_rects(dev, src, dst, rect_list, rect_count);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  // CPU writes are observable by the host differently depending on backing mode:
  // - allocation-backed: notify using RESOURCE_DIRTY_RANGE.
  // - host-backed: upload bytes explicitly using UPLOAD_RESOURCE.
  if (dst->handle != 0 && dst->size_bytes) {
    if (dst->backing_alloc_id != 0) {
      std::lock_guard<std::mutex> lock(dev->mutex);

      if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
        return trace.ret(E_OUTOFMEMORY);
      }
      const HRESULT track_hr = track_resource_allocation_locked(dev, dst, /*write=*/false);
      if (FAILED(track_hr)) {
        return trace.ret(track_hr);
      }
      auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
      if (!cmd) {
        return trace.ret(E_OUTOFMEMORY);
      }
      cmd->resource_handle = dst->handle;
      cmd->reserved0 = 0;
      cmd->offset_bytes = 0;
      cmd->size_bytes = dst->size_bytes;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);

      if (!rect_list || rect_count == 0) {
        // Full-surface copy: upload the base mip bytes (not the full mip chain).
        const HRESULT upload_hr = emit_upload_resource_range_locked(dev, dst, /*offset=*/0, dst->slice_pitch);
        if (FAILED(upload_hr)) {
          return trace.ret(upload_hr);
        }
      } else {
        const uint32_t bpp = bytes_per_pixel(dst->format);
        for (uint32_t i = 0; i < rect_count; ++i) {
          const RECT& r = rect_list[i];
          if (r.right <= r.left || r.bottom <= r.top) {
            continue;
          }

          const uint32_t left = static_cast<uint32_t>(std::max<long>(0, r.left));
          const uint32_t top = static_cast<uint32_t>(std::max<long>(0, r.top));
          const uint32_t right = static_cast<uint32_t>(std::max<long>(0, r.right));
          const uint32_t bottom = static_cast<uint32_t>(std::max<long>(0, r.bottom));

          const uint32_t clamped_right = std::min<uint32_t>({right, src->width, dst->width});
          const uint32_t clamped_bottom = std::min<uint32_t>({bottom, src->height, dst->height});

          if (left >= clamped_right || top >= clamped_bottom) {
            continue;
          }

          const uint32_t row_bytes = (clamped_right - left) * bpp;
          for (uint32_t y = top; y < clamped_bottom; ++y) {
            const uint64_t off_u64 =
                static_cast<uint64_t>(y) * static_cast<uint64_t>(dst->row_pitch) + static_cast<uint64_t>(left) * bpp;
            if (off_u64 > 0xFFFFFFFFull || off_u64 + row_bytes > dst->slice_pitch) {
              return trace.ret(E_INVALIDARG);
            }
            const uint32_t off = static_cast<uint32_t>(off_u64);
            const HRESULT upload_hr = emit_upload_resource_range_locked(dev, dst, off, row_bytes);
            if (FAILED(upload_hr)) {
              return trace.ret(upload_hr);
            }
          }
        }
      }
    }
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_render_target(
    D3DDDI_HDEVICE hDevice,
    uint32_t slot,
    D3DDDI_HRESOURCE hSurface) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetRenderTarget,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(slot),
                      d3d9_trace_arg_ptr(hSurface.pDrvPrivate),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (slot >= 4) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* surf = as_resource(hSurface);

  std::lock_guard<std::mutex> lock(dev->mutex);

  Resource* saved_rts[4] = {dev->render_targets[0], dev->render_targets[1], dev->render_targets[2], dev->render_targets[3]};

  if (surf && slot > 0) {
    for (uint32_t i = 0; i < slot; ++i) {
      if (!dev->render_targets[i]) {
        return trace.ret(kD3DErrInvalidCall);
      }
    }
  }

  dev->render_targets[slot] = surf;
  if (!surf) {
    // Maintain contiguity: clearing an earlier slot implicitly clears any later
    // render targets so the host never sees a gapped binding.
    for (uint32_t i = slot + 1; i < 4; ++i) {
      dev->render_targets[i] = nullptr;
    }
  }

  bool changed = false;
  for (uint32_t i = 0; i < 4; ++i) {
    if (dev->render_targets[i] != saved_rts[i]) {
      changed = true;
      break;
    }
  }
  if (!changed) {
    stateblock_record_render_target_locked(dev, slot, dev->render_targets[slot]);
    if (!surf) {
      for (uint32_t i = slot + 1; i < 4; ++i) {
        stateblock_record_render_target_locked(dev, i, dev->render_targets[i]);
      }
    }
    return trace.ret(S_OK);
  }

  if (!emit_set_render_targets_locked(dev)) {
    for (uint32_t i = 0; i < 4; ++i) {
      dev->render_targets[i] = saved_rts[i];
    }
    return trace.ret(E_OUTOFMEMORY);
  }
  stateblock_record_render_target_locked(dev, slot, dev->render_targets[slot]);
  if (!surf) {
    for (uint32_t i = slot + 1; i < 4; ++i) {
      stateblock_record_render_target_locked(dev, i, dev->render_targets[i]);
    }
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_depth_stencil(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hSurface) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetDepthStencil,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hSurface.pDrvPrivate),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* surf = as_resource(hSurface);

  std::lock_guard<std::mutex> lock(dev->mutex);

  if (dev->depth_stencil == surf) {
    stateblock_record_depth_stencil_locked(dev, surf);
    return trace.ret(S_OK);
  }
  dev->depth_stencil = surf;
  if (!emit_set_render_targets_locked(dev)) {
    return trace.ret(E_OUTOFMEMORY);
  }
  stateblock_record_depth_stencil_locked(dev, surf);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_viewport(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIVIEWPORTINFO* pViewport) {
  const uint64_t xy = pViewport ? d3d9_trace_pack_u32_u32(f32_bits(pViewport->X), f32_bits(pViewport->Y)) : 0;
  const uint64_t wh = pViewport ? d3d9_trace_pack_u32_u32(f32_bits(pViewport->Width), f32_bits(pViewport->Height)) : 0;
  const uint64_t zz = pViewport ? d3d9_trace_pack_u32_u32(f32_bits(pViewport->MinZ), f32_bits(pViewport->MaxZ)) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetViewport, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), xy, wh, zz);
  if (!hDevice.pDrvPrivate || !pViewport) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  const bool cached_same =
      f32_bits(dev->viewport.X) == f32_bits(pViewport->X) &&
      f32_bits(dev->viewport.Y) == f32_bits(pViewport->Y) &&
      f32_bits(dev->viewport.Width) == f32_bits(pViewport->Width) &&
      f32_bits(dev->viewport.Height) == f32_bits(pViewport->Height) &&
      f32_bits(dev->viewport.MinZ) == f32_bits(pViewport->MinZ) &&
      f32_bits(dev->viewport.MaxZ) == f32_bits(pViewport->MaxZ);
  if (!cached_same) {
    dev->viewport = *pViewport;
  }
  stateblock_record_viewport_locked(dev, dev->viewport);

  if (cached_same) {
    // Skip redundant viewport uploads: setting identical viewport again is a
    // no-op, but still recorded above for state blocks.
    return trace.ret(S_OK);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_viewport>(dev, AEROGPU_CMD_SET_VIEWPORT);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->x_f32 = f32_bits(pViewport->X);
  cmd->y_f32 = f32_bits(pViewport->Y);
  cmd->width_f32 = f32_bits(pViewport->Width);
  cmd->height_f32 = f32_bits(pViewport->Height);
  cmd->min_depth_f32 = f32_bits(pViewport->MinZ);
  cmd->max_depth_f32 = f32_bits(pViewport->MaxZ);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_scissor(
    D3DDDI_HDEVICE hDevice,
    const RECT* pRect,
    BOOL enabled) {
  const uint64_t lt = pRect ? d3d9_trace_pack_u32_u32(static_cast<uint32_t>(pRect->left), static_cast<uint32_t>(pRect->top)) : 0;
  const uint64_t rb =
      pRect ? d3d9_trace_pack_u32_u32(static_cast<uint32_t>(pRect->right), static_cast<uint32_t>(pRect->bottom)) : 0;
  D3d9TraceCall trace(
      D3d9TraceFunc::DeviceSetScissorRect, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), lt, rb, static_cast<uint64_t>(enabled));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  const BOOL prev_enabled = dev->scissor_enabled;
  const RECT prev_rect = dev->scissor_rect;

  if (pRect) {
    dev->scissor_rect = *pRect;
    dev->scissor_rect_user_set = true;
  }
  dev->scissor_enabled = enabled;
  // Keep D3DRS_SCISSORTESTENABLE (174) in sync with the dedicated scissor state,
  // since some D3D9 runtimes route SCISSORTESTENABLE through SetScissorRect DDIs.
  //
  // Use the numeric value to avoid requiring the Windows SDK/WDK in portable
  // builds.
  constexpr uint32_t kD3dRsScissorTestEnable = 174u;
  if (kD3dRsScissorTestEnable < 256) {
    dev->render_states[kD3dRsScissorTestEnable] = dev->scissor_enabled ? 1u : 0u;
    stateblock_record_render_state_locked(dev, kD3dRsScissorTestEnable, dev->render_states[kD3dRsScissorTestEnable]);
  }
  scissor_fixup_unset_rect_locked(dev);
  stateblock_record_scissor_locked(dev, dev->scissor_rect, dev->scissor_enabled);

  bool scissor_cmd_same = false;
  if (!prev_enabled && !dev->scissor_enabled) {
    // When scissor is disabled, the command-stream representation does not
    // depend on the stored rect.
    scissor_cmd_same = true;
  } else if (prev_enabled && dev->scissor_enabled) {
    scissor_cmd_same = (prev_rect.left == dev->scissor_rect.left &&
                        prev_rect.top == dev->scissor_rect.top &&
                        prev_rect.right == dev->scissor_rect.right &&
                        prev_rect.bottom == dev->scissor_rect.bottom);
  }
  if (scissor_cmd_same) {
    // Skip redundant scissor uploads: setting identical scissor state again is a
    // no-op, but still recorded above for state blocks.
    return trace.ret(S_OK);
  }

  int32_t x = 0;
  int32_t y = 0;
  int32_t w = 0x7FFFFFFF;
  int32_t h = 0x7FFFFFFF;
  if (enabled) {
    const RECT& rect = dev->scissor_rect;
    x = static_cast<int32_t>(rect.left);
    y = static_cast<int32_t>(rect.top);
    w = static_cast<int32_t>(rect.right - rect.left);
    h = static_cast<int32_t>(rect.bottom - rect.top);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_scissor>(dev, AEROGPU_CMD_SET_SCISSOR);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->x = x;
  cmd->y = y;
  cmd->width = w;
  cmd->height = h;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_texture(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    D3DDDI_HRESOURCE hTexture) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetTexture,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stage),
                      d3d9_trace_arg_ptr(hTexture.pDrvPrivate),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (stage >= 16) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* tex = as_resource(hTexture);

  std::lock_guard<std::mutex> lock(dev->mutex);

  if (dev->textures[stage] == tex) {
    stateblock_record_texture_locked(dev, stage, tex);
    return trace.ret(S_OK);
  }
  dev->textures[stage] = tex;
  stateblock_record_texture_locked(dev, stage, tex);

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
  cmd->slot = stage;
  cmd->texture = tex ? tex->handle : 0;
  cmd->reserved0 = 0;

  // If the app is using a fixed-function pixel stage (no user PS), texture
  // binding/unbinding can affect the selected internal pixel shader variant.
  if (stage < kFixedfuncMaxTextureStages && !dev->user_ps) {
    Shader** ps_slot = nullptr;
    if (dev->user_vs) {
      // VS-only interop path: use the dedicated fixed-function PS cache.
      ps_slot = &dev->fixedfunc_ps_interop;
    } else if (fixedfunc_supported_fvf(dev->fvf)) {
      // Full fixed-function path: update the active variant's cached PS slot.
      const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
      if (variant != FixedFuncVariant::NONE) {
        ps_slot = &dev->fixedfunc_pipelines[static_cast<size_t>(variant)].ps;
      }
    }
    if (ps_slot) {
      // Fixed-function stage-state selection is guarded: if the app configured an
      // unsupported stage-state combination, tolerate state changes (including
      // texture binds) and fail draws with INVALIDCALL instead.
      const FixedfuncPixelShaderKey ps_key = fixedfunc_ps_key_locked(dev);
      if (ps_key.supported) {
        const HRESULT ps_hr = ensure_fixedfunc_pixel_shader_locked(dev, ps_slot);
        if (FAILED(ps_hr)) {
          return trace.ret(ps_hr);
        }
      }
    }
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_texture_stage_state(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t state,
    uint32_t* pValue) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetTextureStageState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(stage, state),
                      d3d9_trace_arg_ptr(pValue),
                      0);
  if (pValue) {
    *pValue = 0u;
  }
  if (!hDevice.pDrvPrivate || !pValue) {
    return trace.ret(E_INVALIDARG);
  }
  if (stage >= 16 || state >= 256) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  *pValue = dev->texture_stage_states[stage][state];
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_sampler_state(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t state,
    uint32_t value) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetSamplerState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stage),
                      static_cast<uint64_t>(state),
                      static_cast<uint64_t>(value));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (stage >= 16) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  stateblock_record_sampler_state_locked(dev, stage, state, value);
  if (stage < 16 && state < 16) {
    if (dev->sampler_states[stage][state] == value) {
      // Skip redundant sampler state uploads: setting identical state again is a
      // no-op, but still record it for state blocks above.
      return trace.ret(S_OK);
    }
    dev->sampler_states[stage][state] = value;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_sampler_state>(dev, AEROGPU_CMD_SET_SAMPLER_STATE);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
  cmd->slot = stage;
  cmd->state = state;
  cmd->value = value;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_render_state(
    D3DDDI_HDEVICE hDevice,
    uint32_t state,
    uint32_t value) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetRenderState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(state),
                      static_cast<uint64_t>(value),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  stateblock_record_render_state_locked(dev, state, value);

  const bool cached_same = (state < 256) && (dev->render_states[state] == value);
  if (!cached_same && state < 256) {
    dev->render_states[state] = value;
  }

  // Fixed-function lighting is implemented by the fixed-function fallback VS
  // variants and consumes a reserved VS constant range. Changes to lighting
  // render state must mark the constant block dirty.
  constexpr uint32_t kD3dRsAmbient = 26u;  // D3DRS_AMBIENT
  constexpr uint32_t kD3dRsLighting = 137u; // D3DRS_LIGHTING
  if (state == kD3dRsAmbient || state == kD3dRsLighting) {
    dev->fixedfunc_lighting_dirty = true;
  }

  // Some D3D9 runtimes may treat SCISSORTESTENABLE as a plain render state (DDI),
  // while the AeroGPU command stream uses an explicit scissor packet. Keep the
  // cached scissor enable flag in sync and emit the matching scissor command so
  // rendering and GetRenderState stay consistent.
  constexpr uint32_t kD3dRsScissorTestEnable = 174u; // D3DRS_SCISSORTESTENABLE
  if (state == kD3dRsScissorTestEnable) {
    const BOOL enabled = value ? TRUE : FALSE;
    if ((dev->scissor_enabled ? TRUE : FALSE) != (enabled ? TRUE : FALSE)) {
      dev->scissor_enabled = enabled;
      scissor_fixup_unset_rect_locked(dev);
      stateblock_record_scissor_locked(dev, dev->scissor_rect, dev->scissor_enabled);

      int32_t x = 0;
      int32_t y = 0;
      int32_t w = 0x7FFFFFFF;
      int32_t h = 0x7FFFFFFF;
      if (dev->scissor_enabled) {
        const RECT& rect = dev->scissor_rect;
        x = static_cast<int32_t>(rect.left);
        y = static_cast<int32_t>(rect.top);
        w = static_cast<int32_t>(rect.right - rect.left);
        h = static_cast<int32_t>(rect.bottom - rect.top);
      }

      auto* sc = append_fixed_locked<aerogpu_cmd_set_scissor>(dev, AEROGPU_CMD_SET_SCISSOR);
      if (!sc) {
        return trace.ret(E_OUTOFMEMORY);
      }
      sc->x = x;
      sc->y = y;
      sc->width = w;
      sc->height = h;
    }
  }

  // D3DRS_TEXTUREFACTOR is consumed by the fixed-function texture stage combiner
  // fallback when the texture combiners reference TFACTOR. Upload it into the reserved PS
  // constant register (c255) on render-state updates so legacy apps that animate
  // TEXTUREFACTOR without touching stage-state still render correctly.
  constexpr uint32_t kD3dRsTextureFactor = 60u; // D3DRS_TEXTUREFACTOR
  if (state == kD3dRsTextureFactor && !dev->user_ps) {
    const FixedfuncPixelShaderKey ps_key = fixedfunc_ps_key_locked(dev);
    if (ps_key.supported && ps_key.uses_tfactor) {
      const HRESULT hr = ensure_fixedfunc_texture_factor_constant_locked(dev);
      if (FAILED(hr)) {
        return trace.ret(hr);
      }
    }
  }

  if (cached_same) {
    // Skip redundant render-state uploads: setting identical state again is a
    // no-op, but still recorded above for state blocks.
    return trace.ret(S_OK);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_render_state>(dev, AEROGPU_CMD_SET_RENDER_STATE);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->state = state;
  cmd->value = value;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_create_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    const void* pDecl,
    uint32_t decl_size,
    D3D9DDI_HVERTEXDECL* phDecl) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCreateVertexDecl,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(decl_size),
                      d3d9_trace_arg_ptr(pDecl),
                      d3d9_trace_arg_ptr(phDecl));
  if (!hDevice.pDrvPrivate || !pDecl || !phDecl || decl_size == 0) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  auto decl = make_unique_nothrow<VertexDecl>();
  if (!decl) {
    return trace.ret(E_OUTOFMEMORY);
  }
  decl->handle = allocate_global_handle(dev->adapter);
  try {
    decl->blob.resize(decl_size);
  } catch (...) {
    return trace.ret(E_OUTOFMEMORY);
  }
  std::memcpy(decl->blob.data(), pDecl, decl_size);

  if (!emit_create_input_layout_locked(dev, decl.get())) {
    return trace.ret(E_OUTOFMEMORY);
  }

  phDecl->pDrvPrivate = decl.release();
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HVERTEXDECL hDecl) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetVertexDecl,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hDecl.pDrvPrivate),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* decl = as_vertex_decl(hDecl);

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (!emit_set_input_layout_locked(dev, decl)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  // Many D3D9 apps (and some D3D9 runtimes implementing SetFVF) use vertex
  // declarations directly, but still rely on the fixed-function pipeline (no
  // shaders). Detect common decl patterns and synthesize an implied FVF value so
  // the fixed-function fallback path can activate even when `pfnSetFVF` is never
  // invoked.
  uint32_t implied_fvf = 0;

  // Infer a fixed-function FVF (including TEXCOORDSIZE bits and position-only
  // decls) so ProcessVertices + fixed-function fallback can activate even when
  // apps use vertex declarations instead of SetFVF.
  if (decl && !decl->blob.empty()) {
    implied_fvf = fixedfunc_implied_fvf_from_decl_blob(decl->blob.data(), decl->blob.size());
  }
  dev->fvf = implied_fvf;
  if (fixedfunc_fvf_needs_matrix(dev->fvf)) {
    // Switching to the WVP-fixed-function path must refresh the reserved VS
    // constant range, even if transforms did not change (user shaders may have
    // written overlapping registers).
    dev->fixedfunc_matrix_dirty = true;
  }
  if (fixedfunc_fvf_has_normal(implied_fvf)) {
    dev->fixedfunc_lighting_dirty = true;
  }
  stateblock_record_vertex_decl_locked(dev, decl, dev->fvf);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_destroy_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HVERTEXDECL hDecl) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroyVertexDecl,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hDecl.pDrvPrivate),
                      0,
                      0);
  auto* dev = as_device(hDevice);
  auto* decl = as_vertex_decl(hDecl);
  if (!dev || !decl) {
    delete decl;
    return trace.ret(S_OK);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->vertex_decl == decl) {
    dev->vertex_decl = nullptr;
    if (auto* cmd = append_fixed_locked<aerogpu_cmd_set_input_layout>(dev, AEROGPU_CMD_SET_INPUT_LAYOUT)) {
      cmd->input_layout_handle = 0;
      cmd->reserved0 = 0;
    }
  }
  (void)emit_destroy_input_layout_locked(dev, decl->handle);
  delete decl;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_fvf(D3DDDI_HDEVICE hDevice, uint32_t fvf) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetFVF,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(fvf),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);

  // Fast path: If the runtime sets the same FVF repeatedly, we can avoid all
  // work. However, for supported fixed-function FVFs we must *still* ensure the
  // matching internal FVF-derived vertex declaration is bound.
  //
  // This matters when the runtime previously set an explicit vertex declaration
  // (and we inferred an implied FVF from it), then calls SetFVF with that same
  // value: SetFVF must override the explicit decl and bind the internal one so
  // fixed-function draws and state blocks observe consistent state.
  if (fvf == dev->fvf && !fixedfunc_supported_fvf(fvf)) {
    stateblock_record_vertex_decl_locked(dev, dev->vertex_decl, dev->fvf);
    return trace.ret(S_OK);
  }

  if (fvf == 0) {
    dev->fvf = 0;
    stateblock_record_vertex_decl_locked(dev, dev->vertex_decl, dev->fvf);
    return trace.ret(S_OK);
  }

  // The AeroGPU fixed-function fallback supports only a small FVF subset (see
  // `drivers/aerogpu/umd/d3d9/README.md`). The SetFVF path synthesizes/binds an
  // internal vertex declaration for supported fixed-function FVFs so the UMD can
  // bind a known input layout.
  //
  // Other FVFs may be accepted and cached so GetFVF + state blocks behave
  // deterministically, but rendering is not guaranteed for unsupported formats.
  const FixedFuncVariant variant = fixedfunc_variant_from_fvf(fvf);
  if (variant != FixedFuncVariant::NONE) {
    VertexDecl* decl = nullptr;

    // TEXCOORDSIZE bits affect vertex layout (stride/offsets), but they do not
    // change which fixed-function shader variant is needed. For TEX1 variants,
    // synthesize a custom internal decl when TEXCOORD0 isn't the default float2.
    const bool has_tex1 = (fixedfunc_fvf_from_variant(variant) & kD3dFvfTex1) != 0;
    constexpr uint32_t kTex0SizeBitsMask = 0x3u << 16u;
    const uint32_t tex0_size_bits = has_tex1 ? (fvf & kTex0SizeBitsMask) : 0u;
    const bool needs_custom_decl = (tex0_size_bits != 0u);

    if (needs_custom_decl) {
      const uint32_t layout_key = fvf_layout_key(fvf);
      FvfVertexDeclTranslation translated{};
      if (!try_translate_fvf_to_vertex_decl(layout_key, &translated)) {
        return trace.ret(kD3DErrInvalidCall);
      }
      const auto it = dev->fvf_vertex_decl_cache.find(layout_key);
      if (it != dev->fvf_vertex_decl_cache.end()) {
        decl = it->second;
      } else {
        // Bounded cache: FVFs are 32-bit and in theory unbounded; keep a generous
        // per-device cap to avoid unbounded growth under pathological inputs.
        constexpr size_t kMaxCachedFvfDecls = 256;
        if (dev->fvf_vertex_decl_cache.size() >= kMaxCachedFvfDecls) {
          static std::once_flag warn_once;
          std::call_once(warn_once, [] {
            logf("aerogpu-d3d9: FVF input-layout cache overflow; refusing to create more internal declarations\n");
          });
          return trace.ret(E_OUTOFMEMORY);
        }

        const uint32_t decl_size = translated.elem_count * static_cast<uint32_t>(sizeof(D3DVERTEXELEMENT9_COMPAT));
        decl = create_internal_vertex_decl_locked(dev, translated.elems.data(), decl_size);
        if (!decl) {
          return trace.ret(E_OUTOFMEMORY);
        }
        try {
          dev->fvf_vertex_decl_cache.emplace(layout_key, decl);
        } catch (...) {
          // Cache insertion is required to keep the internal decl alive; if it fails, tear down the
          // newly-created host object and return OOM.
          (void)emit_destroy_input_layout_locked(dev, decl->handle);
          delete decl;
          return trace.ret(E_OUTOFMEMORY);
        }
      }
    } else {
      const FixedFuncVariantDeclDesc* decl_desc = fixedfunc_decl_desc(variant);
      if (!decl_desc || !decl_desc->elems || decl_desc->elem_count == 0) {
        return trace.ret(E_FAIL);
      }

      auto& pipe = dev->fixedfunc_pipelines[static_cast<size_t>(variant)];
      if (!pipe.vertex_decl) {
        const uint32_t decl_size = static_cast<uint32_t>(decl_desc->elem_count * sizeof(D3DVERTEXELEMENT9_COMPAT));
        pipe.vertex_decl = create_internal_vertex_decl_locked(dev, decl_desc->elems, decl_size);
        if (!pipe.vertex_decl) {
          return trace.ret(E_OUTOFMEMORY);
        }
      }
      decl = pipe.vertex_decl;
    }

    if (!decl || !emit_set_input_layout_locked(dev, decl)) {
      return trace.ret(E_OUTOFMEMORY);
    }

    dev->fvf = fvf;
    if (fixedfunc_fvf_needs_matrix(dev->fvf)) {
      // Switching to the WVP-fixed-function path must refresh the reserved VS
      // constant range, even if transforms did not change (user shaders may have
      // written overlapping registers).
      dev->fixedfunc_matrix_dirty = true;
    }
    if (fixedfunc_fvf_has_normal(dev->fvf)) {
      dev->fixedfunc_lighting_dirty = true;
    }
    stateblock_record_vertex_decl_locked(dev, decl, dev->fvf);
    return trace.ret(S_OK);
  }

  // Programmable pipeline path: synthesize an input layout for a common subset
  // of FVFs beyond the minimal fixed-function bring-up list above. This is
  // required for apps that use user shaders with SetFVF instead of an explicit
  // vertex declaration.
  FvfVertexDeclTranslation translated{};
  const uint32_t layout_key = fvf_layout_key(fvf);
  if (try_translate_fvf_to_vertex_decl(layout_key, &translated)) {
    VertexDecl* decl = nullptr;
    const auto it = dev->fvf_vertex_decl_cache.find(layout_key);
    if (it != dev->fvf_vertex_decl_cache.end()) {
      decl = it->second;
    } else {
      // Bounded cache: FVFs are 32-bit and in theory unbounded; keep a generous
      // per-device cap to avoid unbounded growth under pathological inputs.
      constexpr size_t kMaxCachedFvfDecls = 256;
      if (dev->fvf_vertex_decl_cache.size() >= kMaxCachedFvfDecls) {
        static std::once_flag warn_once;
        std::call_once(warn_once, [] {
          logf("aerogpu-d3d9: FVF input-layout cache overflow; refusing to create more internal declarations\n");
        });
        return trace.ret(E_OUTOFMEMORY);
      }

      const uint32_t decl_size = translated.elem_count * static_cast<uint32_t>(sizeof(D3DVERTEXELEMENT9_COMPAT));
      decl = create_internal_vertex_decl_locked(dev, translated.elems.data(), decl_size);
      if (!decl) {
        return trace.ret(E_OUTOFMEMORY);
      }
      try {
        dev->fvf_vertex_decl_cache.emplace(layout_key, decl);
      } catch (...) {
        (void)emit_destroy_input_layout_locked(dev, decl->handle);
        delete decl;
        return trace.ret(E_OUTOFMEMORY);
      }
    }

    if (!emit_set_input_layout_locked(dev, decl)) {
      return trace.ret(E_OUTOFMEMORY);
    }
    dev->fvf = fvf;
    stateblock_record_vertex_decl_locked(dev, decl, dev->fvf);
    return trace.ret(S_OK);
  }

  dev->fvf = fvf;
  stateblock_record_vertex_decl_locked(dev, dev->vertex_decl, dev->fvf);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_create_shader(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    const void* pBytecode,
    uint32_t bytecode_size,
    D3D9DDI_HSHADER* phShader) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCreateShader,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stage),
                      static_cast<uint64_t>(bytecode_size),
                      d3d9_trace_arg_ptr(pBytecode));
  if (!hDevice.pDrvPrivate || !pBytecode || !phShader || bytecode_size == 0) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  auto sh = make_unique_nothrow<Shader>();
  if (!sh) {
    return trace.ret(E_OUTOFMEMORY);
  }
  sh->handle = allocate_global_handle(dev->adapter);
  sh->stage = stage;
  try {
    sh->bytecode.resize(bytecode_size);
  } catch (...) {
    return trace.ret(E_OUTOFMEMORY);
  }
  std::memcpy(sh->bytecode.data(), pBytecode, bytecode_size);

  if (!emit_create_shader_locked(dev, sh.get())) {
    return trace.ret(E_OUTOFMEMORY);
  }

  phShader->pDrvPrivate = sh.release();
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_shader(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    D3D9DDI_HSHADER hShader) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShader,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stage),
                      d3d9_trace_arg_ptr(hShader.pDrvPrivate),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* sh = as_shader(hShader);

  std::lock_guard<std::mutex> lock(dev->mutex);

  Shader** user_slot = (stage == kD3d9ShaderStageVs) ? &dev->user_vs : &dev->user_ps;
  if (*user_slot == sh) {
    stateblock_record_shader_locked(dev, stage, sh);
    return trace.ret(S_OK);
  }

  *user_slot = sh;
  stateblock_record_shader_locked(dev, stage, sh);

  // Fixed-function bring-up path uses a reserved VS constant range for the
  // WVP matrix. A user vertex shader may have written overlapping registers, so
  // when the app later switches back to fixed-function (without necessarily
  // changing FVF/decl), we must re-upload the matrix constants.
  if (stage == kD3d9ShaderStageVs && sh && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    dev->fixedfunc_matrix_dirty = true;
  }
  if (stage == kD3d9ShaderStageVs && !sh && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    // Some runtimes expect fixed-function WVP constants to be refreshed
    // immediately when a user VS is unbound (not just lazily at draw time).
    dev->fixedfunc_matrix_dirty = true;
    dev->fixedfunc_matrix_force_upload = true;
  }
  // Same idea for fixed-function lighting: the lit VS variants use a reserved
  // high constant range for normals/light/material state.
  if (stage == kD3d9ShaderStageVs && sh && fixedfunc_fvf_has_normal(dev->fvf)) {
    dev->fixedfunc_lighting_dirty = true;
  }

  // D3D9 allows apps to bind only one stage (VS-only or PS-only); the missing
  // stage is treated as fixed-function. Ensure we never emit a BIND_SHADERS
  // packet with a null VS/PS handle.
  const HRESULT bind_hr = ensure_shader_bindings_locked(dev, /*strict_draw_validation=*/false);
  if (FAILED(bind_hr)) {
    return trace.ret(bind_hr);
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_destroy_shader(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HSHADER hShader) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroyShader,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hShader.pDrvPrivate),
                      0,
                      0);
  auto* dev = as_device(hDevice);
  auto* sh = as_shader(hShader);
  if (!dev || !sh) {
    delete sh;
    return trace.ret(S_OK);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  const bool was_user_vs = (dev->user_vs == sh);
  const bool was_user_ps = (dev->user_ps == sh);
  const bool was_bound_vs = (dev->vs == sh);
  const bool was_bound_ps = (dev->ps == sh);
  const bool bindings_changed = was_user_vs || was_user_ps || was_bound_vs || was_bound_ps;

  // The runtime may destroy a shader while it is still bound. Clear both the
  // public "user" bindings so subsequent draws can re-bind the fixed-function
  // fallback if needed.
  if (was_user_vs) {
    dev->user_vs = nullptr;
  }
  if (was_user_ps) {
    dev->user_ps = nullptr;
  }

  // If the shader was currently bound in the command stream, ensure we bind a
  // replacement non-null shader pair before destroying it. This keeps the command
  // stream valid: the host rejects null shader binds and must not observe a
  // DESTROY_SHADER for a still-bound handle.
  if (bindings_changed) {
    const HRESULT bind_hr = ensure_shader_bindings_locked(dev, /*strict_draw_validation=*/false);
    if (FAILED(bind_hr)) {
      // Best-effort: ensure we don't leave dangling pointers even if we failed to
      // emit the replacement bind.
      if (was_bound_vs) {
        dev->vs = nullptr;
      }
      if (was_bound_ps) {
        dev->ps = nullptr;
      }
    }
  }
  (void)emit_destroy_shader_locked(dev, sh->handle);
  delete sh;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_shader_const_f(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t start_reg,
    const float* pData,
    uint32_t vec4_count) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShaderConstF,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stage),
                      d3d9_trace_pack_u32_u32(start_reg, vec4_count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || vec4_count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t stage_norm = (stage == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start_reg >= 256 || vec4_count > 256u - start_reg) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  float* dst = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_f : dev->ps_consts_f;
  stateblock_record_shader_const_f_locked(dev, stage_norm, start_reg, pData, vec4_count);
  const size_t payload_size = static_cast<size_t>(vec4_count) * 4 * sizeof(float);
  if (std::memcmp(dst + start_reg * 4, pData, payload_size) == 0) {
    // Skip redundant constant uploads: setting identical constant data again is
    // a no-op, but still record it for state blocks above.
    return trace.ret(S_OK);
  }

  std::memcpy(dst + start_reg * 4, pData, payload_size);

  // If the app writes to the fixed-function reserved matrix constant range,
  // treat it as clobbered and re-upload on the next fixed-function draw.
  if (stage_norm == kD3d9ShaderStageVs && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const uint32_t end_reg = start_reg + vec4_count;
    const uint32_t ff_start = kFixedfuncMatrixStartRegister;
    const uint32_t ff_end = kFixedfuncMatrixStartRegister + kFixedfuncMatrixVec4Count;
    if (start_reg < ff_end && end_reg > ff_start) {
      dev->fixedfunc_matrix_dirty = true;
    }
  }
  // Likewise for the fixed-function lighting constant block used by the lit VS
  // variants for NORMAL FVFs.
  if (stage_norm == kD3d9ShaderStageVs && fixedfunc_fvf_has_normal(dev->fvf)) {
    const uint32_t end_reg = start_reg + vec4_count;
    const uint32_t ff_start = kFixedfuncLightingStartRegister;
    const uint32_t ff_end = kFixedfuncLightingStartRegister + kFixedfuncLightingVec4Count;
    if (start_reg < ff_end && end_reg > ff_start) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }

  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_f>(
      dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_F, pData, payload_size);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage_norm);
  cmd->start_register = start_reg;
  cmd->vec4_count = vec4_count;
  cmd->reserved0 = 0;

  return trace.ret(S_OK);
}

// -----------------------------------------------------------------------------
// State block DDIs
// -----------------------------------------------------------------------------

static void stateblock_init_for_type_locked(Device* dev, StateBlock* sb, uint32_t type_u32) {
  if (!dev || !sb) {
    return;
  }

  // Reset to a deterministic baseline.
  *sb = StateBlock{};

  // D3DSTATEBLOCKTYPE values (d3d9types.h):
  //   D3DSBT_ALL = 1
  //   D3DSBT_PIXELSTATE = 2
  //   D3DSBT_VERTEXSTATE = 3
  // Be permissive: some runtimes/headers may pass unexpected values (e.g. the
  // `FORCE_DWORD` sentinel). Treat unknown types as ALL so we capture a
  // meaningful snapshot rather than silently omitting most state.
  const bool is_all = (type_u32 == 1u) || (type_u32 == 0u) || (type_u32 > 3u);
  const bool is_pixel = is_all || (type_u32 == 2u);
  const bool is_vertex = is_all || (type_u32 == 3u);

  // Render states are treated as common state: include them in all block types
  // we support since the UMD forwards them generically.
  for (uint32_t i = 0; i < 256; ++i) {
    sb->render_state_mask.set(i);
    sb->render_state_values[i] = dev->render_states[i];
  }

  if (is_pixel) {
    for (uint32_t stage = 0; stage < 16; ++stage) {
      sb->texture_mask.set(stage);
      sb->textures[stage] = dev->textures[stage];
      for (uint32_t s = 0; s < 16; ++s) {
        const uint32_t idx = stage * 16u + s;
        sb->sampler_state_mask.set(idx);
        sb->sampler_state_values[idx] = dev->sampler_states[stage][s];
      }
      for (uint32_t s = 0; s < 256; ++s) {
        const uint32_t idx = stage * 256u + s;
        sb->texture_stage_state_mask.set(idx);
        sb->texture_stage_state_values[idx] = dev->texture_stage_states[stage][s];
      }
    }

    for (uint32_t i = 0; i < 4; ++i) {
      sb->render_target_mask.set(i);
      sb->render_targets[i] = dev->render_targets[i];
    }
    sb->depth_stencil_set = true;
    sb->depth_stencil = dev->depth_stencil;

    sb->viewport_set = true;
    sb->viewport = viewport_effective_locked(dev);
    sb->scissor_set = true;
    sb->scissor_rect = scissor_rect_effective_locked(dev);
    sb->scissor_rect_user_set = dev->scissor_rect_user_set;
    sb->scissor_enabled = dev->scissor_enabled;

    sb->user_ps_set = true;
    sb->user_ps = dev->user_ps;
    for (uint32_t r = 0; r < 256; ++r) {
      sb->ps_const_mask.set(r);
    }
    std::memcpy(sb->ps_consts.data(), dev->ps_consts_f, sizeof(float) * 256u * 4u);

    for (uint32_t r = 0; r < 256; ++r) {
      sb->ps_const_i_mask.set(r);
    }
    std::memcpy(sb->ps_consts_i.data(), dev->ps_consts_i, sizeof(int32_t) * 256u * 4u);

    for (uint32_t r = 0; r < 256; ++r) {
      sb->ps_const_b_mask.set(r);
    }
    std::memcpy(sb->ps_consts_b.data(), dev->ps_consts_b, sizeof(uint8_t) * 256u);

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    // Palette tables (for palettized textures) + current texture palette.
    for (uint32_t p = 0; p < Device::kMaxPalettes; ++p) {
      sb->palette_mask.set(p);
      std::memcpy(sb->palette_entries[p].data(), &dev->palette_entries[p][0], sizeof(dev->palette_entries[p]));
      if (dev->palette_valid[p]) {
        sb->palette_valid_bits.set(p);
      } else {
        sb->palette_valid_bits.reset(p);
      }
    }
    sb->current_texture_palette_set = true;
    sb->current_texture_palette = dev->current_texture_palette;

    sb->gamma_ramp_set = true;
    sb->gamma_ramp_valid = dev->gamma_ramp_valid;
    sb->gamma_ramp = dev->gamma_ramp;
#endif
  }

  if (is_vertex) {
    for (uint32_t t = 0; t < Device::kTransformCacheCount; ++t) {
      sb->transform_mask.set(t);
      std::memcpy(sb->transform_values.data() + static_cast<size_t>(t) * 16u,
                  dev->transform_matrices[t],
                  sizeof(float) * 16u);
    }

    for (uint32_t p = 0; p < 6; ++p) {
      sb->clip_plane_mask.set(p);
      std::memcpy(sb->clip_plane_values.data() + static_cast<size_t>(p) * 4u,
                  dev->clip_planes[p],
                  sizeof(float) * 4u);
    }

    sb->software_vertex_processing_set = true;
    sb->software_vertex_processing = dev->software_vertex_processing;
    sb->n_patch_mode_set = true;
    sb->n_patch_mode = dev->n_patch_mode;

    sb->material_set = true;
    sb->material_valid = dev->material_valid;
    sb->material = dev->material;

    for (uint32_t i = 0; i < Device::kMaxLights; ++i) {
      sb->light_mask.set(i);
      sb->lights[i] = dev->lights[i];
      if (dev->light_valid[i]) {
        sb->light_valid_bits.set(i);
      }
      sb->light_enable_mask.set(i);
      if (dev->light_enabled[i]) {
        sb->light_enable_bits.set(i);
      }
    }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    sb->clip_status_set = true;
    sb->clip_status_valid = dev->clip_status_valid;
    sb->clip_status = dev->clip_status;
#endif

    sb->vertex_decl_set = true;
    sb->vertex_decl = dev->vertex_decl;
    sb->fvf_set = true;
    sb->fvf = dev->fvf;

    for (uint32_t stream = 0; stream < 16; ++stream) {
      sb->stream_mask.set(stream);
      sb->streams[stream] = dev->streams[stream];
      sb->stream_source_freq_mask.set(stream);
      sb->stream_source_freq_values[stream] = dev->stream_source_freq[stream];
    }

    sb->index_buffer_set = true;
    sb->index_buffer = dev->index_buffer;
    sb->index_format = dev->index_format;
    sb->index_offset_bytes = dev->index_offset_bytes;

    sb->user_vs_set = true;
    sb->user_vs = dev->user_vs;
    for (uint32_t r = 0; r < 256; ++r) {
      sb->vs_const_mask.set(r);
    }
    std::memcpy(sb->vs_consts.data(), dev->vs_consts_f, sizeof(float) * 256u * 4u);

    for (uint32_t r = 0; r < 256; ++r) {
      sb->vs_const_i_mask.set(r);
    }
    std::memcpy(sb->vs_consts_i.data(), dev->vs_consts_i, sizeof(int32_t) * 256u * 4u);

    for (uint32_t r = 0; r < 256; ++r) {
      sb->vs_const_b_mask.set(r);
    }
    std::memcpy(sb->vs_consts_b.data(), dev->vs_consts_b, sizeof(uint8_t) * 256u);
  }

}

static void stateblock_capture_locked(Device* dev, StateBlock* sb) {
  if (!dev || !sb) {
    return;
  }

  for (uint32_t i = 0; i < 256; ++i) {
    if (sb->render_state_mask.test(i)) {
      sb->render_state_values[i] = dev->render_states[i];
    }
  }

  for (uint32_t t = 0; t < Device::kTransformCacheCount; ++t) {
    if (sb->transform_mask.test(t)) {
      std::memcpy(sb->transform_values.data() + static_cast<size_t>(t) * 16u,
                  dev->transform_matrices[t],
                  sizeof(float) * 16u);
    }
  }

  for (uint32_t p = 0; p < 6; ++p) {
    if (sb->clip_plane_mask.test(p)) {
      std::memcpy(sb->clip_plane_values.data() + static_cast<size_t>(p) * 4u,
                  dev->clip_planes[p],
                  sizeof(float) * 4u);
    }
  }

  if (sb->software_vertex_processing_set) {
    sb->software_vertex_processing = dev->software_vertex_processing;
  }
  if (sb->n_patch_mode_set) {
    sb->n_patch_mode = dev->n_patch_mode;
  }

  if (sb->material_set) {
    sb->material_valid = dev->material_valid;
    sb->material = dev->material;
  }
  for (uint32_t i = 0; i < Device::kMaxLights; ++i) {
    if (sb->light_mask.test(i)) {
      sb->lights[i] = dev->lights[i];
      if (dev->light_valid[i]) {
        sb->light_valid_bits.set(i);
      } else {
        sb->light_valid_bits.reset(i);
      }
    }
    if (sb->light_enable_mask.test(i)) {
      if (dev->light_enabled[i]) {
        sb->light_enable_bits.set(i);
      } else {
        sb->light_enable_bits.reset(i);
      }
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (sb->gamma_ramp_set) {
    sb->gamma_ramp_valid = dev->gamma_ramp_valid;
    sb->gamma_ramp = dev->gamma_ramp;
  }

  if (sb->clip_status_set) {
    sb->clip_status_valid = dev->clip_status_valid;
    sb->clip_status = dev->clip_status;
  }

  for (uint32_t p = 0; p < Device::kMaxPalettes; ++p) {
    if (!sb->palette_mask.test(p)) {
      continue;
    }
    std::memcpy(sb->palette_entries[p].data(), &dev->palette_entries[p][0], sizeof(dev->palette_entries[p]));
    if (dev->palette_valid[p]) {
      sb->palette_valid_bits.set(p);
    } else {
      sb->palette_valid_bits.reset(p);
    }
  }

  if (sb->current_texture_palette_set) {
    sb->current_texture_palette = dev->current_texture_palette;
  }
#endif

  for (uint32_t idx = 0; idx < 16u * 16u; ++idx) {
    if (sb->sampler_state_mask.test(idx)) {
      const uint32_t stage = idx / 16u;
      const uint32_t s = idx % 16u;
      sb->sampler_state_values[idx] = dev->sampler_states[stage][s];
    }
  }

  for (uint32_t idx = 0; idx < 16u * 256u; ++idx) {
    if (sb->texture_stage_state_mask.test(idx)) {
      const uint32_t stage = idx / 256u;
      const uint32_t s = idx % 256u;
      sb->texture_stage_state_values[idx] = dev->texture_stage_states[stage][s];
    }
  }

  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (sb->texture_mask.test(stage)) {
      sb->textures[stage] = dev->textures[stage];
    }
  }

  for (uint32_t i = 0; i < 4; ++i) {
    if (sb->render_target_mask.test(i)) {
      sb->render_targets[i] = dev->render_targets[i];
    }
  }
  if (sb->depth_stencil_set) {
    sb->depth_stencil = dev->depth_stencil;
  }

  if (sb->viewport_set) {
    sb->viewport = viewport_effective_locked(dev);
  }
  if (sb->scissor_set) {
    sb->scissor_rect = scissor_rect_effective_locked(dev);
    sb->scissor_rect_user_set = dev->scissor_rect_user_set;
    sb->scissor_enabled = dev->scissor_enabled;
  }

  if (sb->vertex_decl_set) {
    sb->vertex_decl = dev->vertex_decl;
  }
  if (sb->fvf_set) {
    sb->fvf = dev->fvf;
  }

  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (sb->stream_mask.test(stream)) {
      sb->streams[stream] = dev->streams[stream];
    }
    if (sb->stream_source_freq_mask.test(stream)) {
      sb->stream_source_freq_values[stream] = dev->stream_source_freq[stream];
    }
  }

  if (sb->index_buffer_set) {
    sb->index_buffer = dev->index_buffer;
    sb->index_format = dev->index_format;
    sb->index_offset_bytes = dev->index_offset_bytes;
  }

  if (sb->user_vs_set) {
    sb->user_vs = dev->user_vs;
  }
  if (sb->user_ps_set) {
    sb->user_ps = dev->user_ps;
  }

  for (uint32_t r = 0; r < 256; ++r) {
    if (sb->vs_const_mask.test(r)) {
      std::memcpy(sb->vs_consts.data() + static_cast<size_t>(r) * 4,
                  dev->vs_consts_f + static_cast<size_t>(r) * 4,
                  4 * sizeof(float));
    }
    if (sb->ps_const_mask.test(r)) {
      std::memcpy(sb->ps_consts.data() + static_cast<size_t>(r) * 4,
                  dev->ps_consts_f + static_cast<size_t>(r) * 4,
                  4 * sizeof(float));
    }
    if (sb->vs_const_i_mask.test(r)) {
      std::memcpy(sb->vs_consts_i.data() + static_cast<size_t>(r) * 4,
                  dev->vs_consts_i + static_cast<size_t>(r) * 4,
                  4 * sizeof(int32_t));
    }
    if (sb->ps_const_i_mask.test(r)) {
      std::memcpy(sb->ps_consts_i.data() + static_cast<size_t>(r) * 4,
                  dev->ps_consts_i + static_cast<size_t>(r) * 4,
                  4 * sizeof(int32_t));
    }
    if (sb->vs_const_b_mask.test(r)) {
      sb->vs_consts_b[r] = dev->vs_consts_b[r];
    }
    if (sb->ps_const_b_mask.test(r)) {
      sb->ps_consts_b[r] = dev->ps_consts_b[r];
    }
  }
}

static HRESULT stateblock_apply_locked(Device* dev, const StateBlock* sb) {
  if (!dev || !sb) {
    return E_INVALIDARG;
  }

  // Render targets / depth-stencil first.
  if (sb->render_target_mask.any() || sb->depth_stencil_set) {
    Resource* old_rts[4] = {dev->render_targets[0], dev->render_targets[1], dev->render_targets[2], dev->render_targets[3]};
    Resource* old_ds = dev->depth_stencil;

    for (uint32_t slot = 0; slot < 4; ++slot) {
      if (!sb->render_target_mask.test(slot)) {
        continue;
      }

      Resource* rt = sb->render_targets[slot];
      if (rt && slot > 0) {
        for (uint32_t i = 0; i < slot; ++i) {
          if (!dev->render_targets[i]) {
            return kD3DErrInvalidCall;
          }
        }
      }

      dev->render_targets[slot] = rt;
      if (!rt) {
        // Maintain contiguity: clearing an earlier slot implicitly clears any
        // later slots.
        for (uint32_t i = slot + 1; i < 4; ++i) {
          dev->render_targets[i] = nullptr;
        }
      }
    }

    if (sb->depth_stencil_set) {
      dev->depth_stencil = sb->depth_stencil;
    }

    bool changed = (dev->depth_stencil != old_ds);
    for (uint32_t i = 0; i < 4 && !changed; ++i) {
      changed = (dev->render_targets[i] != old_rts[i]);
    }

    if (changed) {
      if (!emit_set_render_targets_locked(dev)) {
        dev->depth_stencil = old_ds;
        for (uint32_t i = 0; i < 4; ++i) {
          dev->render_targets[i] = old_rts[i];
        }
        return E_OUTOFMEMORY;
      }
    }

    for (uint32_t i = 0; i < 4; ++i) {
      if (sb->render_target_mask.test(i)) {
        stateblock_record_render_target_locked(dev, i, dev->render_targets[i]);
      }
    }
    if (sb->depth_stencil_set) {
      stateblock_record_depth_stencil_locked(dev, dev->depth_stencil);
    }
  }

  if (sb->viewport_set) {
    dev->viewport = sb->viewport;
    auto* cmd = append_fixed_locked<aerogpu_cmd_set_viewport>(dev, AEROGPU_CMD_SET_VIEWPORT);
    if (!cmd) {
      return E_OUTOFMEMORY;
    }
    cmd->x_f32 = f32_bits(sb->viewport.X);
    cmd->y_f32 = f32_bits(sb->viewport.Y);
    cmd->width_f32 = f32_bits(sb->viewport.Width);
    cmd->height_f32 = f32_bits(sb->viewport.Height);
    cmd->min_depth_f32 = f32_bits(sb->viewport.MinZ);
    cmd->max_depth_f32 = f32_bits(sb->viewport.MaxZ);
    stateblock_record_viewport_locked(dev, dev->viewport);
  }

  if (sb->scissor_set) {
    dev->scissor_rect = sb->scissor_rect;
    dev->scissor_enabled = sb->scissor_enabled;
    dev->scissor_rect_user_set = sb->scissor_rect_user_set;
    scissor_fixup_unset_rect_locked(dev);

    // Keep D3DRS_SCISSORTESTENABLE (174) in sync with the dedicated scissor state
    // so GetRenderState and state blocks observe consistent values.
    constexpr uint32_t kD3dRsScissorTestEnable = 174u;
    dev->render_states[kD3dRsScissorTestEnable] = dev->scissor_enabled ? 1u : 0u;
    stateblock_record_render_state_locked(dev, kD3dRsScissorTestEnable, dev->render_states[kD3dRsScissorTestEnable]);

    int32_t x = 0;
    int32_t y = 0;
    int32_t w = 0x7FFFFFFF;
    int32_t h = 0x7FFFFFFF;
    if (dev->scissor_enabled) {
      x = static_cast<int32_t>(dev->scissor_rect.left);
      y = static_cast<int32_t>(dev->scissor_rect.top);
      w = static_cast<int32_t>(dev->scissor_rect.right - dev->scissor_rect.left);
      h = static_cast<int32_t>(dev->scissor_rect.bottom - dev->scissor_rect.top);
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_set_scissor>(dev, AEROGPU_CMD_SET_SCISSOR);
    if (!cmd) {
      return E_OUTOFMEMORY;
    }
    cmd->x = x;
    cmd->y = y;
    cmd->width = w;
    cmd->height = h;
    stateblock_record_scissor_locked(dev, dev->scissor_rect, dev->scissor_enabled);
  }

  // Render states.
  for (uint32_t i = 0; i < 256; ++i) {
    if (!sb->render_state_mask.test(i)) {
      continue;
    }
    dev->render_states[i] = sb->render_state_values[i];

    // Fixed-function lighting uses a reserved high VS constant range. Keep the
    // dirty bit in sync for state-block-applied lighting render states so the
    // next fixed-function draw re-uploads the constants.
    constexpr uint32_t kD3dRsAmbient = 26u;   // D3DRS_AMBIENT
    constexpr uint32_t kD3dRsLighting = 137u; // D3DRS_LIGHTING
    if (i == kD3dRsAmbient || i == kD3dRsLighting) {
      dev->fixedfunc_lighting_dirty = true;
    }

    // SCISSORTESTENABLE (174) is consumed via the dedicated scissor packet. When
    // it is applied via a state block, propagate the enable bit to the scissor
    // state and emit an updated scissor packet using the current scissor rect.
    if (i == 174u) {
      const BOOL enabled = sb->render_state_values[i] ? TRUE : FALSE;
      if ((dev->scissor_enabled ? TRUE : FALSE) != (enabled ? TRUE : FALSE)) {
        dev->scissor_enabled = enabled;
        scissor_fixup_unset_rect_locked(dev);

        int32_t x = 0;
        int32_t y = 0;
        int32_t w = 0x7FFFFFFF;
        int32_t h = 0x7FFFFFFF;
        if (dev->scissor_enabled) {
          const RECT& rect = dev->scissor_rect;
          x = static_cast<int32_t>(rect.left);
          y = static_cast<int32_t>(rect.top);
          w = static_cast<int32_t>(rect.right - rect.left);
          h = static_cast<int32_t>(rect.bottom - rect.top);
        }

        auto* sc = append_fixed_locked<aerogpu_cmd_set_scissor>(dev, AEROGPU_CMD_SET_SCISSOR);
        if (!sc) {
          return E_OUTOFMEMORY;
        }
        sc->x = x;
        sc->y = y;
        sc->width = w;
        sc->height = h;
        stateblock_record_scissor_locked(dev, dev->scissor_rect, dev->scissor_enabled);
      }
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_set_render_state>(dev, AEROGPU_CMD_SET_RENDER_STATE);
    if (!cmd) {
      return E_OUTOFMEMORY;
    }
    cmd->state = i;
    cmd->value = sb->render_state_values[i];
    stateblock_record_render_state_locked(dev, i, sb->render_state_values[i]);
  }

  // Fixed-function state: transforms. Cached for GetTransform/state blocks.
  //
  // WORLD/VIEW/PROJECTION are consumed by bring-up fixed-function paths:
  // - untransformed `D3DFVF_XYZ*`: internal WVP VS variants and a reserved high VS
  //   constant range (c240..c243) uploaded by `ensure_fixedfunc_wvp_constants_locked()`.
  // - the fixed-function `ProcessVertices` subset.
  //
  // Pre-transformed `D3DFVF_XYZRHW*` draws do not use WORLD/VIEW/PROJECTION; they
  // use a viewport-based draw-time `XYZRHW` -> clip-space conversion.
  if (sb->transform_mask.any()) {
    for (uint32_t t = 0; t < Device::kTransformCacheCount; ++t) {
      if (!sb->transform_mask.test(t)) {
        continue;
      }
      std::memcpy(dev->transform_matrices[t],
                  sb->transform_values.data() + static_cast<size_t>(t) * 16u,
                  sizeof(float) * 16u);
      if (t == kD3dTransformWorld0 || t == kD3dTransformView || t == kD3dTransformProjection) {
        dev->fixedfunc_matrix_dirty = true;
      }
      if (t == kD3dTransformWorld0 || t == kD3dTransformView) {
        dev->fixedfunc_lighting_dirty = true;
      }
      stateblock_record_transform_locked(dev, t, dev->transform_matrices[t]);
    }
  }

  // Additional fixed-function cached state (clip planes, software vertex processing, N-patches).
  for (uint32_t p = 0; p < 6; ++p) {
    if (!sb->clip_plane_mask.test(p)) {
      continue;
    }
    std::memcpy(dev->clip_planes[p],
                sb->clip_plane_values.data() + static_cast<size_t>(p) * 4u,
                sizeof(float) * 4u);
    stateblock_record_clip_plane_locked(dev, p, dev->clip_planes[p]);
  }
  if (sb->software_vertex_processing_set) {
    dev->software_vertex_processing = sb->software_vertex_processing;
    stateblock_record_software_vertex_processing_locked(dev, dev->software_vertex_processing);
  }
  if (sb->n_patch_mode_set) {
    dev->n_patch_mode = sb->n_patch_mode;
    stateblock_record_npatch_mode_locked(dev, dev->n_patch_mode);
  }

  if (sb->material_set) {
    dev->material = sb->material;
    dev->material_valid = sb->material_valid;
    dev->fixedfunc_lighting_dirty = true;
    stateblock_record_material_locked(dev, dev->material, dev->material_valid);
  }
  for (uint32_t i = 0; i < Device::kMaxLights; ++i) {
    if (sb->light_mask.test(i)) {
      dev->lights[i] = sb->lights[i];
      dev->light_valid[i] = sb->light_valid_bits.test(i);
      dev->fixedfunc_lighting_dirty = true;
      stateblock_record_light_locked(dev, i, dev->lights[i], dev->light_valid[i]);
    }
    if (sb->light_enable_mask.test(i)) {
      dev->light_enabled[i] = sb->light_enable_bits.test(i) ? TRUE : FALSE;
      dev->fixedfunc_lighting_dirty = true;
      stateblock_record_light_enable_locked(dev, i, dev->light_enabled[i]);
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (sb->gamma_ramp_set) {
    dev->gamma_ramp = sb->gamma_ramp;
    dev->gamma_ramp_valid = sb->gamma_ramp_valid;
    stateblock_record_gamma_ramp_locked(dev, dev->gamma_ramp, dev->gamma_ramp_valid);
  }

  if (sb->clip_status_set) {
    dev->clip_status = sb->clip_status;
    dev->clip_status_valid = sb->clip_status_valid;
    stateblock_record_clip_status_locked(dev, dev->clip_status, dev->clip_status_valid);
  }

  for (uint32_t p = 0; p < Device::kMaxPalettes; ++p) {
    if (!sb->palette_mask.test(p)) {
      continue;
    }
    std::memcpy(&dev->palette_entries[p][0], sb->palette_entries[p].data(), sizeof(dev->palette_entries[p]));
    dev->palette_valid[p] = sb->palette_valid_bits.test(p);
    stateblock_record_palette_entries_locked(dev, p, &dev->palette_entries[p][0], dev->palette_valid[p]);
  }

  if (sb->current_texture_palette_set) {
    dev->current_texture_palette = sb->current_texture_palette;
    stateblock_record_current_texture_palette_locked(dev, dev->current_texture_palette);
  }
#endif

  // Samplers/textures.
  bool fixedfunc_ps_dirty = false;
  for (uint32_t stage = 0; stage < 16; ++stage) {
    if (sb->texture_mask.test(stage)) {
      Resource* tex = sb->textures[stage];
      dev->textures[stage] = tex;
      auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
      cmd->slot = stage;
      cmd->texture = tex ? tex->handle : 0;
      cmd->reserved0 = 0;
      stateblock_record_texture_locked(dev, stage, tex);
      if (stage < kFixedfuncMaxTextureStages) {
        fixedfunc_ps_dirty = true;
      }
    }

    for (uint32_t s = 0; s < 16; ++s) {
      const uint32_t idx = stage * 16u + s;
      if (!sb->sampler_state_mask.test(idx)) {
        continue;
      }
      const uint32_t value = sb->sampler_state_values[idx];
      dev->sampler_states[stage][s] = value;
      auto* cmd = append_fixed_locked<aerogpu_cmd_set_sampler_state>(dev, AEROGPU_CMD_SET_SAMPLER_STATE);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
      cmd->slot = stage;
      cmd->state = s;
      cmd->value = value;
      stateblock_record_sampler_state_locked(dev, stage, s, value);
    }

    // Texture stage state (fixed function). No direct command emission; values
    // are cached so GetTextureStageState + state blocks are deterministic, and
    // stages 0..3 are consulted by the UMD's fixed-function fallback path to
    // select/synthesize a pixel shader variant.
    for (uint32_t s = 0; s < 256; ++s) {
      const uint32_t idx = stage * 256u + s;
      if (!sb->texture_stage_state_mask.test(idx)) {
        continue;
      }
      const uint32_t value = sb->texture_stage_state_values[idx];
      dev->texture_stage_states[stage][s] = value;
      stateblock_record_texture_stage_state_locked(dev, stage, s, value);
      if (stage < kFixedfuncMaxTextureStages &&
          (s == kD3dTssColorOp || s == kD3dTssColorArg1 || s == kD3dTssColorArg2 ||
           s == kD3dTssAlphaOp || s == kD3dTssAlphaArg1 || s == kD3dTssAlphaArg2)) {
        fixedfunc_ps_dirty = true;
      }
    }
  }

  // Input layout / FVF.
  if (sb->vertex_decl_set) {
    if (!emit_set_input_layout_locked(dev, sb->vertex_decl)) {
      return E_OUTOFMEMORY;
    }
  }
  if (sb->fvf_set) {
    const uint32_t old_fvf = dev->fvf;
    dev->fvf = sb->fvf;
    // Switching to a WVP fixed-function path must refresh the reserved matrix
    // constant range, even if transforms did not change. State-block apply
    // bypasses SetFVF/SetVertexDecl, so handle the transition here as well.
    if (old_fvf != dev->fvf && fixedfunc_fvf_needs_matrix(dev->fvf)) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (old_fvf != dev->fvf && fixedfunc_fvf_has_normal(dev->fvf)) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  if (sb->vertex_decl_set || sb->fvf_set) {
    stateblock_record_vertex_decl_locked(dev, dev->vertex_decl, dev->fvf);
  }

  // Stream source frequency (instancing). Cached for Get*/state blocks and
  // consulted by the CPU instancing expansion path when non-default.
  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (!sb->stream_source_freq_mask.test(stream)) {
      continue;
    }
    dev->stream_source_freq[stream] = sb->stream_source_freq_values[stream];
    stateblock_record_stream_source_freq_locked(dev, stream, dev->stream_source_freq[stream]);
  }

  // VB streams.
  for (uint32_t stream = 0; stream < 16; ++stream) {
    if (!sb->stream_mask.test(stream)) {
      continue;
    }
    const DeviceStateStream& ss = sb->streams[stream];
    if (!emit_set_stream_source_locked(dev, stream, ss.vb, ss.offset_bytes, ss.stride_bytes)) {
      return E_OUTOFMEMORY;
    }
    stateblock_record_stream_source_locked(dev, stream, dev->streams[stream]);
  }

  // Index buffer.
  if (sb->index_buffer_set) {
    dev->index_buffer = sb->index_buffer;
    dev->index_format = sb->index_format;
    dev->index_offset_bytes = sb->index_offset_bytes;
    stateblock_record_index_buffer_locked(dev, dev->index_buffer, dev->index_format, dev->index_offset_bytes);

    auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (!cmd) {
      return E_OUTOFMEMORY;
    }
    cmd->buffer = dev->index_buffer ? dev->index_buffer->handle : 0;
    cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
    cmd->offset_bytes = dev->index_offset_bytes;
    cmd->reserved0 = 0;
  }

  // Shaders.
  bool shaders_dirty = false;
  if (sb->user_vs_set && dev->user_vs != sb->user_vs) {
    dev->user_vs = sb->user_vs;
    shaders_dirty = true;
    // If the app binds a user VS while a WVP fixed-function path is active,
    // treat the reserved WVP constant range as clobbered; the app may have
    // written overlapping constants before returning to fixed-function.
    if (dev->user_vs && fixedfunc_fvf_needs_matrix(dev->fvf)) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (dev->user_vs && fixedfunc_fvf_has_normal(dev->fvf)) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  if (sb->user_ps_set && dev->user_ps != sb->user_ps) {
    dev->user_ps = sb->user_ps;
    shaders_dirty = true;
  }

  // If ApplyStateBlock is invoked while Begin/EndStateBlock recording is active,
  // we must record the shader bindings even when they are already bound (no-op
  // apply). Otherwise, the recorded state block would omit shader state and
  // would not reproduce the intended bindings when applied later.
  if (sb->user_vs_set) {
    stateblock_record_shader_locked(dev, kD3d9ShaderStageVs, dev->user_vs);
  }
  if (sb->user_ps_set) {
    stateblock_record_shader_locked(dev, kD3d9ShaderStagePs, dev->user_ps);
  }
  if (shaders_dirty) {
    const HRESULT bind_hr = ensure_shader_bindings_locked(dev, /*strict_draw_validation=*/false);
    if (FAILED(bind_hr)) {
      return bind_hr;
    }
  }

  // Shader constants.
  auto apply_consts = [&](uint32_t stage,
                          const std::bitset<256>& mask,
                          const float* src,
                          float* dst) -> HRESULT {
    uint32_t reg = 0;
    while (reg < 256) {
      if (!mask.test(reg)) {
        ++reg;
        continue;
      }
      uint32_t start = reg;
      uint32_t end = reg;
      while (end + 1 < 256 && mask.test(end + 1)) {
        ++end;
      }
      const uint32_t count = (end - start + 1);

      // If the state block writes to the reserved WVP constant range, mark it
      // dirty so the next fixed-function draw re-uploads the correct matrix.
      if (stage == kD3d9ShaderStageVs && fixedfunc_fvf_needs_matrix(dev->fvf)) {
        const uint32_t write_end = start + count;
        const uint32_t ff_start = kFixedfuncMatrixStartRegister;
        const uint32_t ff_end = kFixedfuncMatrixStartRegister + kFixedfuncMatrixVec4Count;
        if (start < ff_end && write_end > ff_start) {
          dev->fixedfunc_matrix_dirty = true;
        }
      }
      if (stage == kD3d9ShaderStageVs && fixedfunc_fvf_has_normal(dev->fvf)) {
        const uint32_t write_end = start + count;
        const uint32_t ff_start = kFixedfuncLightingStartRegister;
        const uint32_t ff_end = kFixedfuncLightingStartRegister + kFixedfuncLightingVec4Count;
        if (start < ff_end && write_end > ff_start) {
          dev->fixedfunc_lighting_dirty = true;
        }
      }

      const float* payload = src + static_cast<size_t>(start) * 4;
      const size_t payload_size = static_cast<size_t>(count) * 4 * sizeof(float);

      // Always record into the in-progress state block (if any), even if the
      // constant data is redundant.
      stateblock_record_shader_const_f_locked(dev, stage, start, payload, count);

      if (std::memcmp(dst + static_cast<size_t>(start) * 4, payload, payload_size) == 0) {
        // Skip redundant constant uploads: applying identical state again is a
        // no-op, but still recorded above for state blocks.
        // Leave `dst` unchanged (it already matches).
        reg = end + 1;
        continue;
      }

      std::memcpy(dst + static_cast<size_t>(start) * 4,
                  payload,
                  payload_size);

      auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_f>(
          dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_F, payload, payload_size);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->stage = d3d9_stage_to_aerogpu_stage(stage);
      cmd->start_register = start;
      cmd->vec4_count = count;
      cmd->reserved0 = 0;

      reg = end + 1;
    }
    return S_OK;
  };

  if (sb->vs_const_mask.any()) {
    const HRESULT hr = apply_consts(kD3d9ShaderStageVs, sb->vs_const_mask, sb->vs_consts.data(), dev->vs_consts_f);
    if (FAILED(hr)) {
      return hr;
    }
    // State blocks can capture and re-apply VS constant registers even when the
    // app uses fixed-function rendering. If the fixed-function fallback consumes any
    // reserved VS constant ranges (WVP/lighting), treat them as clobbered and re-upload
    // on the next draw.
    if (fixedfunc_fvf_needs_matrix(dev->fvf)) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (fixedfunc_fvf_has_normal(dev->fvf)) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  if (sb->ps_const_mask.any()) {
    const HRESULT hr = apply_consts(kD3d9ShaderStagePs, sb->ps_const_mask, sb->ps_consts.data(), dev->ps_consts_f);
    if (FAILED(hr)) {
      return hr;
    }
  }

  // Shader int/bool constants.
  //
  // These mirror D3D9's per-stage `i#`/`b#` constant register files and must be
  // emitted into the AeroGPU command stream so state blocks restore them
  // deterministically (matching float constant behavior above).
  auto apply_consts_i = [&](uint32_t stage,
                            const std::bitset<256>& mask,
                            const int32_t* src,
                            int32_t* dst) -> HRESULT {
    uint32_t reg = 0;
    while (reg < 256) {
      if (!mask.test(reg)) {
        ++reg;
        continue;
      }
      uint32_t start = reg;
      uint32_t end = reg;
      while (end + 1 < 256 && mask.test(end + 1)) {
        ++end;
      }
      const uint32_t count = (end - start + 1);
      const size_t base = static_cast<size_t>(start) * 4;
      const size_t payload_size = static_cast<size_t>(count) * 4 * sizeof(int32_t);

      const int32_t* payload = src + base;

      // Always record into the in-progress state block (if any), even if the
      // constant data is redundant.
      stateblock_record_shader_const_i_locked(dev, stage, start, payload, count);

      if (std::memcmp(dst + base, payload, payload_size) == 0) {
        // Skip redundant constant uploads: applying identical state again is a
        // no-op, but still recorded above for state blocks.
        // Leave `dst` unchanged (it already matches).
        reg = end + 1;
        continue;
      }

      std::memcpy(dst + base, payload, payload_size);

      auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_i>(
          dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_I, payload, payload_size);
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->stage = d3d9_stage_to_aerogpu_stage(stage);
      cmd->start_register = start;
      cmd->vec4_count = count;
      cmd->reserved0 = 0;
      reg = end + 1;
    }
    return S_OK;
  };

  auto apply_consts_b = [&](uint32_t stage,
                            const std::bitset<256>& mask,
                            const uint8_t* src,
                            uint8_t* dst) -> HRESULT {
    uint32_t reg = 0;
    while (reg < 256) {
      if (!mask.test(reg)) {
        ++reg;
        continue;
      }
      uint32_t start = reg;
      uint32_t end = reg;
      while (end + 1 < 256 && mask.test(end + 1)) {
        ++end;
      }
      const uint32_t count = (end - start + 1);

      // Always record into the in-progress state block (if any), even if the
      // constant data is redundant.
      stateblock_record_shader_const_b_locked(dev, stage, start, src + start, count);

      if (std::memcmp(dst + start, src + start, static_cast<size_t>(count) * sizeof(uint8_t)) != 0) {
        std::memcpy(dst + start, src + start, static_cast<size_t>(count) * sizeof(uint8_t));
      } else {
        // Skip redundant constant uploads: applying identical state again is a
        // no-op, but still recorded above for state blocks.
        // Leave `dst` unchanged (it already matches).
        reg = end + 1;
        continue;
      }

      std::array<uint32_t, 256> payload{};
      for (uint32_t i = 0; i < count; ++i) {
        payload[i] = src[start + i] ? 1u : 0u;
      }
      auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_b>(
          dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_B, payload.data(), static_cast<size_t>(count) * sizeof(uint32_t));
      if (!cmd) {
        return E_OUTOFMEMORY;
      }
      cmd->stage = d3d9_stage_to_aerogpu_stage(stage);
      cmd->start_register = start;
      cmd->bool_count = count;
      cmd->reserved0 = 0;
      reg = end + 1;
    }
    return S_OK;
  };

  if (sb->vs_const_i_mask.any()) {
    const HRESULT hr = apply_consts_i(kD3d9ShaderStageVs, sb->vs_const_i_mask, sb->vs_consts_i.data(), dev->vs_consts_i);
    if (FAILED(hr)) {
      return hr;
    }
  }
  if (sb->ps_const_i_mask.any()) {
    const HRESULT hr = apply_consts_i(kD3d9ShaderStagePs, sb->ps_const_i_mask, sb->ps_consts_i.data(), dev->ps_consts_i);
    if (FAILED(hr)) {
      return hr;
    }
  }
  if (sb->vs_const_b_mask.any()) {
    const HRESULT hr = apply_consts_b(kD3d9ShaderStageVs, sb->vs_const_b_mask, sb->vs_consts_b.data(), dev->vs_consts_b);
    if (FAILED(hr)) {
      return hr;
    }
  }
  if (sb->ps_const_b_mask.any()) {
    const HRESULT hr = apply_consts_b(kD3d9ShaderStagePs, sb->ps_const_b_mask, sb->ps_consts_b.data(), dev->ps_consts_b);
    if (FAILED(hr)) {
      return hr;
    }
  }

  // Texture stage state drives the fixed-function PS selection logic (and thus
  // shader-stage interop when the app binds only a VS).
  //
  // State-block apply bypasses SetTextureStageState/SetTexture, so keep the
  // cached fixed-function PS selection in sync and re-bind if the currently bound
  // PS changes.
  //
  // Also handle D3DRS_TEXTUREFACTOR updates: state blocks often include
  // TEXTUREFACTOR without changing stage state, so the fixed-function PS constant
  // (c255) must be refreshed when needed.
  constexpr uint32_t kD3dRsTextureFactor = 60u; // D3DRS_TEXTUREFACTOR
  const bool tfactor_dirty = sb->render_state_mask.test(kD3dRsTextureFactor);
  FixedfuncPixelShaderKey ps_key{};
  if (!dev->user_ps && (fixedfunc_ps_dirty || tfactor_dirty)) {
    ps_key = fixedfunc_ps_key_locked(dev);
    if (tfactor_dirty && ps_key.supported && ps_key.uses_tfactor) {
      const HRESULT hr = ensure_fixedfunc_texture_factor_constant_locked(dev);
      if (FAILED(hr)) {
        return hr;
      }
    }
  }
  if (fixedfunc_ps_dirty && !dev->user_ps) {
    Shader** ps_slot = nullptr;
    if (dev->user_vs) {
      // VS-only shader-stage interop uses the fixed-function PS fallback.
      ps_slot = &dev->fixedfunc_ps_interop;
    } else {
      const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
      if (variant != FixedFuncVariant::NONE) {
        ps_slot = &dev->fixedfunc_pipelines[static_cast<size_t>(variant)].ps;
      } else {
        // Unsupported fixed-function FVF still uses a non-null shader bind pair.
        // The fallback PS selection logic is driven by texture stage state (same as VS-only interop).
        ps_slot = &dev->fixedfunc_ps_interop;
      }
    }

    // Texture stage-state selection is guarded: if the app applied an unsupported
    // stage-state combination via a state block, tolerate the apply and fail
    // draws (not ApplyStateBlock) with INVALIDCALL.
    if (ps_slot && ps_key.supported) {
      const HRESULT hr = ensure_fixedfunc_pixel_shader_locked(dev, ps_slot);
      if (FAILED(hr)) {
        return hr;
      }
    }
  }

  // If fixed-function WVP rendering is active, ensure any state-block-applied
  // transform changes are reflected in the internal VS constant registers.
  if (!dev->user_vs && !dev->user_ps && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }
  }

  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_begin_state_block(D3DDDI_HDEVICE hDevice) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceBeginStateBlock, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), 0, 0, 0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  if (dev->recording_state_block) {
    return trace.ret(kD3DErrInvalidCall);
  }

  try {
    dev->recording_state_block = new StateBlock();
  } catch (...) {
    dev->recording_state_block = nullptr;
    return trace.ret(E_OUTOFMEMORY);
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_end_state_block(D3DDDI_HDEVICE hDevice, D3D9DDI_HSTATEBLOCK* phStateBlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceEndStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(phStateBlock),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !phStateBlock) {
    return trace.ret(E_INVALIDARG);
  }
  phStateBlock->pDrvPrivate = nullptr;

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  if (!dev->recording_state_block) {
    return trace.ret(kD3DErrInvalidCall);
  }

  phStateBlock->pDrvPrivate = dev->recording_state_block;
  dev->recording_state_block = nullptr;
  return trace.ret(S_OK);
}

template <typename CreateArgsT>
HRESULT device_create_state_block_from_args(D3DDDI_HDEVICE hDevice, CreateArgsT* pCreateStateBlock) {
  uint32_t type_u32 = 1u; // D3DSBT_ALL
  if (pCreateStateBlock) {
    if constexpr (aerogpu_d3d9_has_member_StateBlockType<CreateArgsT>::value) {
      type_u32 = static_cast<uint32_t>(pCreateStateBlock->StateBlockType);
    } else if constexpr (aerogpu_d3d9_has_member_Type<CreateArgsT>::value) {
      type_u32 = static_cast<uint32_t>(pCreateStateBlock->Type);
    } else if constexpr (aerogpu_d3d9_has_member_type<CreateArgsT>::value) {
      type_u32 = static_cast<uint32_t>(pCreateStateBlock->type);
    }
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCreateStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type_u32),
                      d3d9_trace_arg_ptr(pCreateStateBlock),
                      0);
  if (!hDevice.pDrvPrivate || !pCreateStateBlock) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  // Resolve the output handle field.
  if constexpr (!aerogpu_d3d9_has_member_hStateBlock<CreateArgsT>::value) {
    return trace.ret(E_INVALIDARG);
  }

  pCreateStateBlock->hStateBlock.pDrvPrivate = nullptr;

  auto sb = make_unique_nothrow<StateBlock>();
  if (!sb) {
    return trace.ret(E_OUTOFMEMORY);
  }

  stateblock_init_for_type_locked(dev, sb.get(), type_u32);
  pCreateStateBlock->hStateBlock.pDrvPrivate = sb.release();
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_create_state_block(D3DDDI_HDEVICE hDevice,
                                                    uint32_t type_u32,
                                                    D3D9DDI_HSTATEBLOCK* phStateBlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCreateStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type_u32),
                      d3d9_trace_arg_ptr(phStateBlock),
                      0);
  if (!hDevice.pDrvPrivate || !phStateBlock) {
    return trace.ret(E_INVALIDARG);
  }
  phStateBlock->pDrvPrivate = nullptr;

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  auto sb = make_unique_nothrow<StateBlock>();
  if (!sb) {
    return trace.ret(E_OUTOFMEMORY);
  }

  stateblock_init_for_type_locked(dev, sb.get(), type_u32);
  phStateBlock->pDrvPrivate = sb.release();
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_delete_state_block(D3DDDI_HDEVICE hDevice, D3D9DDI_HSTATEBLOCK hStateBlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDeleteStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hStateBlock.pDrvPrivate),
                      0,
                      0);
  (void)as_device(hDevice);
  delete as_state_block(hStateBlock);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_capture_state_block(D3DDDI_HDEVICE hDevice, D3D9DDI_HSTATEBLOCK hStateBlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCaptureStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hStateBlock.pDrvPrivate),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* sb = as_state_block(hStateBlock);
  if (!sb) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  stateblock_capture_locked(dev, sb);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_apply_state_block(D3DDDI_HDEVICE hDevice, D3D9DDI_HSTATEBLOCK hStateBlock) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceApplyStateBlock,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hStateBlock.pDrvPrivate),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* sb = as_state_block(hStateBlock);
  if (!sb) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  return trace.ret(stateblock_apply_locked(dev, sb));
}

template <typename ValidateArgsT>
HRESULT device_validate_device_from_args(D3DDDI_HDEVICE hDevice, ValidateArgsT* pValidateDevice) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceValidateDevice,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pValidateDevice),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pValidateDevice) {
    return trace.ret(E_INVALIDARG);
  }

  // Conservative: we currently report a single pass for the supported shader
  // pipeline. Unknown/legacy state is forwarded to the host, which may choose
  // to emulate it.
  if constexpr (aerogpu_d3d9_has_member_pNumPasses<ValidateArgsT>::value) {
    if (pValidateDevice->pNumPasses) {
      *pValidateDevice->pNumPasses = 1;
    }
  } else if constexpr (aerogpu_d3d9_has_member_NumPasses<ValidateArgsT>::value) {
    pValidateDevice->NumPasses = 1;
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_validate_device(D3DDDI_HDEVICE hDevice, uint32_t* pNumPasses) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceValidateDevice,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pNumPasses),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pNumPasses) {
    return trace.ret(E_INVALIDARG);
  }
  *pNumPasses = 1;
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_create_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_create_state_block_from_args(args...);
  } else if constexpr (sizeof...(Args) == 3) {
    return device_create_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename... Args>
HRESULT device_delete_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_delete_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename... Args>
HRESULT device_capture_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_capture_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename... Args>
HRESULT device_apply_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_apply_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename... Args>
HRESULT device_begin_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 1) {
    return device_begin_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename... Args>
HRESULT device_end_state_block_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_end_state_block(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename T>
HRESULT device_validate_device_dispatch(D3DDDI_HDEVICE hDevice, T* arg) {
  // Could be either `DWORD*` or a ValidateDevice args struct pointer.
  if constexpr (std::is_integral_v<T>) {
    return device_validate_device(hDevice, reinterpret_cast<uint32_t*>(arg));
  } else {
    return device_validate_device_from_args(hDevice, arg);
  }
}

template <typename... Args>
HRESULT device_validate_device_dispatch(Args... /*args*/) {
  return D3DERR_NOTAVAILABLE;
}

template <typename Fn>
struct aerogpu_d3d9_impl_pfnCreateStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnCreateStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnCreateStateBlock(Args... args) {
    return static_cast<Ret>(device_create_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnCreateStateBlock<Ret(*)(Args...)> {
  static Ret pfnCreateStateBlock(Args... args) {
    return static_cast<Ret>(device_create_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnDeleteStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnDeleteStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnDeleteStateBlock(Args... args) {
    return static_cast<Ret>(device_delete_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnDeleteStateBlock<Ret(*)(Args...)> {
  static Ret pfnDeleteStateBlock(Args... args) {
    return static_cast<Ret>(device_delete_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnCaptureStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnCaptureStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnCaptureStateBlock(Args... args) {
    return static_cast<Ret>(device_capture_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnCaptureStateBlock<Ret(*)(Args...)> {
  static Ret pfnCaptureStateBlock(Args... args) {
    return static_cast<Ret>(device_capture_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnApplyStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnApplyStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnApplyStateBlock(Args... args) {
    return static_cast<Ret>(device_apply_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnApplyStateBlock<Ret(*)(Args...)> {
  static Ret pfnApplyStateBlock(Args... args) {
    return static_cast<Ret>(device_apply_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnBeginStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnBeginStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnBeginStateBlock(Args... args) {
    return static_cast<Ret>(device_begin_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnBeginStateBlock<Ret(*)(Args...)> {
  static Ret pfnBeginStateBlock(Args... args) {
    return static_cast<Ret>(device_begin_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnEndStateBlock;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnEndStateBlock<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnEndStateBlock(Args... args) {
    return static_cast<Ret>(device_end_state_block_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnEndStateBlock<Ret(*)(Args...)> {
  static Ret pfnEndStateBlock(Args... args) {
    return static_cast<Ret>(device_end_state_block_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnValidateDevice;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnValidateDevice<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnValidateDevice(Args... args) {
    return static_cast<Ret>(device_validate_device_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnValidateDevice<Ret(*)(Args...)> {
  static Ret pfnValidateDevice(Args... args) {
    return static_cast<Ret>(device_validate_device_dispatch(args...));
  }
};

// -----------------------------------------------------------------------------
// Minimal D3D9 "Get*" state DDIs
// -----------------------------------------------------------------------------
// Many D3D9 runtimes can call these (directly or indirectly via state blocks).
// Return the UMD's cached state for the subset we currently track.

template <typename T>
uint32_t d3d9_to_u32(T v) {
  if constexpr (std::is_enum_v<T>) {
    using Under = std::underlying_type_t<T>;
    return static_cast<uint32_t>(static_cast<Under>(v));
  } else if constexpr (std::is_integral_v<T>) {
    return static_cast<uint32_t>(v);
  } else {
    return 0u;
  }
}

template <typename T>
void d3d9_write_u32(T* out, uint32_t v) {
  if (!out) {
    return;
  }
  using OutT = std::remove_reference_t<decltype(*out)>;
  if constexpr (std::is_enum_v<OutT>) {
    *out = static_cast<OutT>(v);
  } else if constexpr (std::is_integral_v<OutT>) {
    *out = static_cast<OutT>(v);
  } else {
    (void)v;
  }
}

template <typename HandleT>
void d3d9_write_handle(HandleT* out, void* pDrvPrivate) {
  if (!out) {
    return;
  }
  out->pDrvPrivate = pDrvPrivate;
}

// -----------------------------------------------------------------------------
// Legacy fixed-function/DDI state (cached; minimal fixed-function consumption)
// -----------------------------------------------------------------------------
// Most fixed-function DDIs are cached-only and are not emitted as explicit
// fixed-function GPU state in the AeroGPU command stream. However, the UMD does
// emulate a minimal fixed-function pipeline:
// - WORLD/VIEW/PROJECTION matrices drive the fixed-function WVP behavior for
//   untransformed `D3DFVF_XYZ*` FVFs: internal WVP VS variants consume WVP
//   columns uploaded into a reserved high VS constant range (`c240..c243`).
//   - When `D3DFVF_DIFFUSE` is omitted and lighting is disabled, the internal VS
//     supplies an opaque white diffuse color.
//   - When `D3DRS_LIGHTING` is enabled for the `D3DFVF_XYZ | D3DFVF_NORMAL{,DIFFUSE}{,TEX1}`
//     bring-up subset, the fixed-function fallback binds a lit VS variant and
//     computes a lit diffuse color (without requiring per-vertex diffuse). It
//     consumes an additional reserved lighting constant block (`c208..c236`)
//     uploaded by `ensure_fixedfunc_lighting_constants_locked()`.
// - Pre-transformed `D3DFVF_XYZRHW*` vertices are converted from screen-space
//   `XYZRHW` (`POSITIONT`) to clip-space on the CPU at draw time
//   (`convert_xyzrhw_to_clipspace_locked()`).
  // - Texture stage state selects a fixed-function PS variant.
//
// Cache state so Set*/Get* and state blocks round-trip for legacy apps that treat
// Get* failures as fatal.

template <typename StageT, typename StateT, typename ValueT>
HRESULT device_set_texture_stage_state_impl(D3DDDI_HDEVICE hDevice, StageT stage, StateT state, ValueT value) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetTextureStageState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(d3d9_to_u32(stage), d3d9_to_u32(state)),
                      static_cast<uint64_t>(d3d9_to_u32(value)),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t ss = d3d9_to_u32(state);
  if (st >= 16 || ss >= 256) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const uint32_t v = d3d9_to_u32(value);
  dev->texture_stage_states[st][ss] = v;
  stateblock_record_texture_stage_state_locked(dev, st, ss, v);

  // If the fixed-function fallback path is active, COLOROP/ALPHAOP changes must
  // affect rendering. Select an internal PS variant that approximates the
  // fixed-function texture combiner for the supported subset.
  if (st < kFixedfuncMaxTextureStages &&
      (ss == kD3dTssColorOp || ss == kD3dTssColorArg1 || ss == kD3dTssColorArg2 ||
       ss == kD3dTssAlphaOp || ss == kD3dTssAlphaArg1 || ss == kD3dTssAlphaArg2) &&
      !dev->user_ps) {
    Shader** ps_slot = nullptr;
    if (dev->user_vs) {
      ps_slot = &dev->fixedfunc_ps_interop;
    } else if (fixedfunc_supported_fvf(dev->fvf)) {
      const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
      if (variant != FixedFuncVariant::NONE) {
        ps_slot = &dev->fixedfunc_pipelines[static_cast<size_t>(variant)].ps;
      }
    }
    if (ps_slot) {
      // Fixed-function stage-state selection is guarded: if the app sets an unsupported
      // stage-state combination, fail draws (not state setting) with INVALIDCALL.
      const FixedfuncPixelShaderKey ps_key = fixedfunc_ps_key_locked(dev);
      if (ps_key.supported) {
        const HRESULT ps_hr = ensure_fixedfunc_pixel_shader_locked(dev, ps_slot);
        if (FAILED(ps_hr)) {
          return trace.ret(ps_hr);
        }
      }
    }
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_texture_stage_state_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_set_texture_stage_state_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

// Portable DDI entrypoint.
//
// WDK builds may require a different signature; those entrypoints call the
// templated dispatch via thunks.
HRESULT AEROGPU_D3D9_CALL device_set_texture_stage_state(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t state,
    uint32_t value) {
  return device_set_texture_stage_state_dispatch(hDevice, stage, state, value);
}

template <typename StageT, typename StateT, typename ValueT>
HRESULT device_get_texture_stage_state_impl(D3DDDI_HDEVICE hDevice, StageT stage, StateT state, ValueT* pValue) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetTextureStageState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(d3d9_to_u32(stage), d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pValue),
                      0);
  d3d9_write_u32(pValue, 0u);
  if (!hDevice.pDrvPrivate || !pValue) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t ss = d3d9_to_u32(state);
  if (st >= 16 || ss >= 256) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pValue, dev->texture_stage_states[st][ss]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_texture_stage_state_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_get_texture_stage_state_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StateT, typename MatrixT>
HRESULT device_set_transform_impl(D3DDDI_HDEVICE hDevice, StateT state, const MatrixT* pMatrix) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const bool affects_fixedfunc_matrix =
      (idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection);
  const bool affects_fixedfunc_lighting = (idx == kD3dTransformWorld0 || idx == kD3dTransformView);

  // Avoid spurious fixed-function constant updates when the runtime (or app)
  // redundantly calls SetTransform with identical data.
  bool changed = true;
  if (affects_fixedfunc_matrix || affects_fixedfunc_lighting) {
    changed = std::memcmp(dev->transform_matrices[idx], pMatrix, 16u * sizeof(float)) != 0;
  }
  if (changed) {
    std::memcpy(dev->transform_matrices[idx], pMatrix, 16u * sizeof(float));
    if (affects_fixedfunc_matrix) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (affects_fixedfunc_lighting) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  stateblock_record_transform_locked(dev, idx, dev->transform_matrices[idx]);

  // If fixed-function WVP rendering is active, reflect transform updates in the
  // internal VS constant registers eagerly (not just at draw time).
  if ((idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection) &&
      !dev->user_vs && !dev->user_ps && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_transform_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_transform_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StateT, typename MatrixT>
HRESULT device_multiply_transform_impl(D3DDDI_HDEVICE hDevice, StateT state, const MatrixT* pMatrix) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceMultiplyTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  float tmp[16];
  float rhs[16];
  std::memcpy(rhs, pMatrix, sizeof(rhs));
  d3d9_mul_mat4_row_major(dev->transform_matrices[idx], rhs, tmp);

  const bool affects_fixedfunc_matrix =
      (idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection);
  const bool affects_fixedfunc_lighting = (idx == kD3dTransformWorld0 || idx == kD3dTransformView);

  bool changed = true;
  if (affects_fixedfunc_matrix || affects_fixedfunc_lighting) {
    changed = std::memcmp(dev->transform_matrices[idx], tmp, sizeof(tmp)) != 0;
  }
  if (changed) {
    std::memcpy(dev->transform_matrices[idx], tmp, sizeof(tmp));
    if (affects_fixedfunc_matrix) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (affects_fixedfunc_lighting) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  stateblock_record_transform_locked(dev, idx, dev->transform_matrices[idx]);

  // If fixed-function WVP rendering is active, reflect transform updates in the
  // internal VS constant registers eagerly (not just at draw time).
  if ((idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection) &&
      !dev->user_vs && !dev->user_ps && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
  }

  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_multiply_transform_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_multiply_transform_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StateT, typename MatrixT>
HRESULT device_get_transform_impl(D3DDDI_HDEVICE hDevice, StateT state, MatrixT* pMatrix) {
  // Default to identity so callers never observe uninitialized output.
  if (pMatrix) {
    std::memset(pMatrix, 0, 16 * sizeof(float));
    float* f = reinterpret_cast<float*>(pMatrix);
    f[0] = 1.0f;
    f[5] = 1.0f;
    f[10] = 1.0f;
    f[15] = 1.0f;
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memcpy(pMatrix, dev->transform_matrices[idx], 16 * sizeof(float));
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_transform_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_transform_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename PlaneT>
HRESULT device_set_clip_plane_impl(D3DDDI_HDEVICE hDevice, IndexT index, const PlaneT* pPlane) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetClipPlane,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      d3d9_trace_arg_ptr(pPlane),
                      0);
  if (!hDevice.pDrvPrivate || !pPlane) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= 6) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memcpy(dev->clip_planes[idx], pPlane, 4 * sizeof(float));
  stateblock_record_clip_plane_locked(dev, idx, dev->clip_planes[idx]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_clip_plane_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_clip_plane_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename PlaneT>
HRESULT device_get_clip_plane_impl(D3DDDI_HDEVICE hDevice, IndexT index, PlaneT* pPlane) {
  if (pPlane) {
    std::memset(pPlane, 0, 4 * sizeof(float));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetClipPlane,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      d3d9_trace_arg_ptr(pPlane),
                      0);
  if (!hDevice.pDrvPrivate || !pPlane) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= 6) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memcpy(pPlane, dev->clip_planes[idx], 4 * sizeof(float));
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_clip_plane_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_clip_plane_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
HRESULT device_set_material_impl(D3DDDI_HDEVICE hDevice, const D3DMATERIAL9* pMaterial) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetMaterial,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pMaterial),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pMaterial) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->material = *pMaterial;
  dev->material_valid = true;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_material_locked(dev, dev->material, dev->material_valid);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_material_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_set_material_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

HRESULT device_get_material_impl(D3DDDI_HDEVICE hDevice, D3DMATERIAL9* pMaterial) {
  if (pMaterial) {
    std::memset(pMaterial, 0, sizeof(*pMaterial));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetMaterial,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pMaterial),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pMaterial) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->material_valid) {
    *pMaterial = dev->material;
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_material_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_material_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename LightT>
HRESULT device_set_light_impl(D3DDDI_HDEVICE hDevice, IndexT index, const LightT* pLight) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetLight,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      d3d9_trace_arg_ptr(pLight),
                      0);
  if (!hDevice.pDrvPrivate || !pLight) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= Device::kMaxLights) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memcpy(&dev->lights[idx], pLight, sizeof(dev->lights[idx]));
  dev->light_valid[idx] = true;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_light_locked(dev, idx, dev->lights[idx], dev->light_valid[idx]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_light_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_light_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename LightT>
HRESULT device_get_light_impl(D3DDDI_HDEVICE hDevice, IndexT index, LightT* pLight) {
  if (pLight) {
    std::memset(pLight, 0, sizeof(*pLight));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetLight,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      d3d9_trace_arg_ptr(pLight),
                      0);
  if (!hDevice.pDrvPrivate || !pLight) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= Device::kMaxLights) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->light_valid[idx]) {
    std::memcpy(pLight, &dev->lights[idx], sizeof(dev->lights[idx]));
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_light_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_light_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename BoolT>
HRESULT device_light_enable_impl(D3DDDI_HDEVICE hDevice, IndexT index, BoolT enabled) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceLightEnable,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      static_cast<uint64_t>(d3d9_to_u32(enabled)),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= Device::kMaxLights) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->light_enabled[idx] = d3d9_to_u32(enabled) ? TRUE : FALSE;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_light_enable_locked(dev, idx, dev->light_enabled[idx]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_light_enable_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_light_enable_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename IndexT, typename BoolT>
HRESULT device_get_light_enable_impl(D3DDDI_HDEVICE hDevice, IndexT index, BoolT* pEnabled) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetLightEnable,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(index)),
                      d3d9_trace_arg_ptr(pEnabled),
                      0);
  d3d9_write_u32(pEnabled, 0u);
  if (!hDevice.pDrvPrivate || !pEnabled) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(index);
  if (idx >= Device::kMaxLights) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pEnabled, dev->light_enabled[idx] ? 1u : 0u);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_light_enable_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_light_enable_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename PaletteT, typename EntryT>
HRESULT device_set_palette_entries_impl(D3DDDI_HDEVICE hDevice, PaletteT palette, const EntryT* pEntries) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetPaletteEntries,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(palette)),
                      d3d9_trace_arg_ptr(pEntries),
                      0);
  if (!hDevice.pDrvPrivate || !pEntries) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(palette);
  if (idx >= Device::kMaxPalettes) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memset(&dev->palette_entries[idx][0], 0, sizeof(dev->palette_entries[idx]));
  const size_t src_bytes = static_cast<size_t>(256) * sizeof(*pEntries);
  const size_t dst_bytes = sizeof(dev->palette_entries[idx]);
  std::memcpy(&dev->palette_entries[idx][0], pEntries, std::min(src_bytes, dst_bytes));
  dev->palette_valid[idx] = true;
  stateblock_record_palette_entries_locked(dev, idx, &dev->palette_entries[idx][0], true);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_palette_entries_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_palette_entries_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename PaletteT, typename EntryT>
HRESULT device_get_palette_entries_impl(D3DDDI_HDEVICE hDevice, PaletteT palette, EntryT* pEntries) {
  if (pEntries) {
    std::memset(pEntries, 0, static_cast<size_t>(256) * sizeof(*pEntries));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetPaletteEntries,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(palette)),
                      d3d9_trace_arg_ptr(pEntries),
                      0);
  if (!hDevice.pDrvPrivate || !pEntries) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(palette);
  if (idx >= Device::kMaxPalettes) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->palette_valid[idx]) {
    const size_t src_bytes = sizeof(dev->palette_entries[idx]);
    const size_t dst_bytes = static_cast<size_t>(256) * sizeof(*pEntries);
    std::memcpy(pEntries, &dev->palette_entries[idx][0], std::min(src_bytes, dst_bytes));
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_palette_entries_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_palette_entries_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename PaletteT>
HRESULT device_set_current_texture_palette_impl(D3DDDI_HDEVICE hDevice, PaletteT palette) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCurrentTexturePalette,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(palette)),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(palette);
  if (idx >= Device::kMaxPalettes) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->current_texture_palette = idx;
  stateblock_record_current_texture_palette_locked(dev, idx);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_current_texture_palette_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_set_current_texture_palette_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename PaletteT>
HRESULT device_get_current_texture_palette_impl(D3DDDI_HDEVICE hDevice, PaletteT* pPalette) {
  d3d9_write_u32(pPalette, 0u);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetCurrentTexturePalette,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pPalette),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pPalette) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pPalette, dev->current_texture_palette);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_current_texture_palette_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_current_texture_palette_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ClipStatusT>
HRESULT device_set_clip_status_impl(D3DDDI_HDEVICE hDevice, const ClipStatusT* pClipStatus) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetClipStatus,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pClipStatus),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pClipStatus) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memset(&dev->clip_status, 0, sizeof(dev->clip_status));
  std::memcpy(&dev->clip_status, pClipStatus, std::min(sizeof(dev->clip_status), sizeof(*pClipStatus)));
  dev->clip_status_valid = true;
  stateblock_record_clip_status_locked(dev, dev->clip_status, dev->clip_status_valid);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_clip_status_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_set_clip_status_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ClipStatusT>
HRESULT device_get_clip_status_impl(D3DDDI_HDEVICE hDevice, ClipStatusT* pClipStatus) {
  if (pClipStatus) {
    std::memset(pClipStatus, 0, sizeof(*pClipStatus));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetClipStatus,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pClipStatus),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pClipStatus) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->clip_status_valid) {
    std::memcpy(pClipStatus, &dev->clip_status, std::min(sizeof(*pClipStatus), sizeof(dev->clip_status)));
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_clip_status_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_clip_status_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename A1, typename A2, typename RampT>
HRESULT device_set_gamma_ramp_impl(D3DDDI_HDEVICE hDevice, A1 arg1, A2 arg2, const RampT* pRamp) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetGammaRamp,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(arg1)),
                      static_cast<uint64_t>(d3d9_to_u32(arg2)),
                      d3d9_trace_arg_ptr(pRamp));
  if (!hDevice.pDrvPrivate || !pRamp) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memset(&dev->gamma_ramp, 0, sizeof(dev->gamma_ramp));
  std::memcpy(&dev->gamma_ramp, pRamp, std::min(sizeof(dev->gamma_ramp), sizeof(*pRamp)));
  dev->gamma_ramp_valid = true;
  stateblock_record_gamma_ramp_locked(dev, dev->gamma_ramp, dev->gamma_ramp_valid);
  return trace.ret(S_OK);
}

template <typename A1, typename RampT>
HRESULT device_set_gamma_ramp_impl(D3DDDI_HDEVICE hDevice, A1 arg1, const RampT* pRamp) {
  return device_set_gamma_ramp_impl(hDevice, arg1, 0u, pRamp);
}

template <typename... Args>
HRESULT device_set_gamma_ramp_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_gamma_ramp_impl(args...);
  } else if constexpr (sizeof...(Args) == 4) {
    return device_set_gamma_ramp_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename A1, typename A2, typename RampT>
HRESULT device_get_gamma_ramp_impl(D3DDDI_HDEVICE hDevice, A1 arg1, A2 arg2, RampT* pRamp) {
  if (pRamp) {
    std::memset(pRamp, 0, sizeof(*pRamp));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetGammaRamp,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(arg1)),
                      static_cast<uint64_t>(d3d9_to_u32(arg2)),
                      d3d9_trace_arg_ptr(pRamp));
  if (!hDevice.pDrvPrivate || !pRamp) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const D3DGAMMARAMP* src = dev->gamma_ramp_valid ? &dev->gamma_ramp : nullptr;
  if (src) {
    std::memcpy(pRamp, src, std::min(sizeof(*pRamp), sizeof(*src)));
  }
  return trace.ret(S_OK);
}

template <typename A1, typename RampT>
HRESULT device_get_gamma_ramp_impl(D3DDDI_HDEVICE hDevice, A1 arg1, RampT* pRamp) {
  return device_get_gamma_ramp_impl(hDevice, arg1, 0u, pRamp);
}

template <typename... Args>
HRESULT device_get_gamma_ramp_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_gamma_ramp_impl(args...);
  } else if constexpr (sizeof...(Args) == 4) {
    return device_get_gamma_ramp_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename HandleT>
Resource* device_priority_as_resource(HandleT hResource) {
  if constexpr (aerogpu_has_member_pDrvPrivate<HandleT>::value) {
    return reinterpret_cast<Resource*>(hResource.pDrvPrivate);
  }
  return nullptr;
}

template <typename HandleT, typename PriorityT>
HRESULT device_set_priority_core(D3DDDI_HDEVICE hDevice, HandleT hResource, PriorityT new_priority, uint32_t* out_old_value) {
  if (out_old_value) {
    *out_old_value = 0;
  }
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  auto* res = device_priority_as_resource(hResource);
  if (!dev || !res) {
    return E_INVALIDARG;
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (out_old_value) {
    *out_old_value = res->priority;
  }
  res->priority = d3d9_to_u32(new_priority);
  return S_OK;
}

template <typename Ret, typename HandleT, typename PriorityT>
Ret device_set_priority_impl(D3DDDI_HDEVICE hDevice, HandleT hResource, PriorityT new_priority) {
  const uint32_t new_value = d3d9_to_u32(new_priority);
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      static_cast<uint64_t>(new_value),
                      0);

  if constexpr (!std::is_same_v<Ret, HRESULT> && !std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  uint32_t old_value = 0;
  const HRESULT hr = device_set_priority_core(hDevice, hResource, new_priority, &old_value);

  (void)trace.ret(hr);
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return static_cast<Ret>(hr);
  } else {
    return static_cast<Ret>(old_value);
  }
}

template <typename Ret, typename HandleT, typename PriorityT, typename OutT>
Ret device_set_priority_impl(D3DDDI_HDEVICE hDevice, HandleT hResource, PriorityT new_priority, OutT* pOldPriority) {
  d3d9_write_u32(pOldPriority, 0u);
  const uint32_t new_value = d3d9_to_u32(new_priority);
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      static_cast<uint64_t>(new_value),
                      d3d9_trace_arg_ptr(pOldPriority));

  if constexpr (!std::is_same_v<Ret, HRESULT> && !std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  uint32_t old_value = 0;
  const HRESULT hr = device_set_priority_core(hDevice, hResource, new_priority, &old_value);
  d3d9_write_u32(pOldPriority, old_value);

  (void)trace.ret(hr);
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return static_cast<Ret>(hr);
  } else {
    return static_cast<Ret>(old_value);
  }
}

template <typename Ret, typename... Args>
Ret device_set_priority_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_priority_impl<Ret>(args...);
  } else if constexpr (sizeof...(Args) == 4) {
    return device_set_priority_impl<Ret>(args...);
  }
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return D3DERR_NOTAVAILABLE;
  } else {
    return Ret{};
  }
}

template <typename Ret, typename HandleT, typename OutT>
Ret device_get_priority_impl(D3DDDI_HDEVICE hDevice, HandleT hResource, OutT* pPriority) {
  d3d9_write_u32(pPriority, 0u);
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      d3d9_trace_arg_ptr(pPriority),
                      0);

  if constexpr (!std::is_same_v<Ret, HRESULT> && !std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  HRESULT hr = S_OK;
  uint32_t value = 0;
  if (!hDevice.pDrvPrivate || !pPriority) {
    hr = E_INVALIDARG;
  } else {
    auto* dev = as_device(hDevice);
    auto* res = device_priority_as_resource(hResource);
    if (!dev || !res) {
      hr = E_INVALIDARG;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);
      value = res->priority;
      d3d9_write_u32(pPriority, value);
    }
  }

  (void)trace.ret(hr);
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return static_cast<Ret>(hr);
  } else {
    return static_cast<Ret>(value);
  }
}

template <typename Ret, typename HandleT>
Ret device_get_priority_impl(D3DDDI_HDEVICE hDevice, HandleT hResource) {
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      0,
                      0);

  if constexpr (!std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  HRESULT hr = S_OK;
  uint32_t value = 0;
  if (!hDevice.pDrvPrivate) {
    hr = E_INVALIDARG;
  } else {
    auto* dev = as_device(hDevice);
    auto* res = device_priority_as_resource(hResource);
    if (!dev || !res) {
      hr = E_INVALIDARG;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);
      value = res->priority;
    }
  }

  (void)trace.ret(hr);
  return static_cast<Ret>(value);
}

template <typename Ret, typename... Args>
Ret device_get_priority_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_priority_impl<Ret>(args...);
  } else if constexpr (sizeof...(Args) == 3) {
    return device_get_priority_impl<Ret>(args...);
  }
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return D3DERR_NOTAVAILABLE;
  } else {
    return Ret{};
  }
}

template <typename Ret, typename HandleT, typename FilterT>
Ret device_set_auto_gen_filter_type_impl(D3DDDI_HDEVICE hDevice, HandleT hResource, FilterT filter_type) {
  const uint32_t new_value = d3d9_to_u32(filter_type);
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetAutoGenFilterType,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      static_cast<uint64_t>(new_value),
                      0);

  if constexpr (!std::is_same_v<Ret, HRESULT> && !std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  HRESULT hr = S_OK;
  uint32_t old_value = 0;
  if (!hDevice.pDrvPrivate) {
    hr = E_INVALIDARG;
  } else {
    auto* dev = as_device(hDevice);
    auto* res = device_priority_as_resource(hResource);
    if (!dev || !res) {
      hr = E_INVALIDARG;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);
      old_value = res->auto_gen_filter_type;
      res->auto_gen_filter_type = new_value;
    }
  }

  (void)trace.ret(hr);
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return static_cast<Ret>(hr);
  } else {
    return static_cast<Ret>(old_value);
  }
}

template <typename Ret, typename... Args>
Ret device_set_auto_gen_filter_type_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_auto_gen_filter_type_impl<Ret>(args...);
  }
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return D3DERR_NOTAVAILABLE;
  } else {
    return Ret{};
  }
}

template <typename Ret, typename HandleT, typename OutT>
Ret device_get_auto_gen_filter_type_impl(D3DDDI_HDEVICE hDevice, HandleT hResource, OutT* pFilterType) {
  d3d9_write_u32(pFilterType, 2u);
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetAutoGenFilterType,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      d3d9_trace_arg_ptr(pFilterType),
                      0);

  if constexpr (!std::is_same_v<Ret, HRESULT> && !std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  HRESULT hr = S_OK;
  uint32_t value = 2u;
  if (!hDevice.pDrvPrivate || !pFilterType) {
    hr = E_INVALIDARG;
  } else {
    auto* dev = as_device(hDevice);
    auto* res = device_priority_as_resource(hResource);
    if (!dev || !res) {
      hr = E_INVALIDARG;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);
      value = res->auto_gen_filter_type;
      d3d9_write_u32(pFilterType, value);
    }
  }

  (void)trace.ret(hr);
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return static_cast<Ret>(hr);
  } else {
    return static_cast<Ret>(value);
  }
}

template <typename Ret, typename HandleT>
Ret device_get_auto_gen_filter_type_impl(D3DDDI_HDEVICE hDevice, HandleT hResource) {
  const uint64_t res_ptr = d3d9_trace_arg_ptr(device_priority_as_resource(hResource));
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetAutoGenFilterType,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      res_ptr,
                      0,
                      0);

  if constexpr (!std::is_integral_v<Ret>) {
    (void)trace.ret(D3DERR_NOTAVAILABLE);
    return Ret{};
  }

  HRESULT hr = S_OK;
  uint32_t value = 2u;
  if (!hDevice.pDrvPrivate) {
    hr = E_INVALIDARG;
  } else {
    auto* dev = as_device(hDevice);
    auto* res = device_priority_as_resource(hResource);
    if (!dev || !res) {
      hr = E_INVALIDARG;
    } else {
      std::lock_guard<std::mutex> lock(dev->mutex);
      value = res->auto_gen_filter_type;
    }
  }

  (void)trace.ret(hr);
  return static_cast<Ret>(value);
}

template <typename Ret, typename... Args>
Ret device_get_auto_gen_filter_type_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_auto_gen_filter_type_impl<Ret>(args...);
  } else if constexpr (sizeof...(Args) == 3) {
    return device_get_auto_gen_filter_type_impl<Ret>(args...);
  }
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return D3DERR_NOTAVAILABLE;
  } else {
    return Ret{};
  }
}

HRESULT AEROGPU_D3D9_CALL device_generate_mip_sub_levels(D3DDDI_HDEVICE hDevice, D3DDDI_HRESOURCE hTexture);

template <typename T, typename = void>
struct has_member_hResource : std::false_type {};
template <typename T>
struct has_member_hResource<T, std::void_t<decltype(std::declval<T>().hResource)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_hTexture : std::false_type {};
template <typename T>
struct has_member_hTexture<T, std::void_t<decltype(std::declval<T>().hTexture)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_pDrvPrivate : std::false_type {};
template <typename T>
struct has_member_pDrvPrivate<T, std::void_t<decltype(std::declval<T>().pDrvPrivate)>> : std::true_type {};

template <typename Ret, typename HandleT>
Ret device_generate_mip_sub_levels_dispatch_impl(D3DDDI_HDEVICE hDevice, HandleT hResource) {
  const HRESULT hr = device_generate_mip_sub_levels(hDevice, hResource);
  if constexpr (std::is_same_v<Ret, void>) {
    (void)hr;
    return;
  } else if constexpr (std::is_same_v<Ret, HRESULT>) {
    return hr;
  } else if constexpr (std::is_integral_v<Ret>) {
    return static_cast<Ret>(hr);
  } else {
    return Ret{};
  }
}

template <typename Ret, typename ArgsT>
Ret device_generate_mip_sub_levels_dispatch_impl(D3DDDI_HDEVICE hDevice, const ArgsT* pArgs) {
  if (!pArgs) {
    return device_generate_mip_sub_levels_dispatch_impl<Ret>(hDevice, D3DDDI_HRESOURCE{});
  }
  if constexpr (has_member_pDrvPrivate<ArgsT>::value && !has_member_hResource<ArgsT>::value && !has_member_hTexture<ArgsT>::value) {
    // Some header/runtime vintages may pass a pointer to the resource handle.
    return device_generate_mip_sub_levels_dispatch_impl<Ret>(hDevice, *pArgs);
  }
  if constexpr (has_member_hResource<ArgsT>::value) {
    return device_generate_mip_sub_levels_dispatch_impl<Ret>(hDevice, pArgs->hResource);
  } else if constexpr (has_member_hTexture<ArgsT>::value) {
    return device_generate_mip_sub_levels_dispatch_impl<Ret>(hDevice, pArgs->hTexture);
  } else {
    if constexpr (std::is_same_v<Ret, HRESULT>) {
      return D3DERR_NOTAVAILABLE;
    } else {
      return Ret{};
    }
  }
}

template <typename Ret, typename A0, typename A1>
Ret device_generate_mip_sub_levels_dispatch(A0 a0, A1 a1) {
  return device_generate_mip_sub_levels_dispatch_impl<Ret>(a0, a1);
}

template <typename Ret, typename A0, typename A1, typename A2>
Ret device_generate_mip_sub_levels_dispatch(A0 a0, A1 a1, A2 /*unused*/) {
  return device_generate_mip_sub_levels_dispatch_impl<Ret>(a0, a1);
}

template <typename Ret, typename... Args>
Ret device_generate_mip_sub_levels_dispatch(Args... /*args*/) {
  if constexpr (std::is_same_v<Ret, HRESULT>) {
    return D3DERR_NOTAVAILABLE;
  } else {
    return Ret{};
  }
}
template <typename ValueT>
HRESULT device_set_software_vertex_processing_impl(D3DDDI_HDEVICE hDevice, ValueT enabled) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetSoftwareVertexProcessing,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(enabled)),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->software_vertex_processing = d3d9_to_u32(enabled) ? TRUE : FALSE;
  stateblock_record_software_vertex_processing_locked(dev, dev->software_vertex_processing);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_software_vertex_processing_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_set_software_vertex_processing_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ValueT>
HRESULT device_get_software_vertex_processing_impl(D3DDDI_HDEVICE hDevice, ValueT* pEnabled) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetSoftwareVertexProcessing,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pEnabled),
                      0,
                      0);
  d3d9_write_u32(pEnabled, 0u);
  if (!hDevice.pDrvPrivate || !pEnabled) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pEnabled, dev->software_vertex_processing ? 1u : 0u);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_software_vertex_processing_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_software_vertex_processing_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

// -----------------------------------------------------------------------------
// Device cursor DDIs (SetCursorProperties/SetCursorPosition/ShowCursor)
// -----------------------------------------------------------------------------
// The D3D9 runtime exposes a device-managed cursor API (distinct from the Win32
// cursor). Apps/games frequently use it for custom cursors. AeroGPU implements
// this in two layers:
//
// 1) Preferred: program the KMD hardware cursor via driver-private escapes
//    (requires AEROGPU_FEATURE_CURSOR in the KMD/emulator).
// 2) Fallback: a minimal software cursor overlay composited over the present
//    source surface just before AEROGPU_CMD_PRESENT_EX is issued.
//
// Cursor coordinates are in client pixels. The hotspot (SetCursorProperties) is
// subtracted from the position (SetCursorPosition) when building the overlay
// destination rect.
namespace {
// D3D9 format subset (numeric values from d3d9types.h).
constexpr uint32_t kD3d9FmtA8R8G8B8 = 21u;
constexpr uint32_t kD3d9FmtX8R8G8B8 = 22u;
constexpr uint32_t kD3d9FmtA8B8G8R8 = 32u;
constexpr uint32_t kMaxCursorDim = 512u;
constexpr uint64_t kMaxCursorBytes = 1024ull * 1024ull; // hard cap for safety

inline void init_escape_header(aerogpu_escape_header* hdr, uint32_t op, uint32_t size) {
  if (!hdr) {
    return;
  }
  hdr->version = AEROGPU_ESCAPE_VERSION;
  hdr->op = op;
  hdr->size = size;
  hdr->reserved0 = 0;
}

// Builds a driver-private escape packet for SET_CURSOR_SHAPE (including pixels),
// returning false if cursor bytes are not CPU-readable.
static bool build_cursor_shape_escape_packet(Device* dev,
                                             Resource* cursor,
                                             uint32_t hot_x,
                                             uint32_t hot_y,
                                             std::vector<uint8_t>* out_packet) {
  if (!dev || !dev->adapter || !cursor || !out_packet) {
    return false;
  }

  const uint32_t w = cursor->width;
  const uint32_t h = cursor->height;
  if (w == 0 || h == 0 || w > kMaxCursorDim || h > kMaxCursorDim) {
    return false;
  }

  const uint64_t dst_pitch_u64 = static_cast<uint64_t>(w) * 4ull;
  if (dst_pitch_u64 == 0 || dst_pitch_u64 > 0xFFFFFFFFull) {
    return false;
  }
  const uint64_t pixel_bytes_u64 = dst_pitch_u64 * static_cast<uint64_t>(h);
  if (pixel_bytes_u64 == 0 || pixel_bytes_u64 > kMaxCursorBytes || pixel_bytes_u64 > 0xFFFFFFFFull) {
    return false;
  }

  const uint32_t dst_pitch = static_cast<uint32_t>(dst_pitch_u64);
  const uint32_t pixel_bytes = static_cast<uint32_t>(pixel_bytes_u64);

  uint32_t src_pitch = cursor->row_pitch;
  if (src_pitch == 0) {
    src_pitch = dst_pitch;
  }
  if (src_pitch < dst_pitch) {
    return false;
  }

  const uint64_t src_bytes_u64 = static_cast<uint64_t>(src_pitch) * static_cast<uint64_t>(h);
  if (src_bytes_u64 == 0 || src_bytes_u64 > 0x7FFFFFFFull || src_bytes_u64 > static_cast<uint64_t>(cursor->size_bytes)) {
    return false;
  }
  const uint32_t src_bytes = static_cast<uint32_t>(src_bytes_u64);

  const size_t header_bytes = offsetof(aerogpu_escape_set_cursor_shape_in, pixels);
  const size_t packet_bytes = header_bytes + static_cast<size_t>(pixel_bytes);
  if (packet_bytes > static_cast<size_t>(std::numeric_limits<uint32_t>::max())) {
    return false;
  }

  try {
    out_packet->assign(packet_bytes, 0);
  } catch (...) {
    return false;
  }

  auto* pkt = reinterpret_cast<aerogpu_escape_set_cursor_shape_in*>(out_packet->data());
  init_escape_header(&pkt->hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_SHAPE, static_cast<uint32_t>(packet_bytes));
  pkt->width = w;
  pkt->height = h;
  pkt->hot_x = hot_x;
  pkt->hot_y = hot_y;
  pkt->pitch_bytes = dst_pitch;
  // Advisory: the KMD scans alpha bytes to decide between BGRA vs BGRX. For XRGB
  // cursor surfaces, sanitize alpha bytes to 0 so KMD heuristics are
  // deterministic (some apps leave X8R8G8B8 alpha uninitialized).
  const uint32_t fmt = static_cast<uint32_t>(cursor->format);
  pkt->format = (fmt == kD3d9FmtX8R8G8B8) ? AEROGPU_FORMAT_B8G8R8X8_UNORM : AEROGPU_FORMAT_B8G8R8A8_UNORM;
  pkt->reserved0 = 0;
  pkt->reserved1 = 0;

  uint8_t* dst_base = pkt->pixels;
  const uint8_t* src_base = nullptr;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  void* locked_ptr = nullptr;
  bool locked = false;
  if (cursor->storage.size() >= src_bytes) {
    src_base = cursor->storage.data();
  } else if (cursor->wddm_hAllocation != 0 && dev->wddm_device != 0) {
    const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                            dev->wddm_device,
                                            cursor->wddm_hAllocation,
                                            /*offset_bytes=*/0,
                                            /*size_bytes=*/src_bytes,
                                            kD3DLOCK_READONLY,
                                            &locked_ptr,
                                            dev->wddm_context.hContext);
    if (FAILED(hr) || !locked_ptr) {
      return false;
    }
    locked = true;
    src_base = reinterpret_cast<const uint8_t*>(locked_ptr);
  } else {
    return false;
  }
#else
  if (cursor->storage.size() < src_bytes) {
    return false;
  }
  src_base = cursor->storage.data();
#endif

  // Copy (and convert when needed) into a tightly-packed BGRA/BGRX payload.
  for (uint32_t y = 0; y < h; ++y) {
    const uint8_t* src_row = src_base + static_cast<size_t>(y) * static_cast<size_t>(src_pitch);
    uint8_t* dst_row = dst_base + static_cast<size_t>(y) * static_cast<size_t>(dst_pitch);
    std::memcpy(dst_row, src_row, dst_pitch);

    if (fmt == kD3d9FmtA8B8G8R8) {
      // Convert RGBA -> BGRA (swap R/B).
      for (uint32_t x = 0; x < w; ++x) {
        const size_t i = static_cast<size_t>(x) * 4u;
        std::swap(dst_row[i + 0], dst_row[i + 2]);
      }
    }
    if (fmt == kD3d9FmtX8R8G8B8) {
      // Force alpha to 0 (BGRX) so KMD cursor format detection doesn't treat an
      // uninitialized X channel as ARGB alpha.
      for (uint32_t x = 0; x < w; ++x) {
        dst_row[static_cast<size_t>(x) * 4u + 3u] = 0;
      }
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (locked) {
    (void)wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, cursor->wddm_hAllocation, dev->wddm_context.hContext);
  }
#endif

  return true;
}
} // namespace

template <typename T, typename = void>
struct aerogpu_cursor_has_member_pDrvPrivate : std::false_type {};
template <typename T>
struct aerogpu_cursor_has_member_pDrvPrivate<T, std::void_t<decltype(std::declval<T>().pDrvPrivate)>> : std::true_type {};

template <typename CursorHandleT>
Resource* cursor_handle_as_resource(CursorHandleT hCursor) {
  if constexpr (aerogpu_cursor_has_member_pDrvPrivate<CursorHandleT>::value) {
    return reinterpret_cast<Resource*>(hCursor.pDrvPrivate);
  } else if constexpr (std::is_pointer_v<CursorHandleT>) {
    // Some header vintages may pass the cursor bitmap as a raw driver-private
    // pointer (non-handle). Be permissive.
    return reinterpret_cast<Resource*>(hCursor);
  } else {
    return nullptr;
  }
}

template <typename XHotT, typename YHotT, typename CursorHandleT>
HRESULT device_set_cursor_properties_values_impl(D3DDDI_HDEVICE hDevice,
                                                 XHotT x_hot,
                                                 YHotT y_hot,
                                                 CursorHandleT hCursor) {
  auto* cursor = cursor_handle_as_resource(hCursor);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorProperties,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(d3d9_to_u32(x_hot), d3d9_to_u32(y_hot)),
                      d3d9_trace_arg_ptr(cursor),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (!cursor) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const uint32_t fmt = static_cast<uint32_t>(cursor->format);
  if (fmt != kD3d9FmtA8R8G8B8 && fmt != kD3d9FmtX8R8G8B8 && fmt != kD3d9FmtA8B8G8R8) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const uint32_t hot_x = d3d9_to_u32(x_hot);
  const uint32_t hot_y = d3d9_to_u32(y_hot);
  if (hot_x >= cursor->width || hot_y >= cursor->height) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  Adapter* adapter = dev ? dev->adapter : nullptr;

  std::vector<uint8_t> shape_packet;
  bool shape_packet_ok = false;
  bool prev_hw_active = false;
  int32_t cur_x = 0;
  int32_t cur_y = 0;
  BOOL cur_visible = FALSE;
  uint64_t serial = 0;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    dev->cursor_bitmap = cursor;
    dev->cursor_hot_x = hot_x;
    dev->cursor_hot_y = hot_y;
    dev->cursor_bitmap_serial++;
    serial = dev->cursor_bitmap_serial;
    prev_hw_active = dev->cursor_hw_active;
    cur_x = dev->cursor_x;
    cur_y = dev->cursor_y;
    cur_visible = dev->cursor_visible;
    shape_packet_ok = build_cursor_shape_escape_packet(dev, cursor, hot_x, hot_y, &shape_packet);
  }

  bool hw_ok = false;
  if (adapter && shape_packet_ok && !shape_packet.empty()) {
    hw_ok = adapter->kmd_query.SendEscape(shape_packet.data(), static_cast<uint32_t>(shape_packet.size()));
  }

  // If we successfully programmed the hardware cursor shape, also synchronize the
  // current cached position/visibility so we don't rely on the runtime to
  // redundantly call SetCursorPosition/ShowCursor after SetCursorProperties.
  //
  // Guard all follow-up cursor escapes on `cursor_bitmap_serial` to avoid a
  // stale SetCursorProperties call overriding a more recent cursor update (e.g.
  // hiding the hardware cursor after a newer shape was successfully programmed).
  bool still_current = false;
  if (adapter) {
    std::lock_guard<std::mutex> lock(dev->mutex);
    still_current = (dev->cursor_bitmap_serial == serial);
    if (still_current) {
      cur_x = dev->cursor_x;
      cur_y = dev->cursor_y;
      cur_visible = dev->cursor_visible;
    }
  }
  if (still_current && hw_ok && adapter) {
    aerogpu_escape_set_cursor_visibility_in vis{};
    init_escape_header(&vis.hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_VISIBILITY, sizeof(vis));
    vis.visible = cur_visible ? 1u : 0u;
    vis.reserved0 = 0;
    (void)adapter->kmd_query.SendEscape(&vis, sizeof(vis));

    aerogpu_escape_set_cursor_position_in pos{};
    init_escape_header(&pos.hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_POSITION, sizeof(pos));
    pos.x = static_cast<uint32_t>(cur_x);
    pos.y = static_cast<uint32_t>(cur_y);
    (void)adapter->kmd_query.SendEscape(&pos, sizeof(pos));
  }

  // If we previously had an active hardware cursor and failed to update the
  // shape, attempt to hide the hardware cursor so we can fall back to the
  // software cursor overlay without double-rendering.
  if (still_current && !hw_ok && prev_hw_active && adapter) {
    aerogpu_escape_set_cursor_visibility_in vis{};
    init_escape_header(&vis.hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_VISIBILITY, sizeof(vis));
    vis.visible = 0;
    vis.reserved0 = 0;
    (void)adapter->kmd_query.SendEscape(&vis, sizeof(vis));
  }

  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    if (dev->cursor_bitmap_serial == serial) {
      dev->cursor_hw_active = hw_ok;
    }
  }

  return trace.ret(S_OK);
}

template <typename ArgsT>
HRESULT device_set_cursor_properties_from_args_impl(D3DDDI_HDEVICE hDevice, const ArgsT* pArgs) {
  if (!pArgs) {
    D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorProperties,
                        d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                        0,
                        0,
                        0);
    return trace.ret(E_INVALIDARG);
  }

  // Common WDK member spellings.
  const uint32_t hot_x = [&] {
    if constexpr (aerogpu_d3d9_has_member_XHotSpot<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->XHotSpot);
    } else if constexpr (aerogpu_d3d9_has_member_xHotSpot<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->xHotSpot);
    } else {
      return 0u;
    }
  }();

  const uint32_t hot_y = [&] {
    if constexpr (aerogpu_d3d9_has_member_YHotSpot<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->YHotSpot);
    } else if constexpr (aerogpu_d3d9_has_member_yHotSpot<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->yHotSpot);
    } else {
      return 0u;
    }
  }();

  auto* cursor = [&]() -> Resource* {
    if constexpr (aerogpu_d3d9_has_member_hCursor<ArgsT>::value) {
      return cursor_handle_as_resource(pArgs->hCursor);
    } else if constexpr (aerogpu_d3d9_has_member_hCursorBitmap<ArgsT>::value) {
      return cursor_handle_as_resource(pArgs->hCursorBitmap);
    } else if constexpr (aerogpu_d3d9_has_member_hCursorBitmapResource<ArgsT>::value) {
      return cursor_handle_as_resource(pArgs->hCursorBitmapResource);
    } else {
      return nullptr;
    }
  }();

  return device_set_cursor_properties_values_impl(hDevice, hot_x, hot_y, cursor);
}

template <typename... Args>
HRESULT device_set_cursor_properties_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_set_cursor_properties_values_impl(args...);
  } else if constexpr (sizeof...(Args) == 2) {
    return device_set_cursor_properties_from_args_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename X, typename Y, typename FlagsT>
HRESULT device_set_cursor_position_values_impl(D3DDDI_HDEVICE hDevice, X x, Y y, FlagsT /*flags*/) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorPosition,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(static_cast<uint32_t>(static_cast<int32_t>(x)),
                                              static_cast<uint32_t>(static_cast<int32_t>(y))),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  Adapter* adapter = dev ? dev->adapter : nullptr;
  int32_t x_i32 = static_cast<int32_t>(x);
  int32_t y_i32 = static_cast<int32_t>(y);
  bool hw_active = false;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    dev->cursor_x = x_i32;
    dev->cursor_y = y_i32;
    hw_active = dev->cursor_hw_active;
  }

  // Only emit hardware cursor updates when the hardware cursor path is active.
  if (adapter && hw_active) {
    aerogpu_escape_set_cursor_position_in pos{};
    init_escape_header(&pos.hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_POSITION, sizeof(pos));
    pos.x = static_cast<uint32_t>(x_i32);
    pos.y = static_cast<uint32_t>(y_i32);
    (void)adapter->kmd_query.SendEscape(&pos, sizeof(pos));
  }
  return trace.ret(S_OK);
}

template <typename ArgsT>
HRESULT device_set_cursor_position_from_args_impl(D3DDDI_HDEVICE hDevice, const ArgsT* pArgs) {
  if (!pArgs) {
    D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorPosition,
                        d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                        0,
                        0,
                        0);
    return trace.ret(E_INVALIDARG);
  }

  const int32_t x = [&] {
    if constexpr (aerogpu_d3d9_has_member_X<ArgsT>::value) {
      return static_cast<int32_t>(pArgs->X);
    } else if constexpr (aerogpu_d3d9_has_member_x<ArgsT>::value) {
      return static_cast<int32_t>(pArgs->x);
    } else {
      return 0;
    }
  }();

  const int32_t y = [&] {
    if constexpr (aerogpu_d3d9_has_member_Y<ArgsT>::value) {
      return static_cast<int32_t>(pArgs->Y);
    } else if constexpr (aerogpu_d3d9_has_member_y<ArgsT>::value) {
      return static_cast<int32_t>(pArgs->y);
    } else {
      return 0;
    }
  }();

  const uint32_t flags = [&] {
    if constexpr (aerogpu_d3d9_has_member_Flags<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->Flags);
    } else if constexpr (aerogpu_d3d9_has_member_flags<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->flags);
    } else {
      return 0u;
    }
  }();

  return device_set_cursor_position_values_impl(hDevice, x, y, flags);
}

template <typename... Args>
HRESULT device_set_cursor_position_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_set_cursor_position_values_impl(args...);
  } else if constexpr (sizeof...(Args) == 2) {
    return device_set_cursor_position_from_args_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ShowT>
HRESULT device_show_cursor_impl(D3DDDI_HDEVICE hDevice, ShowT show) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceShowCursor,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(show)),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  Adapter* adapter = dev ? dev->adapter : nullptr;
  const BOOL visible = d3d9_to_u32(show) ? TRUE : FALSE;
  bool hw_active = false;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    dev->cursor_visible = visible;
    hw_active = dev->cursor_hw_active;
  }

  // Only emit hardware cursor updates when the hardware cursor path is active.
  if (adapter && hw_active) {
    aerogpu_escape_set_cursor_visibility_in vis{};
    init_escape_header(&vis.hdr, AEROGPU_ESCAPE_OP_SET_CURSOR_VISIBILITY, sizeof(vis));
    vis.visible = visible ? 1u : 0u;
    vis.reserved0 = 0;
    (void)adapter->kmd_query.SendEscape(&vis, sizeof(vis));
  }
  return trace.ret(S_OK);
}

template <typename ArgsT>
HRESULT device_show_cursor_from_args_impl(D3DDDI_HDEVICE hDevice, const ArgsT* pArgs) {
  if (!pArgs) {
    D3d9TraceCall trace(D3d9TraceFunc::DeviceShowCursor,
                        d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                        0,
                        0,
                        0);
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t show = [&] {
    if constexpr (aerogpu_d3d9_has_member_Show<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->Show);
    } else if constexpr (aerogpu_d3d9_has_member_show<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->show);
    } else if constexpr (aerogpu_d3d9_has_member_bShow<ArgsT>::value) {
      return static_cast<uint32_t>(pArgs->bShow);
    } else {
      return 0u;
    }
  }();

  return device_show_cursor_impl(hDevice, show);
}

template <typename ShowT>
HRESULT device_show_cursor_dispatch(D3DDDI_HDEVICE hDevice, ShowT show) {
  if constexpr (std::is_pointer_v<ShowT>) {
    return device_show_cursor_from_args_impl(hDevice, show);
  } else {
    return device_show_cursor_impl(hDevice, show);
  }
}

template <typename... Args>
HRESULT device_show_cursor_dispatch(Args... /*args*/) {
  return D3DERR_NOTAVAILABLE;
}

template <typename ValueT>
HRESULT device_set_npatch_mode_impl(D3DDDI_HDEVICE hDevice, ValueT mode) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetNPatchMode,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(mode)),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->n_patch_mode = static_cast<float>(mode);
  stateblock_record_npatch_mode_locked(dev, dev->n_patch_mode);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_npatch_mode_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_set_npatch_mode_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ValueT>
HRESULT device_get_npatch_mode_impl(D3DDDI_HDEVICE hDevice, ValueT* pMode) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetNPatchMode,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pMode),
                      0,
                      0);
  if (pMode) {
    *pMode = static_cast<ValueT>(0);
  }
  if (!hDevice.pDrvPrivate || !pMode) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  *pMode = static_cast<ValueT>(dev->n_patch_mode);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_npatch_mode_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_npatch_mode_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StreamT, typename ValueT>
HRESULT device_set_stream_source_freq_impl(D3DDDI_HDEVICE hDevice, StreamT stream, ValueT value) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetStreamSourceFreq,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(stream)),
                      static_cast<uint64_t>(d3d9_to_u32(value)),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stream);
  if (st >= 16) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->stream_source_freq[st] = d3d9_to_u32(value);
  stateblock_record_stream_source_freq_locked(dev, st, dev->stream_source_freq[st]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_stream_source_freq_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_set_stream_source_freq_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StreamT, typename ValueT>
HRESULT device_get_stream_source_freq_impl(D3DDDI_HDEVICE hDevice, StreamT stream, ValueT* pValue) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetStreamSourceFreq,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(stream)),
                      d3d9_trace_arg_ptr(pValue),
                      0);
  d3d9_write_u32(pValue, 1u);
  if (!hDevice.pDrvPrivate || !pValue) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stream);
  if (st >= 16) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pValue, dev->stream_source_freq[st]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_stream_source_freq_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_stream_source_freq_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StartT, typename DataT, typename CountT>
HRESULT device_set_shader_const_i_impl(
    D3DDDI_HDEVICE hDevice,
    StageT stage,
    StartT start_reg,
    const DataT* pData,
    CountT vec4_count) {
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t start = d3d9_to_u32(start_reg);
  const uint32_t count = d3d9_to_u32(vec4_count);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShaderConstI,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  // Be permissive: some header/runtime combinations may not use the exact {0,1}
  // encoding at the DDI boundary. Match the shader binding path, which treats
  // any non-VS stage as PS.
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  int32_t* dst = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_i : dev->ps_consts_i;
  const uint32_t base = start * 4u;
  const uint32_t elems = count * 4u;
  bool changed = false;
  for (uint32_t i = 0; i < elems; ++i) {
    const int32_t v = static_cast<int32_t>(pData[i]);
    if (dst[base + i] != v) {
      changed = true;
    }
    dst[base + i] = v;
  }
  stateblock_record_shader_const_i_locked(dev, stage_norm, start, dst + base, count);

  if (!changed) {
    // Skip redundant constant uploads: setting identical constant data again is a
    // no-op, but still record it for state blocks above.
    return trace.ret(S_OK);
  }

  const size_t payload_size = static_cast<size_t>(elems) * sizeof(int32_t);
  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_i>(
      dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_I, dst + base, payload_size);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage_norm);
  cmd->start_register = start;
  cmd->vec4_count = count;
  cmd->reserved0 = 0;
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_shader_const_i_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_set_shader_const_i_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StartT, typename DataT, typename CountT>
HRESULT device_get_shader_const_i_impl(
    D3DDDI_HDEVICE hDevice,
    StageT stage,
    StartT start_reg,
    DataT* pData,
    CountT vec4_count) {
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t start = d3d9_to_u32(start_reg);
  const uint32_t count = d3d9_to_u32(vec4_count);
  if (pData && count != 0) {
    const uint32_t init_regs = std::min(count, 256u);
    std::memset(pData, 0, static_cast<size_t>(init_regs) * 4 * sizeof(*pData));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetShaderConstI,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const int32_t* src = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_i : dev->ps_consts_i;
  const uint32_t base = start * 4u;
  const uint32_t elems = count * 4u;
  for (uint32_t i = 0; i < elems; ++i) {
    pData[i] = static_cast<DataT>(src[base + i]);
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_shader_const_i_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_get_shader_const_i_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StartT, typename DataT, typename CountT>
HRESULT device_set_shader_const_b_impl(
    D3DDDI_HDEVICE hDevice,
    StageT stage,
    StartT start_reg,
    const DataT* pData,
    CountT bool_count) {
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t start = d3d9_to_u32(start_reg);
  const uint32_t count = d3d9_to_u32(bool_count);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShaderConstB,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  uint8_t* dst = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_b : dev->ps_consts_b;
  bool changed = false;
  for (uint32_t i = 0; i < count; ++i) {
    const uint8_t v = pData[i] ? 1u : 0u;
    if (dst[start + i] != v) {
      changed = true;
    }
    dst[start + i] = v;
  }
  stateblock_record_shader_const_b_locked(dev, stage_norm, start, dst + start, count);

  if (!changed) {
    // Skip redundant constant uploads: setting identical constant data again is a
    // no-op, but still record it for state blocks above.
    return trace.ret(S_OK);
  }

  std::array<uint32_t, 256> payload{};
  for (uint32_t i = 0; i < count; ++i) {
    payload[i] = dst[start + i] ? 1u : 0u;
  }
  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_b>(
      dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_B, payload.data(), static_cast<size_t>(count) * sizeof(uint32_t));
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage_norm);
  cmd->start_register = start;
  cmd->bool_count = count;
  cmd->reserved0 = 0;
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_set_shader_const_b_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_set_shader_const_b_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StartT, typename DataT, typename CountT>
HRESULT device_get_shader_const_b_impl(
    D3DDDI_HDEVICE hDevice,
    StageT stage,
    StartT start_reg,
    DataT* pData,
    CountT bool_count) {
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t start = d3d9_to_u32(start_reg);
  const uint32_t count = d3d9_to_u32(bool_count);
  if (pData && count != 0) {
    const uint32_t init = std::min(count, 256u);
    std::memset(pData, 0, static_cast<size_t>(init) * sizeof(*pData));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetShaderConstB,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const uint8_t* src = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_b : dev->ps_consts_b;
  for (uint32_t i = 0; i < count; ++i) {
    pData[i] = static_cast<DataT>(src[start + i] ? 1u : 0u);
  }
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_shader_const_b_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_get_shader_const_b_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StateT, typename ValueT>
HRESULT device_get_render_state_impl(D3DDDI_HDEVICE hDevice, StateT state, ValueT* pValue) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetRenderState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pValue),
                      0);
  // Always initialize output when possible so callers can't accidentally consume
  // uninitialized memory if they ignore the HRESULT.
  d3d9_write_u32(pValue, 0u);
  if (!hDevice.pDrvPrivate || !pValue) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const uint32_t s = d3d9_to_u32(state);
  if (s >= 256) {
    return trace.ret(kD3DErrInvalidCall);
  }
  d3d9_write_u32(pValue, dev->render_states[s]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_render_state_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_render_state_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StateT, typename ValueT>
HRESULT device_get_sampler_state_impl(D3DDDI_HDEVICE hDevice, StageT stage, StateT state, ValueT* pValue) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetSamplerState,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(d3d9_to_u32(stage), d3d9_to_u32(state)),
                      d3d9_trace_arg_ptr(pValue),
                      0);
  d3d9_write_u32(pValue, 0u);
  if (!hDevice.pDrvPrivate || !pValue) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t ss = d3d9_to_u32(state);
  if (st >= 16 || ss >= 16) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pValue, dev->sampler_states[st][ss]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_sampler_state_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_get_sampler_state_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename HandleT>
HRESULT device_get_texture_impl(D3DDDI_HDEVICE hDevice, StageT stage, HandleT* phTexture) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetTexture,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(stage)),
                      d3d9_trace_arg_ptr(phTexture),
                      0);
  d3d9_write_handle(phTexture, nullptr);
  if (!hDevice.pDrvPrivate || !phTexture) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stage);
  if (st >= 16) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_handle(phTexture, dev->textures[st]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_texture_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_texture_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename SlotT, typename HandleT>
HRESULT device_get_render_target_impl(D3DDDI_HDEVICE hDevice, SlotT slot, HandleT* phSurface) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetRenderTarget,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(slot)),
                      d3d9_trace_arg_ptr(phSurface),
                      0);
  d3d9_write_handle(phSurface, nullptr);
  if (!hDevice.pDrvPrivate || !phSurface) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t idx = d3d9_to_u32(slot);
  if (idx >= 4) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_handle(phSurface, dev->render_targets[idx]);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_render_target_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_render_target_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename HandleT>
HRESULT device_get_depth_stencil_impl(D3DDDI_HDEVICE hDevice, HandleT* phSurface) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetDepthStencil,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(phSurface),
                      0,
                      0);
  d3d9_write_handle(phSurface, nullptr);
  if (!hDevice.pDrvPrivate || !phSurface) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_handle(phSurface, dev->depth_stencil);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_depth_stencil_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_depth_stencil_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ViewportT>
HRESULT device_get_viewport_impl(D3DDDI_HDEVICE hDevice, ViewportT* pViewport) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetViewport,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pViewport),
                      0,
                      0);
  if (pViewport) {
    pViewport->X = 0;
    pViewport->Y = 0;
    pViewport->Width = 0;
    pViewport->Height = 0;
    pViewport->MinZ = 0;
    pViewport->MaxZ = 1;
  }
  if (!hDevice.pDrvPrivate || !pViewport) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const D3DDDIVIEWPORTINFO vp = viewport_effective_locked(dev);
  pViewport->X = static_cast<decltype(pViewport->X)>(vp.X);
  pViewport->Y = static_cast<decltype(pViewport->Y)>(vp.Y);
  pViewport->Width = static_cast<decltype(pViewport->Width)>(vp.Width);
  pViewport->Height = static_cast<decltype(pViewport->Height)>(vp.Height);
  pViewport->MinZ = static_cast<decltype(pViewport->MinZ)>(vp.MinZ);
  pViewport->MaxZ = static_cast<decltype(pViewport->MaxZ)>(vp.MaxZ);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_viewport_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_viewport_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename RectT, typename BoolT>
HRESULT device_get_scissor_rect_impl(D3DDDI_HDEVICE hDevice, RectT* pRect, BoolT* pEnabled) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetScissorRect,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pRect),
                      d3d9_trace_arg_ptr(pEnabled),
                      0);
  if (pRect) {
    *pRect = {};
  }
  d3d9_write_u32(pEnabled, 0u);
  // Some header/runtime combinations may pass NULL for pRect when only the
  // enabled flag is needed. Be permissive: return E_INVALIDARG only if both
  // outputs are missing.
  if (!hDevice.pDrvPrivate || (!pRect && !pEnabled)) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (pRect) {
    *pRect = scissor_rect_effective_locked(dev);
  }
  if (pEnabled) {
    d3d9_write_u32(pEnabled, static_cast<uint32_t>(dev->scissor_enabled));
  }
  return trace.ret(S_OK);
}

template <typename RectT>
HRESULT device_get_scissor_rect_impl(D3DDDI_HDEVICE hDevice, RectT* pRect) {
  return device_get_scissor_rect_impl(hDevice, pRect, static_cast<BOOL*>(nullptr));
}

template <typename... Args>
HRESULT device_get_scissor_rect_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_scissor_rect_impl(args...);
  } else if constexpr (sizeof...(Args) == 3) {
    return device_get_scissor_rect_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StreamT, typename HandleT, typename OffsetT, typename StrideT>
HRESULT device_get_stream_source_impl(
    D3DDDI_HDEVICE hDevice,
    StreamT stream,
    HandleT* phVb,
    OffsetT* pOffset,
    StrideT* pStride) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetStreamSource,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(stream)),
                      d3d9_trace_arg_ptr(phVb),
                      d3d9_trace_pack_u32_u32(d3d9_trace_arg_ptr(pOffset) != 0 ? 1u : 0u,
                                              d3d9_trace_arg_ptr(pStride) != 0 ? 1u : 0u));
  d3d9_write_handle(phVb, nullptr);
  d3d9_write_u32(pOffset, 0u);
  d3d9_write_u32(pStride, 0u);
  if (!hDevice.pDrvPrivate || !phVb || !pOffset || !pStride) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stream);
  if (st >= 16) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const DeviceStateStream& ss = dev->streams[st];
  d3d9_write_handle(phVb, ss.vb);
  d3d9_write_u32(pOffset, ss.offset_bytes);
  d3d9_write_u32(pStride, ss.stride_bytes);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_stream_source_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_get_stream_source_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename HandleT, typename FormatT, typename OffsetT>
HRESULT device_get_indices_impl(D3DDDI_HDEVICE hDevice, HandleT* phIb, FormatT* pFormat, OffsetT* pOffset) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetIndices,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(phIb),
                      d3d9_trace_arg_ptr(pFormat),
                      d3d9_trace_arg_ptr(pOffset));
  d3d9_write_handle(phIb, nullptr);
  d3d9_write_u32(pFormat, 0u);
  d3d9_write_u32(pOffset, 0u);
  if (!hDevice.pDrvPrivate || !phIb || !pFormat || !pOffset) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_handle(phIb, dev->index_buffer);
  d3d9_write_u32(pFormat, static_cast<uint32_t>(dev->index_format));
  d3d9_write_u32(pOffset, dev->index_offset_bytes);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_indices_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 4) {
    return device_get_indices_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename HandleT>
HRESULT device_get_shader_impl(D3DDDI_HDEVICE hDevice, StageT stage, HandleT* phShader) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetShader,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(d3d9_to_u32(stage)),
                      d3d9_trace_arg_ptr(phShader),
                      0);
  d3d9_write_handle(phShader, nullptr);
  if (!hDevice.pDrvPrivate || !phShader) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  Shader* sh = (stage_norm == kD3d9ShaderStageVs) ? dev->user_vs : dev->user_ps;
  d3d9_write_handle(phShader, sh);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_shader_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_shader_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename StageT, typename StartT, typename DataT, typename CountT>
HRESULT device_get_shader_const_f_impl(
    D3DDDI_HDEVICE hDevice,
    StageT stage,
    StartT start_reg,
    DataT* pData,
    CountT vec4_count) {
  const uint32_t st = d3d9_to_u32(stage);
  const uint32_t start = d3d9_to_u32(start_reg);
  const uint32_t count = d3d9_to_u32(vec4_count);
  if (pData && count != 0) {
    // Keep behavior deterministic even if the caller ignores a failure HRESULT.
    // Cap the memset to a sane upper bound (the register file is 256 float4s).
    const uint32_t init_regs = std::min(count, 256u);
    std::memset(pData, 0, static_cast<size_t>(init_regs) * 4 * sizeof(float));
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetShaderConstF,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }
  const uint32_t stage_norm = (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const float* src = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_f : dev->ps_consts_f;
  std::memcpy(pData, src + start * 4, static_cast<size_t>(count) * 4 * sizeof(float));
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_shader_const_f_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 5) {
    return device_get_shader_const_f_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename ValueT>
HRESULT device_get_fvf_impl(D3DDDI_HDEVICE hDevice, ValueT* pFvf) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetFVF,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pFvf),
                      0,
                      0);
  d3d9_write_u32(pFvf, 0u);
  if (!hDevice.pDrvPrivate || !pFvf) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_u32(pFvf, dev->fvf);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_fvf_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_fvf_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename HandleT>
HRESULT device_get_vertex_decl_impl(D3DDDI_HDEVICE hDevice, HandleT* phDecl) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetVertexDecl,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(phDecl),
                      0,
                      0);
  d3d9_write_handle(phDecl, nullptr);
  if (!hDevice.pDrvPrivate || !phDecl) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  d3d9_write_handle(phDecl, dev->vertex_decl);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_vertex_decl_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 2) {
    return device_get_vertex_decl_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetTextureStageState;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetTextureStageState<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetTextureStageState(Args... args) {
    return static_cast<Ret>(device_set_texture_stage_state_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetTextureStageState<Ret(*)(Args...)> {
  static Ret pfnSetTextureStageState(Args... args) {
    return static_cast<Ret>(device_set_texture_stage_state_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetTextureStageState;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTextureStageState<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetTextureStageState(Args... args) {
    return static_cast<Ret>(device_get_texture_stage_state_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTextureStageState<Ret(*)(Args...)> {
  static Ret pfnGetTextureStageState(Args... args) {
    return static_cast<Ret>(device_get_texture_stage_state_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetTransform;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetTransform<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetTransform(Args... args) {
    return static_cast<Ret>(device_set_transform_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetTransform<Ret(*)(Args...)> {
  static Ret pfnSetTransform(Args... args) {
    return static_cast<Ret>(device_set_transform_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnMultiplyTransform;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnMultiplyTransform<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnMultiplyTransform(Args... args) {
    return static_cast<Ret>(device_multiply_transform_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnMultiplyTransform<Ret(*)(Args...)> {
  static Ret pfnMultiplyTransform(Args... args) {
    return static_cast<Ret>(device_multiply_transform_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetTransform;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTransform<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetTransform(Args... args) {
    return static_cast<Ret>(device_get_transform_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTransform<Ret(*)(Args...)> {
  static Ret pfnGetTransform(Args... args) {
    return static_cast<Ret>(device_get_transform_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetClipPlane;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetClipPlane<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetClipPlane(Args... args) {
    return static_cast<Ret>(device_set_clip_plane_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetClipPlane<Ret(*)(Args...)> {
  static Ret pfnSetClipPlane(Args... args) {
    return static_cast<Ret>(device_set_clip_plane_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetClipPlane;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetClipPlane<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetClipPlane(Args... args) {
    return static_cast<Ret>(device_get_clip_plane_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetClipPlane<Ret(*)(Args...)> {
  static Ret pfnGetClipPlane(Args... args) {
    return static_cast<Ret>(device_get_clip_plane_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetMaterial;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetMaterial<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetMaterial(Args... args) {
    return static_cast<Ret>(device_set_material_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetMaterial<Ret(*)(Args...)> {
  static Ret pfnSetMaterial(Args... args) {
    return static_cast<Ret>(device_set_material_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetMaterial;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetMaterial<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetMaterial(Args... args) {
    return static_cast<Ret>(device_get_material_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetMaterial<Ret(*)(Args...)> {
  static Ret pfnGetMaterial(Args... args) {
    return static_cast<Ret>(device_get_material_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetLight;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetLight<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetLight(Args... args) {
    return static_cast<Ret>(device_set_light_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetLight<Ret(*)(Args...)> {
  static Ret pfnSetLight(Args... args) {
    return static_cast<Ret>(device_set_light_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetLight;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetLight<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetLight(Args... args) {
    return static_cast<Ret>(device_get_light_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetLight<Ret(*)(Args...)> {
  static Ret pfnGetLight(Args... args) {
    return static_cast<Ret>(device_get_light_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnLightEnable;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnLightEnable<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnLightEnable(Args... args) {
    return static_cast<Ret>(device_light_enable_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnLightEnable<Ret(*)(Args...)> {
  static Ret pfnLightEnable(Args... args) {
    return static_cast<Ret>(device_light_enable_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetLightEnable;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetLightEnable<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetLightEnable(Args... args) {
    return static_cast<Ret>(device_get_light_enable_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetLightEnable<Ret(*)(Args...)> {
  static Ret pfnGetLightEnable(Args... args) {
    return static_cast<Ret>(device_get_light_enable_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetPaletteEntries;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetPaletteEntries<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetPaletteEntries(Args... args) {
    return static_cast<Ret>(device_set_palette_entries_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetPaletteEntries<Ret(*)(Args...)> {
  static Ret pfnSetPaletteEntries(Args... args) {
    return static_cast<Ret>(device_set_palette_entries_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetPaletteEntries;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetPaletteEntries<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetPaletteEntries(Args... args) {
    return static_cast<Ret>(device_get_palette_entries_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetPaletteEntries<Ret(*)(Args...)> {
  static Ret pfnGetPaletteEntries(Args... args) {
    return static_cast<Ret>(device_get_palette_entries_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetCurrentTexturePalette;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCurrentTexturePalette<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetCurrentTexturePalette(Args... args) {
    return static_cast<Ret>(device_set_current_texture_palette_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCurrentTexturePalette<Ret(*)(Args...)> {
  static Ret pfnSetCurrentTexturePalette(Args... args) {
    return static_cast<Ret>(device_set_current_texture_palette_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetCurrentTexturePalette;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetCurrentTexturePalette<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetCurrentTexturePalette(Args... args) {
    return static_cast<Ret>(device_get_current_texture_palette_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetCurrentTexturePalette<Ret(*)(Args...)> {
  static Ret pfnGetCurrentTexturePalette(Args... args) {
    return static_cast<Ret>(device_get_current_texture_palette_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetClipStatus;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetClipStatus<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetClipStatus(Args... args) {
    return static_cast<Ret>(device_set_clip_status_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetClipStatus<Ret(*)(Args...)> {
  static Ret pfnSetClipStatus(Args... args) {
    return static_cast<Ret>(device_set_clip_status_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetClipStatus;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetClipStatus<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetClipStatus(Args... args) {
    return static_cast<Ret>(device_get_clip_status_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetClipStatus<Ret(*)(Args...)> {
  static Ret pfnGetClipStatus(Args... args) {
    return static_cast<Ret>(device_get_clip_status_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetGammaRamp;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetGammaRamp<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetGammaRamp(Args... args) {
    return static_cast<Ret>(device_set_gamma_ramp_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetGammaRamp<Ret(*)(Args...)> {
  static Ret pfnSetGammaRamp(Args... args) {
    return static_cast<Ret>(device_set_gamma_ramp_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetGammaRamp;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetGammaRamp<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetGammaRamp(Args... args) {
    return static_cast<Ret>(device_get_gamma_ramp_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetGammaRamp<Ret(*)(Args...)> {
  static Ret pfnGetGammaRamp(Args... args) {
    return static_cast<Ret>(device_get_gamma_ramp_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetPriority;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetPriority<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetPriority(Args... args) {
    return device_set_priority_dispatch<Ret>(args...);
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetPriority<Ret(*)(Args...)> {
  static Ret pfnSetPriority(Args... args) {
    return device_set_priority_dispatch<Ret>(args...);
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetPriority;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetPriority<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetPriority(Args... args) {
    return device_get_priority_dispatch<Ret>(args...);
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetPriority<Ret(*)(Args...)> {
  static Ret pfnGetPriority(Args... args) {
    return device_get_priority_dispatch<Ret>(args...);
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetAutoGenFilterType;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetAutoGenFilterType<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetAutoGenFilterType(Args... args) {
    return device_set_auto_gen_filter_type_dispatch<Ret>(args...);
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetAutoGenFilterType<Ret(*)(Args...)> {
  static Ret pfnSetAutoGenFilterType(Args... args) {
    return device_set_auto_gen_filter_type_dispatch<Ret>(args...);
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetAutoGenFilterType;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetAutoGenFilterType<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetAutoGenFilterType(Args... args) {
    return device_get_auto_gen_filter_type_dispatch<Ret>(args...);
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetAutoGenFilterType<Ret(*)(Args...)> {
  static Ret pfnGetAutoGenFilterType(Args... args) {
    return device_get_auto_gen_filter_type_dispatch<Ret>(args...);
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGenerateMipSubLevels;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGenerateMipSubLevels<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGenerateMipSubLevels(Args... args) {
    return device_generate_mip_sub_levels_dispatch<Ret>(args...);
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGenerateMipSubLevels<Ret(*)(Args...)> {
  static Ret pfnGenerateMipSubLevels(Args... args) {
    return device_generate_mip_sub_levels_dispatch<Ret>(args...);
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetSoftwareVertexProcessing;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetSoftwareVertexProcessing<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetSoftwareVertexProcessing(Args... args) {
    return static_cast<Ret>(device_set_software_vertex_processing_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetSoftwareVertexProcessing<Ret(*)(Args...)> {
  static Ret pfnSetSoftwareVertexProcessing(Args... args) {
    return static_cast<Ret>(device_set_software_vertex_processing_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetCursorProperties;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCursorProperties<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetCursorProperties(Args... args) {
    return static_cast<Ret>(device_set_cursor_properties_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCursorProperties<Ret(*)(Args...)> {
  static Ret pfnSetCursorProperties(Args... args) {
    return static_cast<Ret>(device_set_cursor_properties_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetCursorPosition;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCursorPosition<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetCursorPosition(Args... args) {
    return static_cast<Ret>(device_set_cursor_position_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetCursorPosition<Ret(*)(Args...)> {
  static Ret pfnSetCursorPosition(Args... args) {
    return static_cast<Ret>(device_set_cursor_position_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnShowCursor;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnShowCursor<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnShowCursor(Args... args) {
    return static_cast<Ret>(device_show_cursor_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnShowCursor<Ret(*)(Args...)> {
  static Ret pfnShowCursor(Args... args) {
    return static_cast<Ret>(device_show_cursor_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetSoftwareVertexProcessing;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetSoftwareVertexProcessing<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetSoftwareVertexProcessing(Args... args) {
    return static_cast<Ret>(device_get_software_vertex_processing_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetSoftwareVertexProcessing<Ret(*)(Args...)> {
  static Ret pfnGetSoftwareVertexProcessing(Args... args) {
    return static_cast<Ret>(device_get_software_vertex_processing_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetNPatchMode;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetNPatchMode<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetNPatchMode(Args... args) {
    return static_cast<Ret>(device_set_npatch_mode_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetNPatchMode<Ret(*)(Args...)> {
  static Ret pfnSetNPatchMode(Args... args) {
    return static_cast<Ret>(device_set_npatch_mode_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetNPatchMode;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetNPatchMode<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetNPatchMode(Args... args) {
    return static_cast<Ret>(device_get_npatch_mode_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetNPatchMode<Ret(*)(Args...)> {
  static Ret pfnGetNPatchMode(Args... args) {
    return static_cast<Ret>(device_get_npatch_mode_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetStreamSourceFreq;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetStreamSourceFreq<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetStreamSourceFreq(Args... args) {
    return static_cast<Ret>(device_set_stream_source_freq_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetStreamSourceFreq<Ret(*)(Args...)> {
  static Ret pfnSetStreamSourceFreq(Args... args) {
    return static_cast<Ret>(device_set_stream_source_freq_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetStreamSourceFreq;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetStreamSourceFreq<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetStreamSourceFreq(Args... args) {
    return static_cast<Ret>(device_get_stream_source_freq_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetStreamSourceFreq<Ret(*)(Args...)> {
  static Ret pfnGetStreamSourceFreq(Args... args) {
    return static_cast<Ret>(device_get_stream_source_freq_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetViewport;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetViewport<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetViewport(Args... args) {
    return static_cast<Ret>(device_get_viewport_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetViewport<Ret(*)(Args...)> {
  static Ret pfnGetViewport(Args... args) {
    return static_cast<Ret>(device_get_viewport_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetScissorRect;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetScissorRect<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetScissorRect(Args... args) {
    return static_cast<Ret>(device_get_scissor_rect_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetScissorRect<Ret(*)(Args...)> {
  static Ret pfnGetScissorRect(Args... args) {
    return static_cast<Ret>(device_get_scissor_rect_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetRenderTarget;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRenderTarget<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetRenderTarget(Args... args) {
    return static_cast<Ret>(device_get_render_target_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRenderTarget<Ret(*)(Args...)> {
  static Ret pfnGetRenderTarget(Args... args) {
    return static_cast<Ret>(device_get_render_target_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetDepthStencil;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetDepthStencil<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetDepthStencil(Args... args) {
    return static_cast<Ret>(device_get_depth_stencil_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetDepthStencil<Ret(*)(Args...)> {
  static Ret pfnGetDepthStencil(Args... args) {
    return static_cast<Ret>(device_get_depth_stencil_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetTexture;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTexture<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetTexture(Args... args) {
    return static_cast<Ret>(device_get_texture_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetTexture<Ret(*)(Args...)> {
  static Ret pfnGetTexture(Args... args) {
    return static_cast<Ret>(device_get_texture_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetSamplerState;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetSamplerState<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetSamplerState(Args... args) {
    return static_cast<Ret>(device_get_sampler_state_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetSamplerState<Ret(*)(Args...)> {
  static Ret pfnGetSamplerState(Args... args) {
    return static_cast<Ret>(device_get_sampler_state_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetRenderState;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRenderState<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetRenderState(Args... args) {
    return static_cast<Ret>(device_get_render_state_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRenderState<Ret(*)(Args...)> {
  static Ret pfnGetRenderState(Args... args) {
    return static_cast<Ret>(device_get_render_state_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetStreamSource;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetStreamSource<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetStreamSource(Args... args) {
    return static_cast<Ret>(device_get_stream_source_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetStreamSource<Ret(*)(Args...)> {
  static Ret pfnGetStreamSource(Args... args) {
    return static_cast<Ret>(device_get_stream_source_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetIndices;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetIndices<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetIndices(Args... args) {
    return static_cast<Ret>(device_get_indices_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetIndices<Ret(*)(Args...)> {
  static Ret pfnGetIndices(Args... args) {
    return static_cast<Ret>(device_get_indices_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetShader;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShader<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetShader(Args... args) {
    return static_cast<Ret>(device_get_shader_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShader<Ret(*)(Args...)> {
  static Ret pfnGetShader(Args... args) {
    return static_cast<Ret>(device_get_shader_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetShaderConstF;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstF<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetShaderConstF(Args... args) {
    return static_cast<Ret>(device_get_shader_const_f_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstF<Ret(*)(Args...)> {
  static Ret pfnGetShaderConstF(Args... args) {
    return static_cast<Ret>(device_get_shader_const_f_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetShaderConstI;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetShaderConstI<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetShaderConstI(Args... args) {
    return static_cast<Ret>(device_set_shader_const_i_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetShaderConstI<Ret(*)(Args...)> {
  static Ret pfnSetShaderConstI(Args... args) {
    return static_cast<Ret>(device_set_shader_const_i_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetShaderConstI;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstI<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetShaderConstI(Args... args) {
    return static_cast<Ret>(device_get_shader_const_i_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstI<Ret(*)(Args...)> {
  static Ret pfnGetShaderConstI(Args... args) {
    return static_cast<Ret>(device_get_shader_const_i_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnSetShaderConstB;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetShaderConstB<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnSetShaderConstB(Args... args) {
    return static_cast<Ret>(device_set_shader_const_b_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnSetShaderConstB<Ret(*)(Args...)> {
  static Ret pfnSetShaderConstB(Args... args) {
    return static_cast<Ret>(device_set_shader_const_b_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetShaderConstB;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstB<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetShaderConstB(Args... args) {
    return static_cast<Ret>(device_get_shader_const_b_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetShaderConstB<Ret(*)(Args...)> {
  static Ret pfnGetShaderConstB(Args... args) {
    return static_cast<Ret>(device_get_shader_const_b_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetFVF;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetFVF<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetFVF(Args... args) {
    return static_cast<Ret>(device_get_fvf_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetFVF<Ret(*)(Args...)> {
  static Ret pfnGetFVF(Args... args) {
    return static_cast<Ret>(device_get_fvf_dispatch(args...));
  }
};

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetVertexDecl;
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetVertexDecl<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetVertexDecl(Args... args) {
    return static_cast<Ret>(device_get_vertex_decl_dispatch(args...));
  }
};
template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetVertexDecl<Ret(*)(Args...)> {
  static Ret pfnGetVertexDecl(Args... args) {
    return static_cast<Ret>(device_get_vertex_decl_dispatch(args...));
  }
};

#endif // _WIN32 && AEROGPU_D3D9_USE_WDK_DDI

HRESULT AEROGPU_D3D9_CALL device_set_shader_const_i(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t start_reg,
    const int32_t* pData,
    uint32_t vec4_count) {
  const uint32_t st = stage;
  const uint32_t start = start_reg;
  const uint32_t count = vec4_count;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShaderConstI,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }

  // Be permissive: some header/runtime combinations may not use the exact {0,1}
  // encoding at the DDI boundary. Match the shader binding path, which treats
  // any non-VS stage as PS.
  const uint32_t stage_norm =
      (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  int32_t* dst = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_i
                                                    : dev->ps_consts_i;
  stateblock_record_shader_const_i_locked(dev, stage_norm, start, pData, count);

  const size_t payload_size =
      static_cast<size_t>(count) * 4u * sizeof(int32_t);
  if (std::memcmp(dst + start * 4u, pData, payload_size) == 0) {
    // Skip redundant constant uploads: setting identical constant data again is
    // a no-op, but still record it for state blocks above.
    return trace.ret(S_OK);
  }

  std::memcpy(dst + start * 4u, pData, payload_size);

  const uint32_t base = start * 4u;
  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_i>(
      dev, AEROGPU_CMD_SET_SHADER_CONSTANTS_I, dst + base, payload_size);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage_norm);
  cmd->start_register = start;
  cmd->vec4_count = count;
  cmd->reserved0 = 0;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_shader_const_b(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t start_reg,
    const BOOL* pData,
    uint32_t bool_count) {
  const uint32_t st = stage;
  const uint32_t start = start_reg;
  const uint32_t count = bool_count;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetShaderConstB,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(st),
                      d3d9_trace_pack_u32_u32(start, count),
                      d3d9_trace_arg_ptr(pData));
  if (!hDevice.pDrvPrivate || !pData || count == 0) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t stage_norm =
      (st == kD3d9ShaderStageVs) ? kD3d9ShaderStageVs : kD3d9ShaderStagePs;
  if (start >= 256 || count > 256u - start) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  uint8_t* dst = (stage_norm == kD3d9ShaderStageVs) ? dev->vs_consts_b
                                                    : dev->ps_consts_b;
  bool changed = false;
  for (uint32_t i = 0; i < count; ++i) {
    const uint8_t v = pData[i] ? 1u : 0u;
    if (dst[start + i] != v) {
      changed = true;
    }
    dst[start + i] = v;
  }
  stateblock_record_shader_const_b_locked(dev, stage_norm, start, dst + start,
                                          count);

  if (!changed) {
    // Skip redundant constant uploads: setting identical constant data again is
    // a no-op, but still record it for state blocks above.
    return trace.ret(S_OK);
  }

  // Payload uses a single u32 per bool register (0 or 1).
  std::array<uint32_t, 256> payload{};
  for (uint32_t i = 0; i < count; ++i) {
    payload[i] = dst[start + i] ? 1u : 0u;
  }
  auto* cmd = append_with_payload_locked<aerogpu_cmd_set_shader_constants_b>(
      dev,
      AEROGPU_CMD_SET_SHADER_CONSTANTS_B,
      payload.data(),
      static_cast<size_t>(count) * sizeof(uint32_t));
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->stage = d3d9_stage_to_aerogpu_stage(stage_norm);
  cmd->start_register = start;
  cmd->bool_count = count;
  cmd->reserved0 = 0;
  return trace.ret(S_OK);
}
// -----------------------------------------------------------------------------
// Device cursor DDIs (portable build)
// -----------------------------------------------------------------------------
//
// The WDK path implements these DDIs using the real D3D9 UMD DDI structs and
// header vintages. Portable builds only expose a minimal ABI subset, but we
// still want cursor behavior available for host-side tests.
#if !(defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI)
HRESULT AEROGPU_D3D9_CALL device_set_cursor_properties(D3DDDI_HDEVICE hDevice,
                                                       uint32_t x_hotspot,
                                                       uint32_t y_hotspot,
                                                       D3DDDI_HRESOURCE hCursorBitmap) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorProperties,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(x_hotspot, y_hotspot),
                      d3d9_trace_arg_ptr(hCursorBitmap.pDrvPrivate),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (!hCursorBitmap.pDrvPrivate) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  auto* cursor = as_resource(hCursorBitmap);
  if (!dev || !cursor) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t fmt = static_cast<uint32_t>(cursor->format);
  if (fmt != 21u /*D3DFMT_A8R8G8B8*/ && fmt != 22u /*D3DFMT_X8R8G8B8*/ && fmt != 32u /*D3DFMT_A8B8G8R8*/) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (cursor->width == 0 || cursor->height == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (x_hotspot >= cursor->width || y_hotspot >= cursor->height) {
    return trace.ret(kD3DErrInvalidCall);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->cursor_bitmap = cursor;
  dev->cursor_hot_x = x_hotspot;
  dev->cursor_hot_y = y_hotspot;
  dev->cursor_bitmap_serial++;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_cursor_position(D3DDDI_HDEVICE hDevice,
                                                     int32_t x,
                                                     int32_t y,
                                                     uint32_t flags) {
  (void)flags;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetCursorPosition,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(static_cast<uint32_t>(x), static_cast<uint32_t>(y)),
                      static_cast<uint64_t>(flags),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->cursor_x = x;
  dev->cursor_y = y;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_show_cursor(D3DDDI_HDEVICE hDevice, BOOL bShow) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceShowCursor,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(bShow ? 1u : 0u),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->cursor_visible = bShow ? TRUE : FALSE;
  return trace.ret(S_OK);
}
#endif
HRESULT AEROGPU_D3D9_CALL device_blt(D3DDDI_HDEVICE hDevice, const D3D9DDIARG_BLT* pBlt) {
  const D3DDDI_HRESOURCE src_h = pBlt ? d3d9_arg_src_resource(*pBlt) : D3DDDI_HRESOURCE{};
  const D3DDDI_HRESOURCE dst_h = pBlt ? d3d9_arg_dst_resource(*pBlt) : D3DDDI_HRESOURCE{};
  const uint32_t filter = pBlt ? d3d9_blt_filter(*pBlt) : 0;
  const uint32_t flags = pBlt ? d3d9_present_flags(*pBlt) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceBlt,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pBlt ? d3d9_trace_arg_ptr(src_h.pDrvPrivate) : 0,
                      pBlt ? d3d9_trace_arg_ptr(dst_h.pDrvPrivate) : 0,
                      pBlt ? d3d9_trace_pack_u32_u32(filter, flags) : 0);
  if (!hDevice.pDrvPrivate || !pBlt) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  auto* src = as_resource(src_h);
  auto* dst = as_resource(dst_h);

  std::lock_guard<std::mutex> lock(dev->mutex);

  return trace.ret(blit_locked(dev, dst, d3d9_update_surface_dst_rect(*pBlt), src, pBlt->pSrcRect, filter));
}

HRESULT AEROGPU_D3D9_CALL device_color_fill(D3DDDI_HDEVICE hDevice,
                                              const D3D9DDIARG_COLORFILL* pColorFill) {
  const D3DDDI_HRESOURCE dst_h = pColorFill ? d3d9_arg_dst_resource(*pColorFill) : D3DDDI_HRESOURCE{};
  const uint32_t color = pColorFill ? d3d9_color_fill_color(*pColorFill) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceColorFill,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pColorFill ? d3d9_trace_arg_ptr(dst_h.pDrvPrivate) : 0,
                      pColorFill ? static_cast<uint64_t>(color) : 0,
                      pColorFill ? static_cast<uint64_t>(pColorFill->pRect != nullptr ? 1u : 0u) : 0);
  if (!hDevice.pDrvPrivate || !pColorFill) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dst = as_resource(dst_h);
  std::lock_guard<std::mutex> lock(dev->mutex);
  return trace.ret(color_fill_locked(dev, dst, pColorFill->pRect, color));
}

HRESULT AEROGPU_D3D9_CALL device_update_surface(D3DDDI_HDEVICE hDevice,
                                                   const D3D9DDIARG_UPDATESURFACE* pUpdateSurface) {
  const D3DDDI_HRESOURCE src_h = pUpdateSurface ? d3d9_arg_src_resource(*pUpdateSurface) : D3DDDI_HRESOURCE{};
  const D3DDDI_HRESOURCE dst_h = pUpdateSurface ? d3d9_arg_dst_resource(*pUpdateSurface) : D3DDDI_HRESOURCE{};
  const RECT* dst_rect = pUpdateSurface ? d3d9_update_surface_dst_rect(*pUpdateSurface) : nullptr;
  const uint64_t rect_flags = pUpdateSurface ? d3d9_trace_pack_u32_u32(pUpdateSurface->pSrcRect != nullptr ? 1u : 0u,
                                                                       dst_rect != nullptr ? 1u : 0u)
                                             : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceUpdateSurface,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                       pUpdateSurface ? d3d9_trace_arg_ptr(src_h.pDrvPrivate) : 0,
                       pUpdateSurface ? d3d9_trace_arg_ptr(dst_h.pDrvPrivate) : 0,
                       rect_flags);
  if (!hDevice.pDrvPrivate || !pUpdateSurface) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  auto* src = as_resource(src_h);
  auto* dst = as_resource(dst_h);

  std::lock_guard<std::mutex> lock(dev->mutex);
  return trace.ret(update_surface_locked(dev, src, pUpdateSurface->pSrcRect, dst, d3d9_update_surface_dst_point(*pUpdateSurface)));
}

HRESULT AEROGPU_D3D9_CALL device_update_texture(D3DDDI_HDEVICE hDevice,
                                                   const D3D9DDIARG_UPDATETEXTURE* pUpdateTexture) {
  const D3DDDI_HRESOURCE src_h = pUpdateTexture ? d3d9_arg_src_resource(*pUpdateTexture) : D3DDDI_HRESOURCE{};
  const D3DDDI_HRESOURCE dst_h = pUpdateTexture ? d3d9_arg_dst_resource(*pUpdateTexture) : D3DDDI_HRESOURCE{};
  D3d9TraceCall trace(D3d9TraceFunc::DeviceUpdateTexture,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pUpdateTexture ? d3d9_trace_arg_ptr(src_h.pDrvPrivate) : 0,
                      pUpdateTexture ? d3d9_trace_arg_ptr(dst_h.pDrvPrivate) : 0,
                      0);
  if (!hDevice.pDrvPrivate || !pUpdateTexture) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  auto* src = as_resource(src_h);
  auto* dst = as_resource(dst_h);

  std::lock_guard<std::mutex> lock(dev->mutex);
  return trace.ret(update_texture_locked(dev, src, dst));
}

HRESULT AEROGPU_D3D9_CALL device_generate_mip_sub_levels(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hTexture) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGenerateMipSubLevels,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hTexture.pDrvPrivate),
                      0,
                      0);

  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* res = as_resource(hTexture);
  if (!dev || !res) {
    return trace.ret(E_INVALIDARG);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  if (res->locked) {
    return trace.ret(kD3DErrInvalidCall);
  }

  if (res->mip_levels < 2) {
    return trace.ret(kD3DErrInvalidCall);
  }
  const uint32_t layers = std::max(1u, res->depth);
  if (res->width == 0 || res->height == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (res->kind != ResourceKind::Texture2D && res->kind != ResourceKind::Surface) {
    return trace.ret(kD3DErrInvalidCall);
  }

  const bool is_bc = is_block_compressed_format(res->format);

  enum class BcKind {
    kBc1,
    kBc2,
    kBc3,
  };

  BcKind bc_kind = BcKind::kBc1;
  uint32_t bc_block_bytes = 0;

  enum class MipGenFormat {
    kA8R8G8B8,
    kX8R8G8B8,
    kA8B8G8R8,
    kR5G6B5,
    kX1R5G5B5,
    kA1R5G5B5,
  };

  MipGenFormat gen_format = MipGenFormat::kA8R8G8B8;
  uint32_t bpp = 0;
  if (is_bc) {
    switch (static_cast<uint32_t>(res->format)) {
      case static_cast<uint32_t>(kD3dFmtDxt1):
        bc_kind = BcKind::kBc1;
        bc_block_bytes = 8;
        break;
      case static_cast<uint32_t>(kD3dFmtDxt2):
      case static_cast<uint32_t>(kD3dFmtDxt3):
        bc_kind = BcKind::kBc2;
        bc_block_bytes = 16;
        break;
      case static_cast<uint32_t>(kD3dFmtDxt4):
      case static_cast<uint32_t>(kD3dFmtDxt5):
        bc_kind = BcKind::kBc3;
        bc_block_bytes = 16;
        break;
      default:
        return trace.ret(kD3DErrInvalidCall);
    }
  } else {
    switch (static_cast<uint32_t>(res->format)) {
      case 21u: // D3DFMT_A8R8G8B8
        gen_format = MipGenFormat::kA8R8G8B8;
        break;
      case 22u: // D3DFMT_X8R8G8B8
        gen_format = MipGenFormat::kX8R8G8B8;
        break;
      case 23u: // D3DFMT_R5G6B5
        gen_format = MipGenFormat::kR5G6B5;
        break;
      case 24u: // D3DFMT_X1R5G5B5
        gen_format = MipGenFormat::kX1R5G5B5;
        break;
      case 25u: // D3DFMT_A1R5G5B5
        gen_format = MipGenFormat::kA1R5G5B5;
        break;
      case 32u: // D3DFMT_A8B8G8R8
        gen_format = MipGenFormat::kA8B8G8R8;
        break;
      default: {
        static std::once_flag unsupported_fmt_once;
        const uint32_t fmt_u32 = static_cast<uint32_t>(res->format);
        std::call_once(unsupported_fmt_once, [fmt_u32] {
          aerogpu::logf("aerogpu-d3d9: GenerateMipSubLevels unsupported format=%u\n", static_cast<unsigned>(fmt_u32));
        });
        return trace.ret(kD3DErrInvalidCall);
      }
    }

    bpp = bytes_per_pixel(res->format);
    if (bpp != 4u && bpp != 2u) {
      return trace.ret(kD3DErrInvalidCall);
    }
  }

  // Validate the mip-chain layout against the resource size to prevent OOB writes on malformed descriptors.
  Texture2dLayout full_layout{};
  if (!calc_texture2d_layout(res->format, res->width, res->height, res->mip_levels, res->depth, &full_layout)) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (full_layout.total_size_bytes == 0 || full_layout.total_size_bytes > res->size_bytes) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if (layers == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }
  if ((full_layout.total_size_bytes % static_cast<uint64_t>(layers)) != 0) {
    return trace.ret(kD3DErrInvalidCall);
  }
  const uint64_t layer_size_bytes = full_layout.total_size_bytes / static_cast<uint64_t>(layers);
  if (layer_size_bytes == 0) {
    return trace.ret(kD3DErrInvalidCall);
  }

  uint8_t* base = nullptr;
  bool mapped = false;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  void* mapped_ptr = nullptr;
  // Only map WDDM allocations for guest-backed resources. Host-backed resources
  // (backing_alloc_id==0) may still carry a runtime-provided hAllocation, but
  // AeroGPU treats that backing as non-authoritative and uses `res->storage`
  // + UPLOAD_RESOURCE for updates.
  if (res->backing_alloc_id != 0 && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
    const HRESULT hr = wddm_lock_allocation(dev->wddm_callbacks,
                                           dev->wddm_device,
                                           res->wddm_hAllocation,
                                           /*offset=*/0,
                                           /*size=*/res->size_bytes,
                                           /*flags=*/0,
                                           &mapped_ptr,
                                           dev->wddm_context.hContext);
    if (FAILED(hr) || !mapped_ptr) {
      return trace.ret(FAILED(hr) ? hr : E_FAIL);
    }
    base = reinterpret_cast<uint8_t*>(mapped_ptr);
    mapped = true;
  } else
#endif
  {
    if (res->storage.size() < res->size_bytes) {
      return trace.ret(E_FAIL);
    }
    base = res->storage.data();
  }

  auto unlock_and_ret = [&](HRESULT hr_to_return) -> HRESULT {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (mapped && res->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT uhr =
          wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, res->wddm_hAllocation, dev->wddm_context.hContext);
      if (FAILED(uhr)) {
        aerogpu::logf("aerogpu-d3d9: GenerateMipSubLevels: UnlockCb failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
                      static_cast<unsigned long>(uhr),
                      static_cast<unsigned>(res->backing_alloc_id),
                      static_cast<unsigned long long>(res->wddm_hAllocation));
        if (SUCCEEDED(hr_to_return)) {
          hr_to_return = uhr;
        }
      }
    }
#endif
    return trace.ret(hr_to_return);
  };

  struct Rgba8 {
    uint8_t r = 0;
    uint8_t g = 0;
    uint8_t b = 0;
    uint8_t a = 0;
  };

  auto decode = [&](const uint8_t* p) -> Rgba8 {
    Rgba8 c{};
    switch (gen_format) {
      case MipGenFormat::kA8R8G8B8:
        // Stored as BGRA in little-endian memory.
        c.b = p[0];
        c.g = p[1];
        c.r = p[2];
        c.a = p[3];
        break;
      case MipGenFormat::kX8R8G8B8:
        c.b = p[0];
        c.g = p[1];
        c.r = p[2];
        // XRGB alpha is treated as 1.0.
        c.a = 0xFFu;
        break;
      case MipGenFormat::kR5G6B5: {
        const uint16_t v = static_cast<uint16_t>(p[0]) | (static_cast<uint16_t>(p[1]) << 8);
        const uint8_t r5 = static_cast<uint8_t>((v >> 11) & 0x1Fu);
        const uint8_t g6 = static_cast<uint8_t>((v >> 5) & 0x3Fu);
        const uint8_t b5 = static_cast<uint8_t>((v >> 0) & 0x1Fu);
        // Bit replication to 8-bit channels.
        c.r = static_cast<uint8_t>((r5 << 3) | (r5 >> 2));
        c.g = static_cast<uint8_t>((g6 << 2) | (g6 >> 4));
        c.b = static_cast<uint8_t>((b5 << 3) | (b5 >> 2));
        c.a = 0xFFu;
        break;
      }
      case MipGenFormat::kX1R5G5B5: {
        const uint16_t v = static_cast<uint16_t>(p[0]) | (static_cast<uint16_t>(p[1]) << 8);
        const uint8_t r5 = static_cast<uint8_t>((v >> 10) & 0x1Fu);
        const uint8_t g5 = static_cast<uint8_t>((v >> 5) & 0x1Fu);
        const uint8_t b5 = static_cast<uint8_t>((v >> 0) & 0x1Fu);
        c.r = static_cast<uint8_t>((r5 << 3) | (r5 >> 2));
        c.g = static_cast<uint8_t>((g5 << 3) | (g5 >> 2));
        c.b = static_cast<uint8_t>((b5 << 3) | (b5 >> 2));
        // X1 formats treat alpha as always-opaque.
        c.a = 0xFFu;
        break;
      }
      case MipGenFormat::kA1R5G5B5: {
        const uint16_t v = static_cast<uint16_t>(p[0]) | (static_cast<uint16_t>(p[1]) << 8);
        const uint8_t a1 = static_cast<uint8_t>((v >> 15) & 0x1u);
        const uint8_t r5 = static_cast<uint8_t>((v >> 10) & 0x1Fu);
        const uint8_t g5 = static_cast<uint8_t>((v >> 5) & 0x1Fu);
        const uint8_t b5 = static_cast<uint8_t>((v >> 0) & 0x1Fu);
        c.r = static_cast<uint8_t>((r5 << 3) | (r5 >> 2));
        c.g = static_cast<uint8_t>((g5 << 3) | (g5 >> 2));
        c.b = static_cast<uint8_t>((b5 << 3) | (b5 >> 2));
        c.a = a1 ? 0xFFu : 0u;
        break;
      }
      case MipGenFormat::kA8B8G8R8:
        // Stored as RGBA in little-endian memory.
        c.r = p[0];
        c.g = p[1];
        c.b = p[2];
        c.a = p[3];
        break;
    }
    return c;
  };

  auto encode = [&](uint8_t* p, const Rgba8& c) {
    switch (gen_format) {
      case MipGenFormat::kA8R8G8B8:
        p[0] = c.b;
        p[1] = c.g;
        p[2] = c.r;
        p[3] = c.a;
        break;
      case MipGenFormat::kX8R8G8B8:
        p[0] = c.b;
        p[1] = c.g;
        p[2] = c.r;
        p[3] = 0xFFu;
        break;
      case MipGenFormat::kR5G6B5: {
        const uint16_t r5 = static_cast<uint16_t>((static_cast<uint32_t>(c.r) * 31u + 127u) / 255u);
        const uint16_t g6 = static_cast<uint16_t>((static_cast<uint32_t>(c.g) * 63u + 127u) / 255u);
        const uint16_t b5 = static_cast<uint16_t>((static_cast<uint32_t>(c.b) * 31u + 127u) / 255u);
        const uint16_t v = static_cast<uint16_t>((r5 << 11) | (g6 << 5) | (b5 << 0));
        p[0] = static_cast<uint8_t>(v & 0xFFu);
        p[1] = static_cast<uint8_t>((v >> 8) & 0xFFu);
        break;
      }
      case MipGenFormat::kX1R5G5B5: {
        const uint16_t r5 = static_cast<uint16_t>((static_cast<uint32_t>(c.r) * 31u + 127u) / 255u);
        const uint16_t g5 = static_cast<uint16_t>((static_cast<uint32_t>(c.g) * 31u + 127u) / 255u);
        const uint16_t b5 = static_cast<uint16_t>((static_cast<uint32_t>(c.b) * 31u + 127u) / 255u);
        // X1 formats are treated as opaque alpha (top bit set).
        const uint16_t v = static_cast<uint16_t>((1u << 15) | (r5 << 10) | (g5 << 5) | (b5 << 0));
        p[0] = static_cast<uint8_t>(v & 0xFFu);
        p[1] = static_cast<uint8_t>((v >> 8) & 0xFFu);
        break;
      }
      case MipGenFormat::kA1R5G5B5: {
        const uint16_t a1 = static_cast<uint16_t>(c.a >= 128u ? 1u : 0u);
        const uint16_t r5 = static_cast<uint16_t>((static_cast<uint32_t>(c.r) * 31u + 127u) / 255u);
        const uint16_t g5 = static_cast<uint16_t>((static_cast<uint32_t>(c.g) * 31u + 127u) / 255u);
        const uint16_t b5 = static_cast<uint16_t>((static_cast<uint32_t>(c.b) * 31u + 127u) / 255u);
        const uint16_t v = static_cast<uint16_t>((a1 << 15) | (r5 << 10) | (g5 << 5) | (b5 << 0));
        p[0] = static_cast<uint8_t>(v & 0xFFu);
        p[1] = static_cast<uint8_t>((v >> 8) & 0xFFu);
        break;
      }
      case MipGenFormat::kA8B8G8R8:
        p[0] = c.r;
        p[1] = c.g;
        p[2] = c.b;
        p[3] = c.a;
        break;
    }
  };

  auto avg4 = [](uint8_t a, uint8_t b, uint8_t c, uint8_t d) -> uint8_t {
    const uint32_t sum = static_cast<uint32_t>(a) + static_cast<uint32_t>(b) + static_cast<uint32_t>(c) + static_cast<uint32_t>(d);
    // Round to nearest.
    return static_cast<uint8_t>((sum + 2u) / 4u);
  };

  auto read_u16_le = [](const uint8_t* p) -> uint16_t {
    return static_cast<uint16_t>(p[0]) | (static_cast<uint16_t>(p[1]) << 8);
  };
  auto write_u16_le = [](uint8_t* p, uint16_t v) {
    p[0] = static_cast<uint8_t>(v & 0xFFu);
    p[1] = static_cast<uint8_t>((v >> 8) & 0xFFu);
  };
  auto read_u32_le = [](const uint8_t* p) -> uint32_t {
    return static_cast<uint32_t>(p[0]) |
           (static_cast<uint32_t>(p[1]) << 8) |
           (static_cast<uint32_t>(p[2]) << 16) |
           (static_cast<uint32_t>(p[3]) << 24);
  };
  auto write_u32_le = [](uint8_t* p, uint32_t v) {
    p[0] = static_cast<uint8_t>(v & 0xFFu);
    p[1] = static_cast<uint8_t>((v >> 8) & 0xFFu);
    p[2] = static_cast<uint8_t>((v >> 16) & 0xFFu);
    p[3] = static_cast<uint8_t>((v >> 24) & 0xFFu);
  };
  auto read_u64_le = [](const uint8_t* p) -> uint64_t {
    uint64_t v = 0;
    for (uint32_t i = 0; i < 8; ++i) {
      v |= static_cast<uint64_t>(p[i]) << (8u * i);
    }
    return v;
  };
  auto write_u64_le = [](uint8_t* p, uint64_t v) {
    for (uint32_t i = 0; i < 8; ++i) {
      p[i] = static_cast<uint8_t>((v >> (8u * i)) & 0xFFu);
    }
  };

  auto rgb565_to_rgba8 = [](uint16_t v) -> Rgba8 {
    const uint8_t r5 = static_cast<uint8_t>((v >> 11) & 0x1Fu);
    const uint8_t g6 = static_cast<uint8_t>((v >> 5) & 0x3Fu);
    const uint8_t b5 = static_cast<uint8_t>((v >> 0) & 0x1Fu);
    Rgba8 c{};
    c.r = static_cast<uint8_t>((r5 << 3) | (r5 >> 2));
    c.g = static_cast<uint8_t>((g6 << 2) | (g6 >> 4));
    c.b = static_cast<uint8_t>((b5 << 3) | (b5 >> 2));
    c.a = 0xFFu;
    return c;
  };
  auto rgba8_to_rgb565 = [](const Rgba8& c) -> uint16_t {
    const uint16_t r5 = static_cast<uint16_t>((static_cast<uint32_t>(c.r) * 31u + 127u) / 255u);
    const uint16_t g6 = static_cast<uint16_t>((static_cast<uint32_t>(c.g) * 63u + 127u) / 255u);
    const uint16_t b5 = static_cast<uint16_t>((static_cast<uint32_t>(c.b) * 31u + 127u) / 255u);
    return static_cast<uint16_t>((r5 << 11) | (g6 << 5) | (b5 << 0));
  };

  auto bc_decode_color_palette = [&](uint16_t c0, uint16_t c1, bool allow_transparent, Rgba8 pal[4]) {
    pal[0] = rgb565_to_rgba8(c0);
    pal[1] = rgb565_to_rgba8(c1);
    if (allow_transparent && c0 <= c1) {
      pal[2].r = static_cast<uint8_t>((static_cast<uint32_t>(pal[0].r) + pal[1].r + 1u) / 2u);
      pal[2].g = static_cast<uint8_t>((static_cast<uint32_t>(pal[0].g) + pal[1].g + 1u) / 2u);
      pal[2].b = static_cast<uint8_t>((static_cast<uint32_t>(pal[0].b) + pal[1].b + 1u) / 2u);
      pal[2].a = 0xFFu;
      pal[3] = {};
      pal[3].a = 0u;
    } else {
      pal[2].r = static_cast<uint8_t>((2u * pal[0].r + pal[1].r + 1u) / 3u);
      pal[2].g = static_cast<uint8_t>((2u * pal[0].g + pal[1].g + 1u) / 3u);
      pal[2].b = static_cast<uint8_t>((2u * pal[0].b + pal[1].b + 1u) / 3u);
      pal[2].a = 0xFFu;

      pal[3].r = static_cast<uint8_t>((pal[0].r + 2u * pal[1].r + 1u) / 3u);
      pal[3].g = static_cast<uint8_t>((pal[0].g + 2u * pal[1].g + 1u) / 3u);
      pal[3].b = static_cast<uint8_t>((pal[0].b + 2u * pal[1].b + 1u) / 3u);
      pal[3].a = 0xFFu;
    }
  };

  auto bc_decode_block_rgba = [&](const uint8_t* src_block, Rgba8 out[16]) {
    uint8_t alpha[16];
    for (uint32_t i = 0; i < 16; ++i) {
      alpha[i] = 0xFFu;
    }

    const uint8_t* color_block = src_block;
    if (bc_kind == BcKind::kBc2) {
      const uint64_t a_bits = read_u64_le(src_block);
      for (uint32_t i = 0; i < 16; ++i) {
        const uint8_t nibble = static_cast<uint8_t>((a_bits >> (4u * i)) & 0xFu);
        alpha[i] = static_cast<uint8_t>(nibble * 17u);
      }
      color_block += 8;
    } else if (bc_kind == BcKind::kBc3) {
      const uint8_t a0 = src_block[0];
      const uint8_t a1 = src_block[1];
      uint64_t idx_bits = 0;
      for (uint32_t i = 0; i < 6; ++i) {
        idx_bits |= static_cast<uint64_t>(src_block[2 + i]) << (8u * i);
      }
      uint8_t a_tab[8]{};
      a_tab[0] = a0;
      a_tab[1] = a1;
      if (a0 > a1) {
        for (uint32_t i = 2; i < 8; ++i) {
          const uint32_t wa = 8u - i;
          const uint32_t wb = i - 1u;
          a_tab[i] = static_cast<uint8_t>((wa * a0 + wb * a1 + 3u) / 7u);
        }
      } else {
        for (uint32_t i = 2; i < 6; ++i) {
          const uint32_t wa = 6u - i;
          const uint32_t wb = i - 1u;
          a_tab[i] = static_cast<uint8_t>((wa * a0 + wb * a1 + 2u) / 5u);
        }
        a_tab[6] = 0u;
        a_tab[7] = 255u;
      }
      for (uint32_t i = 0; i < 16; ++i) {
        const uint8_t idx = static_cast<uint8_t>((idx_bits >> (3u * i)) & 0x7u);
        alpha[i] = a_tab[idx];
      }
      color_block += 8;
    }

    const uint16_t c0 = read_u16_le(color_block + 0);
    const uint16_t c1 = read_u16_le(color_block + 2);
    const uint32_t idx_bits = read_u32_le(color_block + 4);

    Rgba8 pal[4]{};
    const bool allow_transparent = (bc_kind == BcKind::kBc1);
    bc_decode_color_palette(c0, c1, allow_transparent, pal);

    for (uint32_t i = 0; i < 16; ++i) {
      const uint32_t idx = (idx_bits >> (2u * i)) & 0x3u;
      out[i] = pal[idx];
      if (bc_kind != BcKind::kBc1) {
        out[i].a = alpha[i];
      }
    }
  };

  auto bc_encode_block_rgba = [&](const Rgba8 in[16], uint8_t* dst_block) {
    uint8_t* color_block = dst_block;
    if (bc_kind == BcKind::kBc2) {
      uint64_t a_bits = 0;
      for (uint32_t i = 0; i < 16; ++i) {
        const uint32_t a4 = std::min<uint32_t>(15u, (static_cast<uint32_t>(in[i].a) + 8u) / 17u);
        a_bits |= (static_cast<uint64_t>(a4) & 0xFu) << (4u * i);
      }
      write_u64_le(dst_block, a_bits);
      color_block += 8;
    } else if (bc_kind == BcKind::kBc3) {
      uint8_t a_min = 255u;
      uint8_t a_max = 0u;
      for (uint32_t i = 0; i < 16; ++i) {
        a_min = std::min(a_min, in[i].a);
        a_max = std::max(a_max, in[i].a);
      }
      const uint8_t a0 = a_max;
      const uint8_t a1 = a_min;
      dst_block[0] = a0;
      dst_block[1] = a1;

      uint8_t a_tab[8]{};
      a_tab[0] = a0;
      a_tab[1] = a1;
      if (a0 > a1) {
        for (uint32_t i = 2; i < 8; ++i) {
          const uint32_t wa = 8u - i;
          const uint32_t wb = i - 1u;
          a_tab[i] = static_cast<uint8_t>((wa * a0 + wb * a1 + 3u) / 7u);
        }
      } else {
        for (uint32_t i = 2; i < 6; ++i) {
          const uint32_t wa = 6u - i;
          const uint32_t wb = i - 1u;
          a_tab[i] = static_cast<uint8_t>((wa * a0 + wb * a1 + 2u) / 5u);
        }
        a_tab[6] = 0u;
        a_tab[7] = 255u;
      }

      uint64_t idx_bits = 0;
      for (uint32_t i = 0; i < 16; ++i) {
        uint32_t best_idx = 0;
        uint32_t best_err = 0xFFFFFFFFu;
        for (uint32_t j = 0; j < 8; ++j) {
          const int32_t d = static_cast<int32_t>(in[i].a) - static_cast<int32_t>(a_tab[j]);
          const uint32_t err = static_cast<uint32_t>(d * d);
          if (err < best_err) {
            best_err = err;
            best_idx = j;
          }
        }
        idx_bits |= static_cast<uint64_t>(best_idx & 0x7u) << (3u * i);
      }
      for (uint32_t i = 0; i < 6; ++i) {
        dst_block[2 + i] = static_cast<uint8_t>((idx_bits >> (8u * i)) & 0xFFu);
      }
      color_block += 8;
    }

    bool any_transparent = false;
    if (bc_kind == BcKind::kBc1) {
      for (uint32_t i = 0; i < 16; ++i) {
        if (in[i].a < 128u) {
          any_transparent = true;
          break;
        }
      }
    }

    bool uniform_rgb = true;
    for (uint32_t i = 1; i < 16; ++i) {
      if (in[i].r != in[0].r || in[i].g != in[0].g || in[i].b != in[0].b) {
        uniform_rgb = false;
        break;
      }
    }

    uint16_t c0 = 0;
    uint16_t c1 = 0;
    uint32_t c_idx_bits = 0;

    if (uniform_rgb) {
      c0 = rgba8_to_rgb565(in[0]);
      c1 = c0;
      if (any_transparent) {
        for (uint32_t i = 0; i < 16; ++i) {
          const uint32_t idx = in[i].a < 128u ? 3u : 0u;
          c_idx_bits |= idx << (2u * i);
        }
      } else {
        c_idx_bits = 0;
      }
    } else {
      uint16_t min_c = 0xFFFFu;
      uint16_t max_c = 0u;
      for (uint32_t i = 0; i < 16; ++i) {
        if (any_transparent && in[i].a < 128u) {
          continue;
        }
        const uint16_t c = rgba8_to_rgb565(in[i]);
        min_c = std::min(min_c, c);
        max_c = std::max(max_c, c);
      }
      if (min_c == 0xFFFFu) {
        min_c = 0u;
        max_c = 0u;
      }

      if (any_transparent) {
        c0 = min_c;
        c1 = max_c;
        if (c0 > c1) {
          std::swap(c0, c1);
        }
      } else {
        c0 = max_c;
        c1 = min_c;
        if (bc_kind == BcKind::kBc1 && c0 < c1) {
          std::swap(c0, c1);
        }
      }

      Rgba8 pal[4]{};
      bc_decode_color_palette(c0, c1, any_transparent, pal);

      for (uint32_t i = 0; i < 16; ++i) {
        if (any_transparent && in[i].a < 128u) {
          c_idx_bits |= 3u << (2u * i);
          continue;
        }
        const uint32_t max_idx = any_transparent ? 3u : 4u;
        uint32_t best_idx = 0;
        uint32_t best_err = 0xFFFFFFFFu;
        for (uint32_t j = 0; j < max_idx; ++j) {
          const int32_t dr = static_cast<int32_t>(in[i].r) - static_cast<int32_t>(pal[j].r);
          const int32_t dg = static_cast<int32_t>(in[i].g) - static_cast<int32_t>(pal[j].g);
          const int32_t db = static_cast<int32_t>(in[i].b) - static_cast<int32_t>(pal[j].b);
          const uint32_t err = static_cast<uint32_t>(dr * dr + dg * dg + db * db);
          if (err < best_err) {
            best_err = err;
            best_idx = j;
          }
        }
        c_idx_bits |= (best_idx & 0x3u) << (2u * i);
      }
    }

    write_u16_le(color_block + 0, c0);
    write_u16_le(color_block + 2, c1);
    write_u32_le(color_block + 4, c_idx_bits);
  };

  std::vector<Rgba8> bc_src_pixels;
  std::vector<Rgba8> bc_dst_pixels;

  for (uint32_t layer = 0; layer < layers; ++layer) {
    const uint64_t layer_base = static_cast<uint64_t>(layer) * layer_size_bytes;
    if (layer_base > res->size_bytes) {
      return unlock_and_ret(kD3DErrInvalidCall);
    }

    for (uint32_t level = 0; level + 1 < res->mip_levels; ++level) {
      Texture2dMipLevelLayout src{};
      Texture2dMipLevelLayout dst{};
      if (!calc_texture2d_mip_level_layout(res->format, res->width, res->height, res->mip_levels, res->depth, level, &src) ||
          !calc_texture2d_mip_level_layout(res->format, res->width, res->height, res->mip_levels, res->depth, level + 1, &dst)) {
        return unlock_and_ret(kD3DErrInvalidCall);
      }

      const uint64_t src_off = layer_base + src.offset_bytes;
      const uint64_t dst_off = layer_base + dst.offset_bytes;
      const uint64_t src_end = src_off + static_cast<uint64_t>(src.slice_pitch_bytes);
      const uint64_t dst_end = dst_off + static_cast<uint64_t>(dst.slice_pitch_bytes);
      if (src_end > res->size_bytes || dst_end > res->size_bytes) {
        return unlock_and_ret(kD3DErrInvalidCall);
      }

      const uint8_t* src_base = base + static_cast<size_t>(src_off);
      uint8_t* dst_base = base + static_cast<size_t>(dst_off);

      const uint32_t src_w = src.width;
      const uint32_t src_h = src.height;
      const uint32_t dst_w = dst.width;
      const uint32_t dst_h = dst.height;

       if (src_w == 0 || src_h == 0 || dst_w == 0 || dst_h == 0) {
         return unlock_and_ret(kD3DErrInvalidCall);
       }

      if (is_bc) {
        if (bc_block_bytes == 0) {
          return unlock_and_ret(kD3DErrInvalidCall);
        }

        const uint32_t src_blocks_w = std::max(1u, (src_w + 3u) / 4u);
        const uint32_t src_blocks_h = std::max(1u, (src_h + 3u) / 4u);
        const uint32_t dst_blocks_w = std::max(1u, (dst_w + 3u) / 4u);
        const uint32_t dst_blocks_h = std::max(1u, (dst_h + 3u) / 4u);

        const uint64_t src_expected_row = static_cast<uint64_t>(src_blocks_w) * bc_block_bytes;
        const uint64_t dst_expected_row = static_cast<uint64_t>(dst_blocks_w) * bc_block_bytes;
        if (src_expected_row != src.row_pitch_bytes || dst_expected_row != dst.row_pitch_bytes) {
          return unlock_and_ret(kD3DErrInvalidCall);
        }
        if (static_cast<uint64_t>(src.row_pitch_bytes) * src_blocks_h != src.slice_pitch_bytes ||
            static_cast<uint64_t>(dst.row_pitch_bytes) * dst_blocks_h != dst.slice_pitch_bytes) {
          return unlock_and_ret(kD3DErrInvalidCall);
        }

        const uint64_t src_pixel_count_u64 = static_cast<uint64_t>(src_w) * static_cast<uint64_t>(src_h);
        const uint64_t dst_pixel_count_u64 = static_cast<uint64_t>(dst_w) * static_cast<uint64_t>(dst_h);
        if (src_pixel_count_u64 > std::numeric_limits<size_t>::max() ||
            dst_pixel_count_u64 > std::numeric_limits<size_t>::max()) {
          return unlock_and_ret(E_OUTOFMEMORY);
        }

        try {
          bc_src_pixels.resize(static_cast<size_t>(src_pixel_count_u64));
          bc_dst_pixels.resize(static_cast<size_t>(dst_pixel_count_u64));
        } catch (...) {
          return unlock_and_ret(E_OUTOFMEMORY);
        }

        // Decompress src mip into RGBA8 pixels.
        for (uint32_t by = 0; by < src_blocks_h; ++by) {
          const uint8_t* row = src_base + static_cast<size_t>(by) * src.row_pitch_bytes;
          for (uint32_t bx = 0; bx < src_blocks_w; ++bx) {
            const uint8_t* block = row + static_cast<size_t>(bx) * bc_block_bytes;
            Rgba8 block_px[16]{};
            bc_decode_block_rgba(block, block_px);

            for (uint32_t py = 0; py < 4; ++py) {
              const uint32_t y = by * 4u + py;
              if (y >= src_h) {
                continue;
              }
              for (uint32_t px = 0; px < 4; ++px) {
                const uint32_t x = bx * 4u + px;
                if (x >= src_w) {
                  continue;
                }
                bc_src_pixels[static_cast<size_t>(y) * src_w + x] = block_px[py * 4u + px];
              }
            }
          }
        }

        // Downsample into dst pixels using 2x2 box filter (clamped).
        for (uint32_t y = 0; y < dst_h; ++y) {
          const uint32_t sy0 = std::min<uint32_t>(2u * y, src_h - 1u);
          const uint32_t sy1 = std::min<uint32_t>(2u * y + 1u, src_h - 1u);
          for (uint32_t x = 0; x < dst_w; ++x) {
            const uint32_t sx0 = std::min<uint32_t>(2u * x, src_w - 1u);
            const uint32_t sx1 = std::min<uint32_t>(2u * x + 1u, src_w - 1u);

            const Rgba8 c00 = bc_src_pixels[static_cast<size_t>(sy0) * src_w + sx0];
            const Rgba8 c10 = bc_src_pixels[static_cast<size_t>(sy0) * src_w + sx1];
            const Rgba8 c01 = bc_src_pixels[static_cast<size_t>(sy1) * src_w + sx0];
            const Rgba8 c11 = bc_src_pixels[static_cast<size_t>(sy1) * src_w + sx1];

            Rgba8 out{};
            out.r = avg4(c00.r, c10.r, c01.r, c11.r);
            out.g = avg4(c00.g, c10.g, c01.g, c11.g);
            out.b = avg4(c00.b, c10.b, c01.b, c11.b);
            out.a = avg4(c00.a, c10.a, c01.a, c11.a);
            bc_dst_pixels[static_cast<size_t>(y) * dst_w + x] = out;
          }
        }

        // Compress dst pixels into BC blocks.
        for (uint32_t by = 0; by < dst_blocks_h; ++by) {
          uint8_t* row = dst_base + static_cast<size_t>(by) * dst.row_pitch_bytes;
          for (uint32_t bx = 0; bx < dst_blocks_w; ++bx) {
            Rgba8 block_px[16]{};
            for (uint32_t py = 0; py < 4; ++py) {
              const uint32_t y = std::min<uint32_t>(by * 4u + py, dst_h - 1u);
              for (uint32_t px = 0; px < 4; ++px) {
                const uint32_t x = std::min<uint32_t>(bx * 4u + px, dst_w - 1u);
                block_px[py * 4u + px] = bc_dst_pixels[static_cast<size_t>(y) * dst_w + x];
              }
            }
            bc_encode_block_rgba(block_px, row + static_cast<size_t>(bx) * bc_block_bytes);
          }
        }
      } else {
        for (uint32_t y = 0; y < dst_h; ++y) {
          const uint32_t sy0 = std::min<uint32_t>(2u * y, src_h - 1u);
          const uint32_t sy1 = std::min<uint32_t>(2u * y + 1u, src_h - 1u);

          const uint8_t* row0 = src_base + static_cast<size_t>(sy0) * src.row_pitch_bytes;
          const uint8_t* row1 = src_base + static_cast<size_t>(sy1) * src.row_pitch_bytes;
          uint8_t* out_row = dst_base + static_cast<size_t>(y) * dst.row_pitch_bytes;

          for (uint32_t x = 0; x < dst_w; ++x) {
            const uint32_t sx0 = std::min<uint32_t>(2u * x, src_w - 1u);
            const uint32_t sx1 = std::min<uint32_t>(2u * x + 1u, src_w - 1u);

            const uint8_t* p00 = row0 + static_cast<size_t>(sx0) * bpp;
            const uint8_t* p10 = row0 + static_cast<size_t>(sx1) * bpp;
            const uint8_t* p01 = row1 + static_cast<size_t>(sx0) * bpp;
            const uint8_t* p11 = row1 + static_cast<size_t>(sx1) * bpp;

            const Rgba8 c00 = decode(p00);
            const Rgba8 c10 = decode(p10);
            const Rgba8 c01 = decode(p01);
            const Rgba8 c11 = decode(p11);

            Rgba8 out{};
            out.r = avg4(c00.r, c10.r, c01.r, c11.r);
            out.g = avg4(c00.g, c10.g, c01.g, c11.g);
            out.b = avg4(c00.b, c10.b, c01.b, c11.b);
            const bool opaque_alpha = (gen_format == MipGenFormat::kX8R8G8B8) ||
                                      (gen_format == MipGenFormat::kX1R5G5B5);
            out.a = opaque_alpha ? 0xFFu : avg4(c00.a, c10.a, c01.a, c11.a);

            encode(out_row + static_cast<size_t>(x) * bpp, out);
          }
        }
      }
    }
  }

  // Host visibility:
  // - Guest-backed: signal the dirty byte range so the host can re-upload from guest memory.
  // - Host-allocated: upload the generated bytes (mips beyond level 0) directly via UPLOAD_RESOURCE.
  if (res->handle != 0) {
    // GenerateMipSubLevels only writes mips beyond level 0.
    Texture2dMipLevelLayout mip1{};
    if (!calc_texture2d_mip_level_layout(res->format, res->width, res->height, res->mip_levels, res->depth, /*level=*/1, &mip1)) {
      return unlock_and_ret(kD3DErrInvalidCall);
    }
    if (mip1.offset_bytes > layer_size_bytes || mip1.offset_bytes > res->size_bytes) {
      return unlock_and_ret(kD3DErrInvalidCall);
    }

    for (uint32_t layer = 0; layer < layers; ++layer) {
      const uint64_t layer_base = static_cast<uint64_t>(layer) * layer_size_bytes;
      const uint64_t start = layer_base + mip1.offset_bytes;
      const uint64_t end = layer_base + layer_size_bytes;
      if (start > end || end > res->size_bytes) {
        return unlock_and_ret(kD3DErrInvalidCall);
      }
      const uint64_t size = end - start;
      if (size == 0) {
        continue;
      }

      if (res->backing_alloc_id != 0) {
        if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_resource_dirty_range), 4))) {
          return unlock_and_ret(E_OUTOFMEMORY);
        }
        const HRESULT track_hr = track_resource_allocation_locked(dev, res, /*write=*/false);
        if (FAILED(track_hr)) {
          return unlock_and_ret(track_hr);
        }
        auto* cmd = append_fixed_locked<aerogpu_cmd_resource_dirty_range>(dev, AEROGPU_CMD_RESOURCE_DIRTY_RANGE);
        if (!cmd) {
          return unlock_and_ret(E_OUTOFMEMORY);
        }
        cmd->resource_handle = res->handle;
        cmd->reserved0 = 0;
        cmd->offset_bytes = start;
        cmd->size_bytes = size;
      } else {
        if (start > 0xFFFFFFFFull || size > 0xFFFFFFFFull) {
          return unlock_and_ret(E_FAIL);
        }
        const HRESULT upload_hr = emit_upload_resource_range_locked(dev,
                                                                    res,
                                                                    static_cast<uint32_t>(start),
                                                                    static_cast<uint32_t>(size));
        if (FAILED(upload_hr)) {
          return unlock_and_ret(upload_hr);
        }
      }
    }
  }

  return unlock_and_ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_stream_source(
    D3DDDI_HDEVICE hDevice,
    uint32_t stream,
    D3DDDI_HRESOURCE hVb,
    uint32_t offset_bytes,
    uint32_t stride_bytes) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetStreamSource,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(stream),
                      d3d9_trace_arg_ptr(hVb.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(offset_bytes, stride_bytes));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  if (stream >= 16) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* vb = as_resource(hVb);

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (!emit_set_stream_source_locked(dev, stream, vb, offset_bytes, stride_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }
  stateblock_record_stream_source_locked(dev, stream, dev->streams[stream]);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_indices(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hIb,
    D3DDDIFORMAT fmt,
    uint32_t offset_bytes) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetIndices,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hIb.pDrvPrivate),
                      d3d9_trace_pack_u32_u32(static_cast<uint32_t>(fmt), offset_bytes),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  auto* ib = as_resource(hIb);

  std::lock_guard<std::mutex> lock(dev->mutex);

  dev->index_buffer = ib;
  dev->index_format = fmt;
  dev->index_offset_bytes = offset_bytes;
  stateblock_record_index_buffer_locked(dev, ib, fmt, offset_bytes);

  auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->buffer = ib ? ib->handle : 0;
  cmd->format = d3d9_index_format_to_aerogpu(fmt);
  cmd->offset_bytes = offset_bytes;
  cmd->reserved0 = 0;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_begin_scene(D3DDDI_HDEVICE hDevice) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return E_INVALIDARG;
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->scene_depth++;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_end_scene(D3DDDI_HDEVICE hDevice) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return E_INVALIDARG;
  }

  std::lock_guard<std::mutex> lock(dev->mutex);
  if (dev->scene_depth > 0) {
    dev->scene_depth--;
  }
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_clear(
    D3DDDI_HDEVICE hDevice,
    uint32_t flags,
    uint32_t color_rgba8,
    float depth,
    uint32_t stencil) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceClear,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(flags),
                      static_cast<uint64_t>(color_rgba8),
                      d3d9_trace_pack_u32_u32(f32_bits(depth), stencil));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_clear), 4))) {
    return E_OUTOFMEMORY;
  }

  HRESULT hr = track_render_targets_locked(dev);
  if (hr < 0) {
    return hr;
  }

  const float a = static_cast<float>((color_rgba8 >> 24) & 0xFF) / 255.0f;
  const float r = static_cast<float>((color_rgba8 >> 16) & 0xFF) / 255.0f;
  const float g = static_cast<float>((color_rgba8 >> 8) & 0xFF) / 255.0f;
  const float b = static_cast<float>((color_rgba8 >> 0) & 0xFF) / 255.0f;

  auto* cmd = append_fixed_locked<aerogpu_cmd_clear>(dev, AEROGPU_CMD_CLEAR);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->flags = flags;
  cmd->color_rgba_f32[0] = f32_bits(r);
  cmd->color_rgba_f32[1] = f32_bits(g);
  cmd->color_rgba_f32[2] = f32_bits(b);
  cmd->color_rgba_f32[3] = f32_bits(a);
  cmd->depth_f32 = f32_bits(depth);
  cmd->stencil = stencil;
  return trace.ret(S_OK);
}

namespace {

bool patch_sig_equal(const PatchCacheSignature& a, const PatchCacheSignature& b) {
  if (a.kind != b.kind) return false;
  // D3DFVF_TEXCOORDSIZE* bits can contain garbage for *unused* texcoord sets. Patch
  // emulation supports float1/2/3/4 TEXCOORD0 when TEX1 is present, so cache keys
  // must:
  // - ignore garbage size bits for unused sets, but
  // - still distinguish float1 vs float2/3/4 for TEX0 so we don't accidentally
  //   reuse tessellated vertices with a different TEXCOORD0 layout.
  if (fvf_layout_key(a.fvf) != fvf_layout_key(b.fvf)) return false;
  if (a.stride_bytes != b.stride_bytes) return false;
  if (a.start_vertex_offset != b.start_vertex_offset) return false;
  if (a.num_vertices != b.num_vertices) return false;
  if (a.basis != b.basis) return false;
  if (a.degree != b.degree) return false;
  for (size_t i = 0; i < 4; ++i) {
    if (a.seg_bits[i] != b.seg_bits[i]) return false;
  }
  if (a.control_point_hash != b.control_point_hash) return false;
  return true;
}

uint32_t clamp_patch_segs(float v) {
  if (!std::isfinite(v) || v <= 0.0f) {
    return 1u;
  }
  if (v >= 64.0f) {
    return 64u;
  }
  long r = std::lround(static_cast<double>(v));
  if (r < 1) {
    return 1u;
  }
  if (r > 64) {
    return 64u;
  }
  return static_cast<uint32_t>(r);
}

struct PatchEvalPoint {
  float x = 0.0f;
  float y = 0.0f;
  float z = 0.0f;
  float rhw = 1.0f;
  float a = 255.0f;
  float r = 255.0f;
  float g = 255.0f;
  float b = 255.0f;
  float u = 0.0f;
  float v = 0.0f;
};

void unpack_color_u32(uint32_t c, PatchEvalPoint* out) {
  if (!out) {
    return;
  }
  out->a = static_cast<float>((c >> 24) & 0xFF);
  out->r = static_cast<float>((c >> 16) & 0xFF);
  out->g = static_cast<float>((c >> 8) & 0xFF);
  out->b = static_cast<float>((c >> 0) & 0xFF);
}

uint32_t pack_color_u32(const PatchEvalPoint& p) {
  auto clamp_u8 = [](float v) -> uint32_t {
    if (!std::isfinite(v)) {
      return 0u;
    }
    v = std::clamp(v, 0.0f, 255.0f);
    return static_cast<uint32_t>(std::lround(static_cast<double>(v)));
  };
  const uint32_t a = clamp_u8(p.a);
  const uint32_t r = clamp_u8(p.r);
  const uint32_t g = clamp_u8(p.g);
  const uint32_t b = clamp_u8(p.b);
  return (a << 24) | (r << 16) | (g << 8) | b;
}

void bezier_cubic_weights(float t, float out_w[4]) {
  const float s = 1.0f - t;
  const float s2 = s * s;
  const float t2 = t * t;
  out_w[0] = s2 * s;
  out_w[1] = 3.0f * t * s2;
  out_w[2] = 3.0f * t2 * s;
  out_w[3] = t2 * t;
}

HRESULT tessellate_rect_patch_cubic(
    const uint8_t* control_points,
    uint32_t stride_bytes,
    bool has_tex0,
    uint32_t tex0_dim,
    uint32_t seg_u,
    uint32_t seg_v,
    PatchCacheEntry* out) {
  if (!control_points || stride_bytes < 20 || seg_u == 0 || seg_v == 0 || !out) {
    return E_INVALIDARG;
  }
  if (has_tex0) {
    if (tex0_dim < 1u || tex0_dim > 4u) {
      return E_INVALIDARG;
    }
    if (stride_bytes < 24u) {
      return E_INVALIDARG;
    }
    if (tex0_dim >= 2u && stride_bytes < 28u) {
      return E_INVALIDARG;
    }
  }
  // Only cubic Bezier rect patches are supported: 4x4 control points.
  PatchEvalPoint cp[16]{};
  for (uint32_t i = 0; i < 16; ++i) {
    const uint8_t* src = control_points + static_cast<size_t>(i) * stride_bytes;
    cp[i].x = read_f32_unaligned(src + 0);
    cp[i].y = read_f32_unaligned(src + 4);
    cp[i].z = read_f32_unaligned(src + 8);
    cp[i].rhw = read_f32_unaligned(src + 12);
    uint32_t c = 0;
    std::memcpy(&c, src + 16, sizeof(c));
    unpack_color_u32(c, &cp[i]);
    if (has_tex0) {
      // TEXCOORD0 is always stored immediately after DIFFUSE.
      // - float1: u at +20, v implicitly 0
      // - float2+: u at +20, v at +24
      cp[i].u = read_f32_unaligned(src + 20);
      if (tex0_dim >= 2u) {
        cp[i].v = read_f32_unaligned(src + 24);
      }
    }
  }

  const uint32_t vert_w = seg_u + 1;
  const uint32_t vert_h = seg_v + 1;
  const uint64_t vert_count_u64 = static_cast<uint64_t>(vert_w) * static_cast<uint64_t>(vert_h);
  if (vert_count_u64 == 0 || vert_count_u64 > 0x7FFFFFFFull) {
    return E_INVALIDARG;
  }
  const uint32_t vert_count = static_cast<uint32_t>(vert_count_u64);

  const uint64_t vb_bytes_u64 = vert_count_u64 * stride_bytes;
  if (vb_bytes_u64 == 0 || vb_bytes_u64 > 0x7FFFFFFFull) {
    return E_INVALIDARG;
  }

  try {
    out->vertices.resize(static_cast<size_t>(vb_bytes_u64));
    out->indices_u16.clear();
    out->indices_u16.reserve(static_cast<size_t>(seg_u) * static_cast<size_t>(seg_v) * 6);
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  const uint8_t* template_vertex = control_points;

  for (uint32_t y = 0; y < vert_h; ++y) {
    const float v = static_cast<float>(y) / static_cast<float>(seg_v);
    float wv[4];
    bezier_cubic_weights(v, wv);
    for (uint32_t x = 0; x < vert_w; ++x) {
      const float u = static_cast<float>(x) / static_cast<float>(seg_u);
      float wu[4];
      bezier_cubic_weights(u, wu);

      PatchEvalPoint p{};
      p.x = p.y = p.z = p.rhw = 0.0f;
      p.a = p.r = p.g = p.b = 0.0f;
      p.u = p.v = 0.0f;

      for (uint32_t j = 0; j < 4; ++j) {
        for (uint32_t i = 0; i < 4; ++i) {
          const float w = wu[i] * wv[j];
          const PatchEvalPoint& c = cp[j * 4 + i];
          p.x += c.x * w;
          p.y += c.y * w;
          p.z += c.z * w;
          p.rhw += c.rhw * w;
          p.a += c.a * w;
          p.r += c.r * w;
          p.g += c.g * w;
          p.b += c.b * w;
          if (has_tex0) {
            p.u += c.u * w;
            p.v += c.v * w;
          }
        }
      }

      const uint32_t color = pack_color_u32(p);
      uint8_t* dst = out->vertices.data() + (static_cast<size_t>(y) * vert_w + x) * stride_bytes;
      std::memcpy(dst, template_vertex, stride_bytes);
      write_f32_unaligned(dst + 0, p.x);
      write_f32_unaligned(dst + 4, p.y);
      write_f32_unaligned(dst + 8, p.z);
      write_f32_unaligned(dst + 12, p.rhw);
      std::memcpy(dst + 16, &color, sizeof(color));
      if (has_tex0) {
        write_f32_unaligned(dst + 20, p.u);
        if (tex0_dim >= 2u) {
          write_f32_unaligned(dst + 24, p.v);
        }
      }
    }
  }

  for (uint32_t y = 0; y < seg_v; ++y) {
    for (uint32_t x = 0; x < seg_u; ++x) {
      const uint16_t v0 = static_cast<uint16_t>(y * vert_w + x);
      const uint16_t v1 = static_cast<uint16_t>(v0 + 1);
      const uint16_t v2 = static_cast<uint16_t>(v0 + vert_w);
      const uint16_t v3 = static_cast<uint16_t>(v2 + 1);
      out->indices_u16.push_back(v0);
      out->indices_u16.push_back(v1);
      out->indices_u16.push_back(v2);
      out->indices_u16.push_back(v1);
      out->indices_u16.push_back(v3);
      out->indices_u16.push_back(v2);
    }
  }

  return S_OK;
}

HRESULT tessellate_tri_patch_cubic(
    const uint8_t* control_points,
    uint32_t stride_bytes,
    bool has_tex0,
    uint32_t tex0_dim,
    uint32_t segs,
    PatchCacheEntry* out) {
  if (!control_points || stride_bytes < 20 || segs == 0 || !out) {
    return E_INVALIDARG;
  }
  if (has_tex0) {
    if (tex0_dim < 1u || tex0_dim > 4u) {
      return E_INVALIDARG;
    }
    if (stride_bytes < 24u) {
      return E_INVALIDARG;
    }
    if (tex0_dim >= 2u && stride_bytes < 28u) {
      return E_INVALIDARG;
    }
  }

  // Only cubic Bezier tri patches are supported: 10 control points.
  PatchEvalPoint cp[10]{};
  for (uint32_t i = 0; i < 10; ++i) {
    const uint8_t* src = control_points + static_cast<size_t>(i) * stride_bytes;
    cp[i].x = read_f32_unaligned(src + 0);
    cp[i].y = read_f32_unaligned(src + 4);
    cp[i].z = read_f32_unaligned(src + 8);
    cp[i].rhw = read_f32_unaligned(src + 12);
    uint32_t c = 0;
    std::memcpy(&c, src + 16, sizeof(c));
    unpack_color_u32(c, &cp[i]);
    if (has_tex0) {
      cp[i].u = read_f32_unaligned(src + 20);
      if (tex0_dim >= 2u) {
        cp[i].v = read_f32_unaligned(src + 24);
      }
    }
  }

  const uint64_t vert_count_u64 =
      static_cast<uint64_t>(segs + 1) * static_cast<uint64_t>(segs + 2) / 2ull;
  if (vert_count_u64 == 0 || vert_count_u64 > 0x7FFFFFFFull) {
    return E_INVALIDARG;
  }
  const uint64_t vb_bytes_u64 = vert_count_u64 * stride_bytes;
  if (vb_bytes_u64 == 0 || vb_bytes_u64 > 0x7FFFFFFFull) {
    return E_INVALIDARG;
  }

  try {
    out->vertices.resize(static_cast<size_t>(vb_bytes_u64));
    out->indices_u16.clear();
    out->indices_u16.reserve(static_cast<size_t>(segs) * static_cast<size_t>(segs) * 3);
  } catch (...) {
    return E_OUTOFMEMORY;
  }

  const uint8_t* template_vertex = control_points;
  const auto index_of = [segs](uint32_t i, uint32_t j) -> uint32_t {
    // Row i has (segs+1-i) vertices.
    return i * (segs + 1) - (i * (i - 1)) / 2 + j;
  };

  uint32_t out_idx = 0;
  for (uint32_t i = 0; i <= segs; ++i) {
    for (uint32_t j = 0; j <= (segs - i); ++j) {
      const float u = static_cast<float>(i) / static_cast<float>(segs);
      const float v = static_cast<float>(j) / static_cast<float>(segs);
      const float w = 1.0f - u - v;

      const float u2 = u * u;
      const float v2 = v * v;
      const float w2 = w * w;

      const float u3 = u2 * u;
      const float v3 = v2 * v;
      const float w3 = w2 * w;

      // Assumed control point order:
      // [0]=u^3, [1]=3u^2v, [2]=3uv^2, [3]=v^3,
      // [4]=3u^2w, [5]=6uvw, [6]=3v^2w,
      // [7]=3uw^2, [8]=3vw^2, [9]=w^3.
      const float terms[10] = {
          u3,
          3.0f * u2 * v,
          3.0f * u * v2,
          v3,
          3.0f * u2 * w,
          6.0f * u * v * w,
          3.0f * v2 * w,
          3.0f * u * w2,
          3.0f * v * w2,
          w3,
      };

      PatchEvalPoint p{};
      p.x = p.y = p.z = p.rhw = 0.0f;
      p.a = p.r = p.g = p.b = 0.0f;
      p.u = p.v = 0.0f;
      for (uint32_t k = 0; k < 10; ++k) {
        const float t = terms[k];
        const PatchEvalPoint& c = cp[k];
        p.x += c.x * t;
        p.y += c.y * t;
        p.z += c.z * t;
        p.rhw += c.rhw * t;
        p.a += c.a * t;
        p.r += c.r * t;
        p.g += c.g * t;
        p.b += c.b * t;
        if (has_tex0) {
          p.u += c.u * t;
          p.v += c.v * t;
        }
      }

      const uint32_t color = pack_color_u32(p);
      uint8_t* dst = out->vertices.data() + static_cast<size_t>(out_idx) * stride_bytes;
      out_idx++;
      std::memcpy(dst, template_vertex, stride_bytes);
      write_f32_unaligned(dst + 0, p.x);
      write_f32_unaligned(dst + 4, p.y);
      write_f32_unaligned(dst + 8, p.z);
      write_f32_unaligned(dst + 12, p.rhw);
      std::memcpy(dst + 16, &color, sizeof(color));
      if (has_tex0) {
        write_f32_unaligned(dst + 20, p.u);
        if (tex0_dim >= 2u) {
          write_f32_unaligned(dst + 24, p.v);
        }
      }
    }
  }

  for (uint32_t i = 0; i < segs; ++i) {
    for (uint32_t j = 0; j < (segs - i); ++j) {
      const uint32_t i0 = index_of(i, j);
      const uint32_t i1 = index_of(i + 1, j);
      const uint32_t i2 = index_of(i, j + 1);
      out->indices_u16.push_back(static_cast<uint16_t>(i0));
      out->indices_u16.push_back(static_cast<uint16_t>(i1));
      out->indices_u16.push_back(static_cast<uint16_t>(i2));

      if (j + 1 < (segs - i)) {
        const uint32_t i3 = index_of(i + 1, j + 1);
        out->indices_u16.push_back(static_cast<uint16_t>(i1));
        out->indices_u16.push_back(static_cast<uint16_t>(i3));
        out->indices_u16.push_back(static_cast<uint16_t>(i2));
      }
    }
  }

  return S_OK;
}

} // namespace

HRESULT AEROGPU_D3D9_CALL device_draw_rect_patch(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWRECTPATCH* pDrawRectPatch) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawRectPatch,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pDrawRectPatch ? static_cast<uint64_t>(pDrawRectPatch->Handle) : 0,
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pDrawRectPatch || !pDrawRectPatch->pNumSegs || !pDrawRectPatch->pRectPatchInfo) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  if (device_is_lost(dev)) {
    return trace.ret(kD3dErrDeviceLost);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return trace.ret(kD3dErrDeviceLost);
  }

  if (!fixedfunc_fvf_supported(dev->fvf) || dev->user_vs || dev->user_ps) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  DeviceStateStream& ss = dev->streams[0];
  const uint32_t min_stride = fixedfunc_min_stride_bytes(dev->fvf);
  if (!ss.vb || min_stride == 0 || ss.stride_bytes < min_stride) {
    return trace.ret(E_FAIL);
  }

  const D3DRECTPATCH_INFO* info = pDrawRectPatch->pRectPatchInfo;
  if (!info || info->Basis != D3DBASIS_BEZIER || info->Degree != D3DDEGREE_CUBIC || info->NumVertices != 16) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  const uint32_t seg_bits[4] = {
      f32_bits(pDrawRectPatch->pNumSegs[0]),
      f32_bits(pDrawRectPatch->pNumSegs[1]),
      f32_bits(pDrawRectPatch->pNumSegs[2]),
      f32_bits(pDrawRectPatch->pNumSegs[3]),
  };

  const float seg_f0 = pDrawRectPatch->pNumSegs[0];
  const float seg_f1 = pDrawRectPatch->pNumSegs[1];
  const float seg_f2 = pDrawRectPatch->pNumSegs[2];
  const float seg_f3 = pDrawRectPatch->pNumSegs[3];

  const uint32_t seg_u = clamp_patch_segs(std::max(seg_f0, seg_f2));
  const uint32_t seg_v = clamp_patch_segs(std::max(seg_f1, seg_f3));

  const uint32_t handle = static_cast<uint32_t>(pDrawRectPatch->Handle);
  const uint32_t start_vertex = static_cast<uint32_t>(info->StartVertexOffset);
  const uint32_t cp_count = static_cast<uint32_t>(info->NumVertices);
  const uint64_t src_offset_u64 = static_cast<uint64_t>(ss.offset_bytes) +
                                  static_cast<uint64_t>(start_vertex) * static_cast<uint64_t>(ss.stride_bytes);
  const uint64_t size_u64 = static_cast<uint64_t>(cp_count) * static_cast<uint64_t>(ss.stride_bytes);
  const uint64_t vb_size_u64 = ss.vb->size_bytes;
  if (src_offset_u64 > vb_size_u64 || size_u64 > vb_size_u64 - src_offset_u64) {
    return trace.ret(E_INVALIDARG);
  }

  const uint8_t* control_bytes = nullptr;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  void* vb_ptr = nullptr;
  bool vb_locked = false;
#endif

  bool use_vb_storage = ss.vb->storage.size() >= static_cast<size_t>(src_offset_u64 + size_u64);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (ss.vb->backing_alloc_id != 0) {
    use_vb_storage = false;
  }
#endif

  if (use_vb_storage) {
    control_bytes = ss.vb->storage.data() + static_cast<size_t>(src_offset_u64);
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (ss.vb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                   dev->wddm_device,
                                                   ss.vb->wddm_hAllocation,
                                                   src_offset_u64,
                                                   size_u64,
                                                   kD3DLOCK_READONLY,
                                                   &vb_ptr,
                                                   dev->wddm_context.hContext);
      if (FAILED(lock_hr) || !vb_ptr) {
        return trace.ret(FAILED(lock_hr) ? lock_hr : E_FAIL);
      }
      vb_locked = true;
      control_bytes = static_cast<const uint8_t*>(vb_ptr);
    } else
#endif
    {
      return trace.ret(E_INVALIDARG);
    }
  }

  const uint64_t control_hash = fnv1a64_hash(control_bytes, static_cast<size_t>(size_u64));

  PatchCacheSignature sig{};
  sig.kind = PatchKind::Rect;
  sig.fvf = dev->fvf;
  sig.stride_bytes = ss.stride_bytes;
  sig.start_vertex_offset = start_vertex;
  sig.num_vertices = cp_count;
  sig.basis = static_cast<uint32_t>(info->Basis);
  sig.degree = static_cast<uint32_t>(info->Degree);
  sig.seg_bits[0] = seg_bits[0];
  sig.seg_bits[1] = seg_bits[1];
  sig.seg_bits[2] = seg_bits[2];
  sig.seg_bits[3] = seg_bits[3];
  sig.control_point_hash = control_hash;

  PatchCacheEntry temp{};
  PatchCacheEntry* entry = &temp;
  if (handle != 0) {
    auto it = dev->patch_cache.find(handle);
    if (it != dev->patch_cache.end()) {
      entry = &it->second;
    } else {
      // Patch cache is best-effort; if we cannot allocate/insert (OOM), fall back to
      // a local temp entry so exceptions never escape driver code.
      try {
        auto [new_it, inserted] = dev->patch_cache.emplace(handle, PatchCacheEntry{});
        (void)inserted;
        entry = &new_it->second;
      } catch (...) {
        entry = &temp;
      }
    }
  }

  HRESULT hr = S_OK;
  const bool has_tex0 = (fvf_variant == FixedFuncVariant::RHW_COLOR_TEX1);
  const uint32_t tex0_dim = has_tex0 ? fvf_decode_texcoord_size(dev->fvf, 0) : 0u;
  bool hit = (handle != 0) && patch_sig_equal(entry->sig, sig) && !entry->vertices.empty() && !entry->indices_u16.empty();
  if (hit) {
    dev->patch_cache_hit_count++;
  } else {
    dev->patch_tessellate_count++;
    entry->sig = sig;
    hr = tessellate_rect_patch_cubic(control_bytes, ss.stride_bytes, has_tex0, tex0_dim, seg_u, seg_v, entry);
    if (FAILED(hr)) {
      if (handle != 0) {
        dev->patch_cache.erase(handle);
      }
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (vb_locked) {
    const HRESULT unlock_hr =
        wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, ss.vb->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(unlock_hr)) {
      logf("aerogpu-d3d9: DrawRectPatch: UnlockCb failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
           static_cast<unsigned long>(unlock_hr),
           static_cast<unsigned>(ss.vb->backing_alloc_id),
           static_cast<unsigned long long>(ss.vb->wddm_hAllocation));
      return trace.ret(unlock_hr);
    }
  }
#endif
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  if (entry->vertices.empty() || entry->indices_u16.empty()) {
    return trace.ret(E_FAIL);
  }

  // Upload + draw using the scratch UP buffers.
  const uint32_t index_count = static_cast<uint32_t>(entry->indices_u16.size());
  const uint64_t vertex_count_u64 = static_cast<uint64_t>(entry->vertices.size()) / ss.stride_bytes;
  if (vertex_count_u64 == 0 || vertex_count_u64 > 0x7FFFFFFFull) {
    return trace.ret(E_FAIL);
  }
  const uint32_t vertex_count = static_cast<uint32_t>(vertex_count_u64);

  DeviceStateStream saved_stream = dev->streams[0];
  Resource* saved_ib = dev->index_buffer;
  const D3DDDIFORMAT saved_fmt = dev->index_format;
  const uint32_t saved_offset = dev->index_offset_bytes;

  std::vector<uint8_t> converted;
  const void* vb_upload_data = entry->vertices.data();
  uint32_t vb_upload_size = static_cast<uint32_t>(entry->vertices.size());
  if (fixedfunc_fvf_supported(dev->fvf) && !dev->user_vs && !dev->user_ps) {
    hr = convert_xyzrhw_to_clipspace_locked(dev, vb_upload_data, ss.stride_bytes, vertex_count, &converted);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
    vb_upload_data = converted.data();
    vb_upload_size = static_cast<uint32_t>(converted.size());
  }

  hr = ensure_up_vertex_buffer_locked(dev, vb_upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }
  hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, vb_upload_data, vb_upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const uint32_t ib_size = index_count * 2;
  hr = ensure_up_index_buffer_locked(dev, ib_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }
  hr = emit_upload_buffer_locked(dev, dev->up_index_buffer, entry->indices_u16.data(), ib_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, ss.stride_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  dev->index_buffer = dev->up_index_buffer;
  dev->index_format = kD3dFmtIndex16;
  dev->index_offset_bytes = 0;

  auto* ib_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (!ib_cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    return trace.ret(E_OUTOFMEMORY);
  }
  ib_cmd->buffer = dev->up_index_buffer ? dev->up_index_buffer->handle : 0;
  ib_cmd->format = d3d9_index_format_to_aerogpu(kD3dFmtIndex16);
  ib_cmd->offset_bytes = 0;
  ib_cmd->reserved0 = 0;

  hr = ensure_fixedfunc_pipeline_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const uint32_t topology = d3d9_prim_to_topology(D3DDDIPT_TRIANGLELIST);
  if (!emit_set_topology_locked(dev, topology)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  // Ensure there is enough room for the draw packet and for restoring the app's
  // stream/index bindings after the draw. Patch draws temporarily bind scratch
  // UP buffers and must restore the D3D9-visible bindings before returning.
  const size_t restore_bytes = align_up(sizeof(aerogpu_cmd_set_vertex_buffers) + sizeof(aerogpu_vertex_buffer_binding), 4) +
                               align_up(sizeof(aerogpu_cmd_set_index_buffer), 4);
  const size_t draw_bytes = align_up(sizeof(aerogpu_cmd_draw_indexed), 4);
  if (!ensure_cmd_space(dev, draw_bytes + restore_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }
  hr = track_draw_state_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->index_count = index_count;
  cmd->instance_count = 1;
  cmd->first_index = 0;
  cmd->base_vertex = 0;
  cmd->first_instance = 0;

  // Restore stream source 0 and index buffer.
  (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
  dev->index_buffer = saved_ib;
  dev->index_format = saved_fmt;
  dev->index_offset_bytes = saved_offset;
  auto* restore_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (restore_cmd) {
    restore_cmd->buffer = saved_ib ? saved_ib->handle : 0;
    restore_cmd->format = d3d9_index_format_to_aerogpu(saved_fmt);
    restore_cmd->offset_bytes = saved_offset;
    restore_cmd->reserved0 = 0;
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_draw_tri_patch(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWTRIPATCH* pDrawTriPatch) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawTriPatch,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pDrawTriPatch ? static_cast<uint64_t>(pDrawTriPatch->Handle) : 0,
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pDrawTriPatch || !pDrawTriPatch->pNumSegs || !pDrawTriPatch->pTriPatchInfo) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  if (device_is_lost(dev)) {
    return trace.ret(kD3dErrDeviceLost);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return trace.ret(kD3dErrDeviceLost);
  }

  if (!fixedfunc_fvf_supported(dev->fvf) || dev->user_vs || dev->user_ps) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  DeviceStateStream& ss = dev->streams[0];
  const uint32_t min_stride = fixedfunc_min_stride_bytes(dev->fvf);
  if (!ss.vb || min_stride == 0 || ss.stride_bytes < min_stride) {
    return trace.ret(E_FAIL);
  }

  const D3DTRIPATCH_INFO* info = pDrawTriPatch->pTriPatchInfo;
  if (!info || info->Basis != D3DBASIS_BEZIER || info->Degree != D3DDEGREE_CUBIC || info->NumVertices != 10) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  const uint32_t seg_bits[3] = {
      f32_bits(pDrawTriPatch->pNumSegs[0]),
      f32_bits(pDrawTriPatch->pNumSegs[1]),
      f32_bits(pDrawTriPatch->pNumSegs[2]),
  };

  const float seg_f0 = pDrawTriPatch->pNumSegs[0];
  const float seg_f1 = pDrawTriPatch->pNumSegs[1];
  const float seg_f2 = pDrawTriPatch->pNumSegs[2];
  const uint32_t segs = clamp_patch_segs(std::max(seg_f0, std::max(seg_f1, seg_f2)));

  const uint32_t handle = static_cast<uint32_t>(pDrawTriPatch->Handle);
  const uint32_t start_vertex = static_cast<uint32_t>(info->StartVertexOffset);
  const uint32_t cp_count = static_cast<uint32_t>(info->NumVertices);
  const uint64_t src_offset_u64 = static_cast<uint64_t>(ss.offset_bytes) +
                                  static_cast<uint64_t>(start_vertex) * static_cast<uint64_t>(ss.stride_bytes);
  const uint64_t size_u64 = static_cast<uint64_t>(cp_count) * static_cast<uint64_t>(ss.stride_bytes);
  const uint64_t vb_size_u64 = ss.vb->size_bytes;
  if (src_offset_u64 > vb_size_u64 || size_u64 > vb_size_u64 - src_offset_u64) {
    return trace.ret(E_INVALIDARG);
  }

  const uint8_t* control_bytes = nullptr;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  void* vb_ptr = nullptr;
  bool vb_locked = false;
#endif

  bool use_vb_storage = ss.vb->storage.size() >= static_cast<size_t>(src_offset_u64 + size_u64);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (ss.vb->backing_alloc_id != 0) {
    use_vb_storage = false;
  }
#endif

  if (use_vb_storage) {
    control_bytes = ss.vb->storage.data() + static_cast<size_t>(src_offset_u64);
  } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (ss.vb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
      const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                   dev->wddm_device,
                                                   ss.vb->wddm_hAllocation,
                                                   src_offset_u64,
                                                   size_u64,
                                                   kD3DLOCK_READONLY,
                                                   &vb_ptr,
                                                   dev->wddm_context.hContext);
      if (FAILED(lock_hr) || !vb_ptr) {
        return trace.ret(FAILED(lock_hr) ? lock_hr : E_FAIL);
      }
      vb_locked = true;
      control_bytes = static_cast<const uint8_t*>(vb_ptr);
    } else
#endif
    {
      return trace.ret(E_INVALIDARG);
    }
  }

  const uint64_t control_hash = fnv1a64_hash(control_bytes, static_cast<size_t>(size_u64));

  PatchCacheSignature sig{};
  sig.kind = PatchKind::Tri;
  sig.fvf = dev->fvf;
  sig.stride_bytes = ss.stride_bytes;
  sig.start_vertex_offset = start_vertex;
  sig.num_vertices = cp_count;
  sig.basis = static_cast<uint32_t>(info->Basis);
  sig.degree = static_cast<uint32_t>(info->Degree);
  sig.seg_bits[0] = seg_bits[0];
  sig.seg_bits[1] = seg_bits[1];
  sig.seg_bits[2] = seg_bits[2];
  sig.seg_bits[3] = 0;
  sig.control_point_hash = control_hash;

  PatchCacheEntry temp{};
  PatchCacheEntry* entry = &temp;
  if (handle != 0) {
    auto it = dev->patch_cache.find(handle);
    if (it != dev->patch_cache.end()) {
      entry = &it->second;
    } else {
      try {
        auto [new_it, inserted] = dev->patch_cache.emplace(handle, PatchCacheEntry{});
        (void)inserted;
        entry = &new_it->second;
      } catch (...) {
        entry = &temp;
      }
    }
  }

  HRESULT hr = S_OK;
  const bool has_tex0 = (fvf_variant == FixedFuncVariant::RHW_COLOR_TEX1);
  const uint32_t tex0_dim = has_tex0 ? fvf_decode_texcoord_size(dev->fvf, 0) : 0u;
  const bool hit = (handle != 0) && patch_sig_equal(entry->sig, sig) && !entry->vertices.empty() && !entry->indices_u16.empty();
  if (hit) {
    dev->patch_cache_hit_count++;
  } else {
    dev->patch_tessellate_count++;
    entry->sig = sig;
    hr = tessellate_tri_patch_cubic(control_bytes, ss.stride_bytes, has_tex0, tex0_dim, segs, entry);
    if (FAILED(hr)) {
      if (handle != 0) {
        dev->patch_cache.erase(handle);
      }
    }
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (vb_locked) {
    const HRESULT unlock_hr =
        wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, ss.vb->wddm_hAllocation, dev->wddm_context.hContext);
    if (FAILED(unlock_hr)) {
      logf("aerogpu-d3d9: DrawTriPatch: UnlockCb failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
           static_cast<unsigned long>(unlock_hr),
           static_cast<unsigned>(ss.vb->backing_alloc_id),
           static_cast<unsigned long long>(ss.vb->wddm_hAllocation));
      return trace.ret(unlock_hr);
    }
  }
#endif
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  if (entry->vertices.empty() || entry->indices_u16.empty()) {
    return trace.ret(E_FAIL);
  }

  // Upload + draw using the scratch UP buffers.
  const uint32_t index_count = static_cast<uint32_t>(entry->indices_u16.size());
  const uint64_t vertex_count_u64 = static_cast<uint64_t>(entry->vertices.size()) / ss.stride_bytes;
  if (vertex_count_u64 == 0 || vertex_count_u64 > 0x7FFFFFFFull) {
    return trace.ret(E_FAIL);
  }
  const uint32_t vertex_count = static_cast<uint32_t>(vertex_count_u64);

  DeviceStateStream saved_stream = dev->streams[0];
  Resource* saved_ib = dev->index_buffer;
  const D3DDDIFORMAT saved_fmt = dev->index_format;
  const uint32_t saved_offset = dev->index_offset_bytes;

  std::vector<uint8_t> converted;
  const void* vb_upload_data = entry->vertices.data();
  uint32_t vb_upload_size = static_cast<uint32_t>(entry->vertices.size());
  if (fixedfunc_fvf_supported(dev->fvf) && !dev->user_vs && !dev->user_ps) {
    hr = convert_xyzrhw_to_clipspace_locked(dev, vb_upload_data, ss.stride_bytes, vertex_count, &converted);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
    vb_upload_data = converted.data();
    vb_upload_size = static_cast<uint32_t>(converted.size());
  }

  hr = ensure_up_vertex_buffer_locked(dev, vb_upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }
  hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, vb_upload_data, vb_upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const uint32_t ib_size = index_count * 2;
  hr = ensure_up_index_buffer_locked(dev, ib_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }
  hr = emit_upload_buffer_locked(dev, dev->up_index_buffer, entry->indices_u16.data(), ib_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, ss.stride_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  dev->index_buffer = dev->up_index_buffer;
  dev->index_format = kD3dFmtIndex16;
  dev->index_offset_bytes = 0;

  auto* ib_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (!ib_cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    return trace.ret(E_OUTOFMEMORY);
  }
  ib_cmd->buffer = dev->up_index_buffer ? dev->up_index_buffer->handle : 0;
  ib_cmd->format = d3d9_index_format_to_aerogpu(kD3dFmtIndex16);
  ib_cmd->offset_bytes = 0;
  ib_cmd->reserved0 = 0;

  hr = ensure_fixedfunc_pipeline_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const uint32_t topology = d3d9_prim_to_topology(D3DDDIPT_TRIANGLELIST);
  if (!emit_set_topology_locked(dev, topology)) {
    return trace.ret(E_OUTOFMEMORY);
  }

  // Ensure there is enough room for the draw packet and for restoring the app's
  // stream/index bindings after the draw. Patch draws temporarily bind scratch
  // UP buffers and must restore the D3D9-visible bindings before returning.
  const size_t restore_bytes = align_up(sizeof(aerogpu_cmd_set_vertex_buffers) + sizeof(aerogpu_vertex_buffer_binding), 4) +
                               align_up(sizeof(aerogpu_cmd_set_index_buffer), 4);
  const size_t draw_bytes = align_up(sizeof(aerogpu_cmd_draw_indexed), 4);
  if (!ensure_cmd_space(dev, draw_bytes + restore_bytes)) {
    return trace.ret(E_OUTOFMEMORY);
  }
  hr = track_draw_state_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
  if (!cmd) {
    return trace.ret(E_OUTOFMEMORY);
  }
  cmd->index_count = index_count;
  cmd->instance_count = 1;
  cmd->first_index = 0;
  cmd->base_vertex = 0;
  cmd->first_instance = 0;

  // Restore stream source 0 and index buffer.
  (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
  dev->index_buffer = saved_ib;
  dev->index_format = saved_fmt;
  dev->index_offset_bytes = saved_offset;
  auto* restore_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (restore_cmd) {
    restore_cmd->buffer = saved_ib ? saved_ib->handle : 0;
    restore_cmd->format = d3d9_index_format_to_aerogpu(saved_fmt);
    restore_cmd->offset_bytes = saved_offset;
    restore_cmd->reserved0 = 0;
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_delete_patch(D3DDDI_HDEVICE hDevice, UINT Handle) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDeletePatch,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(Handle),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->patch_cache.erase(static_cast<uint32_t>(Handle));
  return trace.ret(S_OK);
}

namespace {

struct ScopedResourceRead {
  const uint8_t* data = nullptr;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  Device* dev = nullptr;
  WddmAllocationHandle hAllocation = 0;
  uint32_t alloc_id = 0;
  const char* tag = nullptr;
  bool locked = false;
  void* locked_ptr = nullptr;
#endif

  HRESULT lock(Device* dev_in, Resource* res, uint64_t offset_bytes, uint64_t size_bytes, const char* tag_in) {
    data = nullptr;
    if (!dev_in || !res || size_bytes == 0) {
      return E_INVALIDARG;
    }
    if (offset_bytes > res->size_bytes || size_bytes > res->size_bytes - offset_bytes) {
      return E_INVALIDARG;
    }

    bool use_storage = res->storage.size() >= static_cast<size_t>(offset_bytes + size_bytes);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    // Guest-backed buffers can have a CPU shadow allocation when they are
    // shared/OpenResource'd; in WDDM builds the underlying allocation memory is
    // authoritative.
    if (res->backing_alloc_id != 0) {
      use_storage = false;
    }
#endif
    if (use_storage) {
      data = res->storage.data() + static_cast<size_t>(offset_bytes);
      return S_OK;
    }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
    if (res->wddm_hAllocation != 0 && dev_in->wddm_device != 0) {
      dev = dev_in;
      hAllocation = res->wddm_hAllocation;
      alloc_id = res->backing_alloc_id;
      tag = tag_in;
      const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                   dev->wddm_device,
                                                   res->wddm_hAllocation,
                                                   offset_bytes,
                                                   size_bytes,
                                                   kD3DLOCK_READONLY,
                                                   &locked_ptr,
                                                   dev->wddm_context.hContext);
      if (FAILED(lock_hr) || !locked_ptr) {
        return FAILED(lock_hr) ? lock_hr : E_FAIL;
      }
      locked = true;
      data = static_cast<const uint8_t*>(locked_ptr);
      return S_OK;
    }
#endif

    return E_INVALIDARG;
  }

  ~ScopedResourceRead() noexcept {
    try {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (locked && dev && dev->wddm_device != 0 && hAllocation != 0) {
        const HRESULT hr =
            wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, hAllocation, dev->wddm_context.hContext);
        if (FAILED(hr)) {
          logf("aerogpu-d3d9: ScopedResourceRead UnlockCb(%s) failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
               tag ? tag : "?",
               static_cast<unsigned long>(hr),
               static_cast<unsigned>(alloc_id),
               static_cast<unsigned long long>(hAllocation));
        }
      }
#endif
    } catch (...) {
    }
  }
};

bool vertex_decl_used_streams(const VertexDecl* decl, std::bitset<16>* out) {
  if (!out) {
    return false;
  }
  out->reset();
  if (!decl || decl->blob.empty()) {
    return false;
  }
  if ((decl->blob.size() % sizeof(D3DVERTEXELEMENT9_COMPAT)) != 0) {
    return false;
  }
  const auto* elems = reinterpret_cast<const D3DVERTEXELEMENT9_COMPAT*>(decl->blob.data());
  const size_t count = decl->blob.size() / sizeof(D3DVERTEXELEMENT9_COMPAT);
  for (size_t i = 0; i < count; ++i) {
    const auto& e = elems[i];
    if (e.Stream == 0xFF && e.Type == kD3dDeclTypeUnused) {
      break;
    }
    if (e.Stream < 16) {
      out->set(e.Stream);
    }
  }
  return true;
}

struct InstancingConfig {
  uint32_t instance_count = 1;
  // For each stream, 0 means "per-vertex" (default / INDEXEDDATA / freq==1).
  // Non-zero means "per-instance" and encodes the D3D9 INSTANCEDATA divisor:
  // how many instances to draw for each element of the instanced stream.
  std::array<uint32_t, 16> instanced_divisor{};
};

HRESULT parse_instancing_config_locked(Device* dev, const std::bitset<16>& used_streams, InstancingConfig* out) {
  if (!dev || !out) {
    return E_INVALIDARG;
  }
  *out = {};
  out->instance_count = 1;
  out->instanced_divisor.fill(0u);

  // Instancing in D3D9 is driven by stream 0's INDEXEDDATA | N encoding.
  if (!used_streams.test(0)) {
    return kD3DErrInvalidCall;
  }

  const uint32_t freq0 = dev->stream_source_freq[0];
  const uint32_t mode0 = freq0 & kD3DStreamSourceMask;
  if (mode0 != kD3DStreamSourceIndexedData) {
    return kD3DErrInvalidCall;
  }
  const uint32_t instance_count = freq0 & ~kD3DStreamSourceMask;
  if (instance_count == 0) {
    return kD3DErrInvalidCall;
  }
  out->instance_count = instance_count;

  // Validate and classify the streams that participate in the current vertex decl.
  for (uint32_t s = 0; s < 16; ++s) {
    if (!used_streams.test(s)) {
      continue;
    }

    const uint32_t freq = dev->stream_source_freq[s];
    if (freq == 1u) {
      continue;
    }
    const uint32_t mode = freq & kD3DStreamSourceMask;
    const uint32_t val = freq & ~kD3DStreamSourceMask;

    if (mode == kD3DStreamSourceIndexedData) {
      if (val != instance_count) {
        return kD3DErrInvalidCall;
      }
      continue;
    }
    if (mode == kD3DStreamSourceInstanceData) {
      if (val == 0u) {
        return kD3DErrInvalidCall;
      }
      out->instanced_divisor[s] = val;
      continue;
    }
    return kD3DErrInvalidCall;
  }

  return S_OK;
}

bool any_stream_source_freq_non_default_locked(const Device* dev) {
  if (!dev) {
    return false;
  }
  for (uint32_t i = 0; i < 16; ++i) {
    if (dev->stream_source_freq[i] != 1u) {
      return true;
    }
  }
  return false;
}

HRESULT try_draw_instanced_primitive_locked(
    Device* dev,
    D3DDDIPRIMITIVETYPE type,
    uint32_t start_vertex,
    uint32_t primitive_count) {
  if (!dev) {
    return E_INVALIDARG;
  }

  if (!any_stream_source_freq_non_default_locked(dev)) {
    return S_FALSE;
  }

  // D3D9 instancing can be used with strip/fan topologies. When CPU-expanding
  // instancing into a single draw, we must avoid connecting primitives across
  // instance boundaries. For list topologies, concatenating instances is safe;
  // for strip/fan, we issue one draw per instance below.
  const bool single_draw_safe =
      (type == D3DDDIPT_TRIANGLELIST || type == D3DDDIPT_LINELIST || type == D3DDDIPT_POINTLIST);

  std::bitset<16> used_streams{};
  if (!vertex_decl_used_streams(dev->vertex_decl, &used_streams)) {
    return kD3DErrInvalidCall;
  }

  bool any_used_non_default = false;
  for (uint32_t s = 0; s < 16; ++s) {
    if (used_streams.test(s) && dev->stream_source_freq[s] != 1u) {
      any_used_non_default = true;
      break;
    }
  }
  if (!any_used_non_default) {
    return S_FALSE;
  }

  InstancingConfig cfg{};
  HRESULT hr = parse_instancing_config_locked(dev, used_streams, &cfg);
  if (FAILED(hr)) {
    return hr;
  }

  bool has_instanced_stream = false;
  for (uint32_t s = 0; s < 16; ++s) {
    if (used_streams.test(s) && cfg.instanced_divisor[s] != 0u) {
      has_instanced_stream = true;
      break;
    }
  }
  if (cfg.instance_count == 1 && !has_instanced_stream) {
    return S_FALSE;
  }

  // Instancing requires a vertex shader to consume per-instance attributes.
  if (!dev->user_vs) {
    return kD3DErrInvalidCall;
  }

  const uint32_t vertices_per_instance = vertex_count_from_primitive(type, primitive_count);
  if (vertices_per_instance == 0) {
    return S_OK;
  }

  const uint64_t expanded_vertex_count_u64 =
      static_cast<uint64_t>(vertices_per_instance) * static_cast<uint64_t>(cfg.instance_count);
  if (expanded_vertex_count_u64 == 0 || expanded_vertex_count_u64 > 0x7FFFFFFFu) {
    return E_INVALIDARG;
  }
  const uint32_t expanded_vertex_count = static_cast<uint32_t>(expanded_vertex_count_u64);

  if (single_draw_safe) {
    // CPU-expand all participating streams *before* emitting any vertex-buffer
    // bindings, so invalid-call failures do not mutate device state.
    std::array<std::vector<uint8_t>, 16> expanded_streams{};
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }
      const DeviceStateStream& ss = dev->streams[s];
      if (!ss.vb || ss.stride_bytes == 0) {
        return kD3DErrInvalidCall;
      }

      const uint64_t stride = ss.stride_bytes;
      const uint64_t expanded_size_u64 =
          static_cast<uint64_t>(vertices_per_instance) * static_cast<uint64_t>(cfg.instance_count) * stride;
      if (expanded_size_u64 == 0 || expanded_size_u64 > 0x7FFFFFFFu) {
        return E_INVALIDARG;
      }

      const uint32_t expanded_size = static_cast<uint32_t>(expanded_size_u64);
      try {
        expanded_streams[s].resize(expanded_size);
      } catch (...) {
        return E_OUTOFMEMORY;
      }

      uint8_t* dst_all = expanded_streams[s].data();

      if (cfg.instanced_divisor[s] != 0u) {
        const uint32_t divisor = cfg.instanced_divisor[s];
        const uint64_t element_count_u64 =
            (static_cast<uint64_t>(cfg.instance_count) + static_cast<uint64_t>(divisor) - 1u) /
            static_cast<uint64_t>(divisor);
        const uint64_t src_offset = ss.offset_bytes;
        const uint64_t src_size = element_count_u64 * stride;
        ScopedResourceRead src;
        hr = src.lock(dev, ss.vb, src_offset, src_size, "instancing VB(inst)");
        if (FAILED(hr) || !src.data) {
          return FAILED(hr) ? hr : E_INVALIDARG;
        }

        for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
          const uint32_t elem = inst / divisor;
          const uint8_t* inst_data = src.data + static_cast<size_t>(elem) * static_cast<size_t>(stride);
          uint8_t* dst_base =
              dst_all + (static_cast<size_t>(inst) * vertices_per_instance) * static_cast<size_t>(stride);
          for (uint32_t v = 0; v < vertices_per_instance; ++v) {
            std::memcpy(dst_base + static_cast<size_t>(v) * static_cast<size_t>(stride),
                        inst_data,
                        static_cast<size_t>(stride));
          }
        }
      } else {
        const uint64_t src_offset =
            static_cast<uint64_t>(ss.offset_bytes) + static_cast<uint64_t>(start_vertex) * stride;
        const uint64_t src_size = static_cast<uint64_t>(vertices_per_instance) * stride;
        ScopedResourceRead src;
        hr = src.lock(dev, ss.vb, src_offset, src_size, "instancing VB(vtx)");
        if (FAILED(hr) || !src.data) {
          return FAILED(hr) ? hr : E_INVALIDARG;
        }

        for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
          uint8_t* dst = dst_all + static_cast<size_t>(inst) * static_cast<size_t>(src_size);
          std::memcpy(dst, src.data, static_cast<size_t>(src_size));
        }
      }
    }

    hr = ensure_draw_pipeline_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }

    // Save stream bindings (for state restoration after draw + in error paths).
    std::array<DeviceStateStream, 16> saved_streams{};
    for (uint32_t s = 0; s < 16; ++s) {
      saved_streams[s] = dev->streams[s];
    }

    auto restore_streams = [&]() {
      bool ok = true;
      for (uint32_t s = 0; s < 16; ++s) {
        if (!used_streams.test(s)) {
          continue;
        }
        const DeviceStateStream& saved = saved_streams[s];
        ok &= emit_set_stream_source_locked(dev, s, saved.vb, saved.offset_bytes, saved.stride_bytes);
      }
      return ok;
    };

    // Upload + bind expanded buffers.
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }

      const DeviceStateStream& saved = saved_streams[s];
      const uint32_t expanded_size = static_cast<uint32_t>(expanded_streams[s].size());

      hr = ensure_instancing_vertex_buffer_locked(dev, s, expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      hr = emit_upload_buffer_locked(dev, dst_vb, expanded_streams[s].data(), expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      if (!emit_set_stream_source_locked(dev, s, dst_vb, 0, saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    }

    const uint32_t topology = d3d9_prim_to_topology(type);
    const size_t draw_pkt_bytes = align_up(sizeof(aerogpu_cmd_draw), 4);
    const size_t min_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) + draw_pkt_bytes;
    if (!ensure_cmd_space(dev, min_bytes)) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    if (!emit_set_topology_locked(dev, topology)) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    if (!ensure_cmd_space(dev, draw_pkt_bytes)) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)restore_streams();
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
    if (!cmd) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->vertex_count = expanded_vertex_count;
    cmd->instance_count = 1;
    cmd->first_vertex = 0;
    cmd->first_instance = 0;

    if (!restore_streams()) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    return S_OK;
  }

  // Strip/fan topologies must be issued as one draw per instance to avoid
  // connecting primitives across instance boundaries. When drawing each instance
  // separately, avoid CPU-expanding per-vertex streams across instances. Instead,
  // adjust per-vertex stream offsets so each draw uses `first_vertex=0` and only
  // expand/upload instanced streams for the current instance.
  std::array<ScopedResourceRead, 16> instanced_src{};
  std::array<std::vector<uint8_t>, 16> instanced_expanded{};
  for (uint32_t s = 0; s < 16; ++s) {
    if (!used_streams.test(s)) {
      continue;
    }
    const DeviceStateStream& ss = dev->streams[s];
    if (!ss.vb || ss.stride_bytes == 0) {
      return kD3DErrInvalidCall;
    }
    const uint64_t stride = ss.stride_bytes;

    if (cfg.instanced_divisor[s] != 0u) {
      const uint64_t expanded_size_u64 = static_cast<uint64_t>(vertices_per_instance) * stride;
      if (expanded_size_u64 == 0 || expanded_size_u64 > 0x7FFFFFFFu) {
        return E_INVALIDARG;
      }
      try {
        instanced_expanded[s].resize(static_cast<size_t>(expanded_size_u64));
      } catch (...) {
        return E_OUTOFMEMORY;
      }

      const uint32_t divisor = cfg.instanced_divisor[s];
      const uint64_t element_count_u64 =
          (static_cast<uint64_t>(cfg.instance_count) + static_cast<uint64_t>(divisor) - 1u) /
          static_cast<uint64_t>(divisor);
      const uint64_t src_offset = ss.offset_bytes;
      const uint64_t src_size = element_count_u64 * stride;
      hr = instanced_src[s].lock(dev, ss.vb, src_offset, src_size, "instancing VB(inst)");
      if (FAILED(hr) || !instanced_src[s].data) {
        return FAILED(hr) ? hr : E_INVALIDARG;
      }
    } else {
      // Validate the per-vertex range. We'll rebind the stream offset to
      // `offset + start_vertex * stride` and draw with first_vertex=0.
      const uint64_t vertex_base_u64 = static_cast<uint64_t>(ss.offset_bytes) +
                                       static_cast<uint64_t>(start_vertex) * stride;
      const uint64_t src_size = static_cast<uint64_t>(vertices_per_instance) * stride;
      if (vertex_base_u64 > ss.vb->size_bytes || src_size > ss.vb->size_bytes - vertex_base_u64) {
        return E_INVALIDARG;
      }
    }
  }

  hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return hr;
  }

  // Save stream bindings (for state restoration after draw + in error paths).
  std::array<DeviceStateStream, 16> saved_streams{};
  for (uint32_t s = 0; s < 16; ++s) {
    saved_streams[s] = dev->streams[s];
  }

  auto restore_streams = [&]() {
    bool ok = true;
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }
      const DeviceStateStream& saved = saved_streams[s];
      ok &= emit_set_stream_source_locked(dev, s, saved.vb, saved.offset_bytes, saved.stride_bytes);
    }
    return ok;
  };

  // Bind scratch buffers for instanced streams and adjust per-vertex stream
  // offsets so we can draw each instance with `first_vertex=0`.
  for (uint32_t s = 0; s < 16; ++s) {
    if (!used_streams.test(s)) {
      continue;
    }
    const DeviceStateStream& saved = saved_streams[s];
    const uint64_t stride = saved.stride_bytes;

    if (cfg.instanced_divisor[s] != 0u) {
      const uint32_t expanded_size = static_cast<uint32_t>(instanced_expanded[s].size());
      hr = ensure_instancing_vertex_buffer_locked(dev, s, expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      if (!emit_set_stream_source_locked(dev, s, dst_vb, 0, saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    } else {
      const uint64_t delta_bytes = static_cast<uint64_t>(start_vertex) * stride;
      const uint64_t new_offset_u64 = static_cast<uint64_t>(saved.offset_bytes) + delta_bytes;
      if (new_offset_u64 > std::numeric_limits<uint32_t>::max()) {
        (void)restore_streams();
        return E_INVALIDARG;
      }
      if (!emit_set_stream_source_locked(dev, s, saved.vb, static_cast<uint32_t>(new_offset_u64), saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    }
  }

  const uint32_t topology = d3d9_prim_to_topology(type);
  const size_t draw_pkt_bytes = align_up(sizeof(aerogpu_cmd_draw), 4);
  const size_t min_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) + draw_pkt_bytes;
  if (!ensure_cmd_space(dev, min_bytes)) {
    (void)restore_streams();
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  if (!emit_set_topology_locked(dev, topology)) {
    (void)restore_streams();
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  std::array<uint32_t, 16> last_elem{};
  last_elem.fill(std::numeric_limits<uint32_t>::max());

  for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s) || cfg.instanced_divisor[s] == 0u) {
        continue;
      }
      const DeviceStateStream& saved = saved_streams[s];
      const uint64_t stride = saved.stride_bytes;
      const uint32_t divisor = cfg.instanced_divisor[s];
      const uint32_t elem = inst / divisor;
      if (elem == last_elem[s]) {
        continue;
      }
      last_elem[s] = elem;

      const uint8_t* inst_data =
          instanced_src[s].data + static_cast<size_t>(elem) * static_cast<size_t>(stride);
      uint8_t* dst_all = instanced_expanded[s].data();
      for (uint32_t v = 0; v < vertices_per_instance; ++v) {
        std::memcpy(dst_all + static_cast<size_t>(v) * static_cast<size_t>(stride),
                    inst_data,
                    static_cast<size_t>(stride));
      }

      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      const uint32_t expanded_size = static_cast<uint32_t>(instanced_expanded[s].size());
      hr = emit_upload_buffer_locked(dev, dst_vb, instanced_expanded[s].data(), expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
    }

    if (!ensure_cmd_space(dev, draw_pkt_bytes)) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)restore_streams();
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
    if (!cmd) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->vertex_count = vertices_per_instance;
    cmd->instance_count = 1;
    cmd->first_vertex = 0;
    cmd->first_instance = 0;
  }

  if (!restore_streams()) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  return S_OK;
}

HRESULT try_draw_instanced_indexed_primitive_locked(
    Device* dev,
    D3DDDIPRIMITIVETYPE type,
    int32_t base_vertex,
    uint32_t min_index,
    uint32_t num_vertices,
    uint32_t start_index,
    uint32_t primitive_count) {
  if (!dev) {
    return E_INVALIDARG;
  }

  if (!any_stream_source_freq_non_default_locked(dev)) {
    return S_FALSE;
  }

  // See try_draw_instanced_primitive_locked() for discussion: strip/fan
  // topologies must be issued as one draw per instance to avoid connecting
  // primitives across instance boundaries.
  const bool single_draw_safe =
      (type == D3DDDIPT_TRIANGLELIST || type == D3DDDIPT_LINELIST || type == D3DDDIPT_POINTLIST);

  std::bitset<16> used_streams{};
  if (!vertex_decl_used_streams(dev->vertex_decl, &used_streams)) {
    return kD3DErrInvalidCall;
  }

  bool any_used_non_default = false;
  for (uint32_t s = 0; s < 16; ++s) {
    if (used_streams.test(s) && dev->stream_source_freq[s] != 1u) {
      any_used_non_default = true;
      break;
    }
  }
  if (!any_used_non_default) {
    return S_FALSE;
  }

  InstancingConfig cfg{};
  HRESULT hr = parse_instancing_config_locked(dev, used_streams, &cfg);
  if (FAILED(hr)) {
    return hr;
  }

  bool has_instanced_stream = false;
  for (uint32_t s = 0; s < 16; ++s) {
    if (used_streams.test(s) && cfg.instanced_divisor[s] != 0u) {
      has_instanced_stream = true;
      break;
    }
  }
  if (cfg.instance_count == 1 && !has_instanced_stream) {
    return S_FALSE;
  }

  if (!dev->user_vs) {
    return kD3DErrInvalidCall;
  }

  if (!dev->index_buffer || (dev->index_format != kD3dFmtIndex16 && dev->index_format != kD3dFmtIndex32)) {
    return kD3DErrInvalidCall;
  }
  if (num_vertices == 0) {
    return kD3DErrInvalidCall;
  }

  const uint32_t index_count = index_count_from_primitive(type, primitive_count);
  const uint32_t index_size = (dev->index_format == kD3dFmtIndex32) ? 4u : 2u;
  const uint64_t src_index_offset =
      static_cast<uint64_t>(dev->index_offset_bytes) + static_cast<uint64_t>(start_index) * index_size;
  const uint64_t src_index_size = static_cast<uint64_t>(index_count) * index_size;

  // Read indices on the CPU so we can derive the referenced vertex range and,
  // for TRIANGLELIST/LINELIST/POINTLIST, expand them for a single concatenated draw.
  ScopedResourceRead index_src;
  hr = index_src.lock(dev, dev->index_buffer, src_index_offset, src_index_size, "instancing IB");
  if (FAILED(hr) || !index_src.data) {
    return FAILED(hr) ? hr : E_INVALIDARG;
  }

  // `min_index` and `num_vertices` are advisory at the D3D9 API boundary. Some apps
  // pass conservative/incorrect values. To keep the instancing emulation robust
  // (and to size the per-instance scratch streams correctly), derive the actual
  // [min,max] index range from the index buffer contents.
  uint32_t min_index_actual = std::numeric_limits<uint32_t>::max();
  uint32_t max_index_actual = 0;
  for (uint32_t i = 0; i < index_count; ++i) {
    uint32_t idx = 0;
    if (index_size == 4) {
      std::memcpy(&idx, index_src.data + static_cast<size_t>(i) * 4u, sizeof(idx));
    } else {
      uint16_t idx16 = 0;
      std::memcpy(&idx16, index_src.data + static_cast<size_t>(i) * 2u, sizeof(idx16));
      idx = idx16;
    }
    min_index_actual = std::min(min_index_actual, idx);
    max_index_actual = std::max(max_index_actual, idx);
  }
  if (min_index_actual == std::numeric_limits<uint32_t>::max()) {
    // No indices.
    return S_OK;
  }

  const uint64_t num_vertices_u64 = static_cast<uint64_t>(max_index_actual) -
                                    static_cast<uint64_t>(min_index_actual) + 1u;
  if (num_vertices_u64 == 0 || num_vertices_u64 > std::numeric_limits<uint32_t>::max()) {
    return kD3DErrInvalidCall;
  }
  min_index = min_index_actual;
  num_vertices = static_cast<uint32_t>(num_vertices_u64);

  const uint64_t max_index_excl_u64 = static_cast<uint64_t>(min_index) + static_cast<uint64_t>(num_vertices);
  if (max_index_excl_u64 == 0 || max_index_excl_u64 > 0x1'0000'0000ull) {
    // Overflow or empty range.
    return kD3DErrInvalidCall;
  }

  const bool use_expanded_indices = single_draw_safe;
  uint32_t expanded_index_count = 0;
  uint32_t expanded_index_bytes = 0;
  std::vector<uint32_t> expanded_indices;
  if (use_expanded_indices) {
    const uint64_t expanded_index_count_u64 =
        static_cast<uint64_t>(index_count) * static_cast<uint64_t>(cfg.instance_count);
    const uint64_t expanded_index_bytes_u64 = expanded_index_count_u64 * 4u;
    if (expanded_index_count_u64 == 0 || expanded_index_bytes_u64 == 0 || expanded_index_bytes_u64 > 0x7FFFFFFFu) {
      return E_INVALIDARG;
    }
    expanded_index_count = static_cast<uint32_t>(expanded_index_count_u64);
    expanded_index_bytes = static_cast<uint32_t>(expanded_index_bytes_u64);

    try {
      expanded_indices.resize(static_cast<size_t>(expanded_index_count));
    } catch (...) {
      return E_OUTOFMEMORY;
    }

    for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
      const uint32_t base = inst * num_vertices;
      for (uint32_t i = 0; i < index_count; ++i) {
        uint32_t idx = 0;
        if (index_size == 4) {
          std::memcpy(&idx, index_src.data + static_cast<size_t>(i) * 4u, sizeof(idx));
        } else {
          uint16_t idx16 = 0;
          std::memcpy(&idx16, index_src.data + static_cast<size_t>(i) * 2u, sizeof(idx16));
          idx = idx16;
        }
        const uint64_t idx_u64 = idx;
        if (idx_u64 < static_cast<uint64_t>(min_index) || idx_u64 >= max_index_excl_u64) {
          return kD3DErrInvalidCall;
        }
        expanded_indices[static_cast<size_t>(inst) * index_count + i] = (idx - min_index) + base;
      }
    }
  } else {
    // We will issue one draw per instance (strip/fan). Reuse the app's index
    // buffer and adjust base_vertex instead of expanding indices.
    for (uint32_t i = 0; i < index_count; ++i) {
      uint32_t idx = 0;
      if (index_size == 4) {
        std::memcpy(&idx, index_src.data + static_cast<size_t>(i) * 4u, sizeof(idx));
      } else {
        uint16_t idx16 = 0;
        std::memcpy(&idx16, index_src.data + static_cast<size_t>(i) * 2u, sizeof(idx16));
        idx = idx16;
      }
      const uint64_t idx_u64 = idx;
      if (idx_u64 < static_cast<uint64_t>(min_index) || idx_u64 >= max_index_excl_u64) {
        return kD3DErrInvalidCall;
      }
    }
  }

  const int64_t vertex_start_signed = static_cast<int64_t>(base_vertex) + static_cast<int64_t>(min_index);
  auto compute_vertex_base_offset = [&](uint32_t offset_bytes, uint64_t stride, uint64_t* out) -> bool {
    if (!out || stride == 0) {
      return false;
    }
    const uint64_t base = static_cast<uint64_t>(offset_bytes);
    if (vertex_start_signed >= 0) {
      const uint64_t v = static_cast<uint64_t>(vertex_start_signed);
      if (v != 0 && stride > (std::numeric_limits<uint64_t>::max() / v)) {
        return false;
      }
      const uint64_t delta = v * stride;
      if (delta > (std::numeric_limits<uint64_t>::max() - base)) {
        return false;
      }
      *out = base + delta;
      return true;
    }
    if (vertex_start_signed == std::numeric_limits<int64_t>::min()) {
      return false;
    }
    const uint64_t v = static_cast<uint64_t>(-vertex_start_signed);
    if (v != 0 && stride > (std::numeric_limits<uint64_t>::max() / v)) {
      return false;
    }
    const uint64_t delta = v * stride;
    if (delta > base) {
      return false;
    }
    *out = base - delta;
    return true;
  };

  if (single_draw_safe) {
    // CPU-expand all participating streams *before* mutating stream bindings.
    std::array<std::vector<uint8_t>, 16> expanded_streams{};
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }
      const DeviceStateStream& ss = dev->streams[s];
      if (!ss.vb || ss.stride_bytes == 0) {
        return kD3DErrInvalidCall;
      }

      const uint64_t stride = ss.stride_bytes;
      const uint64_t expanded_size_u64 =
          static_cast<uint64_t>(num_vertices) * static_cast<uint64_t>(cfg.instance_count) * stride;
      if (expanded_size_u64 == 0 || expanded_size_u64 > 0x7FFFFFFFu) {
        return E_INVALIDARG;
      }
      const uint32_t expanded_size = static_cast<uint32_t>(expanded_size_u64);
      try {
        expanded_streams[s].resize(expanded_size);
      } catch (...) {
        return E_OUTOFMEMORY;
      }

      uint8_t* dst_all = expanded_streams[s].data();

      if (cfg.instanced_divisor[s] != 0u) {
        const uint32_t divisor = cfg.instanced_divisor[s];
        const uint64_t element_count_u64 =
            (static_cast<uint64_t>(cfg.instance_count) + static_cast<uint64_t>(divisor) - 1u) /
            static_cast<uint64_t>(divisor);
        const uint64_t src_offset = ss.offset_bytes;
        const uint64_t src_size = element_count_u64 * stride;
        ScopedResourceRead src;
        hr = src.lock(dev, ss.vb, src_offset, src_size, "instancing VB(inst)");
        if (FAILED(hr) || !src.data) {
          return FAILED(hr) ? hr : E_INVALIDARG;
        }

        for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
          const uint32_t elem = inst / divisor;
          const uint8_t* inst_data = src.data + static_cast<size_t>(elem) * static_cast<size_t>(stride);
          uint8_t* dst_base = dst_all + (static_cast<size_t>(inst) * num_vertices) * static_cast<size_t>(stride);
          for (uint32_t v = 0; v < num_vertices; ++v) {
            std::memcpy(dst_base + static_cast<size_t>(v) * static_cast<size_t>(stride),
                        inst_data,
                        static_cast<size_t>(stride));
          }
        }
      } else {
        uint64_t src_offset = 0;
        if (!compute_vertex_base_offset(ss.offset_bytes, stride, &src_offset)) {
          return E_INVALIDARG;
        }
        const uint64_t src_size = static_cast<uint64_t>(num_vertices) * stride;
        ScopedResourceRead src;
        hr = src.lock(dev, ss.vb, src_offset, src_size, "instancing VB(vtx)");
        if (FAILED(hr) || !src.data) {
          return FAILED(hr) ? hr : E_INVALIDARG;
        }

        for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
          uint8_t* dst = dst_all + static_cast<size_t>(inst) * static_cast<size_t>(src_size);
          std::memcpy(dst, src.data, static_cast<size_t>(src_size));
        }
      }
    }

    hr = ensure_draw_pipeline_locked(dev);
    if (FAILED(hr)) {
      return hr;
    }

    // Save state for restoration.
    std::array<DeviceStateStream, 16> saved_streams{};
    for (uint32_t s = 0; s < 16; ++s) {
      saved_streams[s] = dev->streams[s];
    }
    Resource* saved_ib = dev->index_buffer;
    const D3DDDIFORMAT saved_fmt = dev->index_format;
    const uint32_t saved_offset = dev->index_offset_bytes;

    auto restore_streams = [&]() {
      bool ok = true;
      for (uint32_t s = 0; s < 16; ++s) {
        if (!used_streams.test(s)) {
          continue;
        }
        const DeviceStateStream& saved = saved_streams[s];
        ok &= emit_set_stream_source_locked(dev, s, saved.vb, saved.offset_bytes, saved.stride_bytes);
      }
      return ok;
    };

    auto restore_index = [&]() {
      dev->index_buffer = saved_ib;
      dev->index_format = saved_fmt;
      dev->index_offset_bytes = saved_offset;
      auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
      if (!cmd) {
        return false;
      }
      cmd->buffer = saved_ib ? saved_ib->handle : 0;
      cmd->format = d3d9_index_format_to_aerogpu(saved_fmt);
      cmd->offset_bytes = saved_offset;
      cmd->reserved0 = 0;
      return true;
    };

    // Upload + bind expanded vertex buffers.
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }
      const DeviceStateStream& saved = saved_streams[s];
      const uint32_t expanded_size = static_cast<uint32_t>(expanded_streams[s].size());

      hr = ensure_instancing_vertex_buffer_locked(dev, s, expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      hr = emit_upload_buffer_locked(dev, dst_vb, expanded_streams[s].data(), expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      if (!emit_set_stream_source_locked(dev, s, dst_vb, 0, saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    }

    // Upload expanded index buffer and bind it.
    hr = ensure_up_index_buffer_locked(dev, expanded_index_bytes);
    if (FAILED(hr)) {
      (void)restore_streams();
      return hr;
    }
    hr = emit_upload_buffer_locked(dev, dev->up_index_buffer, expanded_indices.data(), expanded_index_bytes);
    if (FAILED(hr)) {
      (void)restore_streams();
      return hr;
    }

    dev->index_buffer = dev->up_index_buffer;
    dev->index_format = kD3dFmtIndex32;
    dev->index_offset_bytes = 0;
    auto* ib_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (!ib_cmd) {
      (void)restore_streams();
      dev->index_buffer = saved_ib;
      dev->index_format = saved_fmt;
      dev->index_offset_bytes = saved_offset;
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    ib_cmd->buffer = dev->up_index_buffer ? dev->up_index_buffer->handle : 0;
    ib_cmd->format = d3d9_index_format_to_aerogpu(kD3dFmtIndex32);
    ib_cmd->offset_bytes = 0;
    ib_cmd->reserved0 = 0;

    const uint32_t topology = d3d9_prim_to_topology(type);
    const size_t draw_pkt_bytes = align_up(sizeof(aerogpu_cmd_draw_indexed), 4);
    const size_t min_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) + draw_pkt_bytes;
    if (!ensure_cmd_space(dev, min_bytes)) {
      (void)restore_streams();
      (void)restore_index();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    if (!emit_set_topology_locked(dev, topology)) {
      (void)restore_streams();
      (void)restore_index();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    if (!ensure_cmd_space(dev, draw_pkt_bytes)) {
      (void)restore_streams();
      (void)restore_index();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)restore_streams();
      (void)restore_index();
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
    if (!cmd) {
      (void)restore_streams();
      (void)restore_index();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->index_count = expanded_index_count;
    cmd->instance_count = 1;
    cmd->first_index = 0;
    cmd->base_vertex = 0;
    cmd->first_instance = 0;

    if (!restore_streams()) {
      (void)restore_index();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    if (!restore_index()) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    return S_OK;
  }

  // Strip/fan topologies: issue one draw per instance. Avoid expanding per-vertex
  // streams across instances by rebinding them with an adjusted stream offset
  // and uploading instanced stream data per draw.
  const int64_t base_vertex_i64 = -static_cast<int64_t>(min_index);
  if (base_vertex_i64 < std::numeric_limits<int32_t>::min() || base_vertex_i64 > std::numeric_limits<int32_t>::max()) {
    return E_INVALIDARG;
  }
  const int32_t draw_base_vertex = static_cast<int32_t>(base_vertex_i64);

  std::array<ScopedResourceRead, 16> instanced_src{};
  std::array<std::vector<uint8_t>, 16> instanced_expanded{};
  for (uint32_t s = 0; s < 16; ++s) {
    if (!used_streams.test(s)) {
      continue;
    }
    const DeviceStateStream& ss = dev->streams[s];
    if (!ss.vb || ss.stride_bytes == 0) {
      return kD3DErrInvalidCall;
    }
    const uint64_t stride = ss.stride_bytes;

    if (cfg.instanced_divisor[s] != 0u) {
      const uint64_t expanded_size_u64 = static_cast<uint64_t>(num_vertices) * stride;
      if (expanded_size_u64 == 0 || expanded_size_u64 > 0x7FFFFFFFu) {
        return E_INVALIDARG;
      }
      try {
        instanced_expanded[s].resize(static_cast<size_t>(expanded_size_u64));
      } catch (...) {
        return E_OUTOFMEMORY;
      }

      const uint32_t divisor = cfg.instanced_divisor[s];
      const uint64_t element_count_u64 =
          (static_cast<uint64_t>(cfg.instance_count) + static_cast<uint64_t>(divisor) - 1u) /
          static_cast<uint64_t>(divisor);
      const uint64_t src_offset = ss.offset_bytes;
      const uint64_t src_size = element_count_u64 * stride;
      hr = instanced_src[s].lock(dev, ss.vb, src_offset, src_size, "instancing VB(inst)");
      if (FAILED(hr) || !instanced_src[s].data) {
        return FAILED(hr) ? hr : E_INVALIDARG;
      }
    } else {
      uint64_t vertex_base_u64 = 0;
      if (!compute_vertex_base_offset(ss.offset_bytes, stride, &vertex_base_u64)) {
        return E_INVALIDARG;
      }
      const uint64_t src_size = static_cast<uint64_t>(num_vertices) * stride;
      if (vertex_base_u64 > ss.vb->size_bytes || src_size > ss.vb->size_bytes - vertex_base_u64) {
        return E_INVALIDARG;
      }
    }
  }

  hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return hr;
  }

  // Save stream bindings (for state restoration after draw + in error paths).
  std::array<DeviceStateStream, 16> saved_streams{};
  for (uint32_t s = 0; s < 16; ++s) {
    saved_streams[s] = dev->streams[s];
  }

  auto restore_streams = [&]() {
    bool ok = true;
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s)) {
        continue;
      }
      const DeviceStateStream& saved = saved_streams[s];
      ok &= emit_set_stream_source_locked(dev, s, saved.vb, saved.offset_bytes, saved.stride_bytes);
    }
    return ok;
  };

  // Bind scratch buffers for instanced streams and adjust per-vertex stream
  // offsets so each draw uses indices starting at `min_index`.
  for (uint32_t s = 0; s < 16; ++s) {
    if (!used_streams.test(s)) {
      continue;
    }
    const DeviceStateStream& saved = saved_streams[s];
    const uint64_t stride = saved.stride_bytes;

    if (cfg.instanced_divisor[s] != 0u) {
      const uint32_t expanded_size = static_cast<uint32_t>(instanced_expanded[s].size());
      hr = ensure_instancing_vertex_buffer_locked(dev, s, expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      if (!emit_set_stream_source_locked(dev, s, dst_vb, 0, saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    } else {
      uint64_t new_offset_u64 = 0;
      if (!compute_vertex_base_offset(saved.offset_bytes, stride, &new_offset_u64)
          || new_offset_u64 > std::numeric_limits<uint32_t>::max()) {
        (void)restore_streams();
        return E_INVALIDARG;
      }
      if (!emit_set_stream_source_locked(dev, s, saved.vb, static_cast<uint32_t>(new_offset_u64), saved.stride_bytes)) {
        (void)restore_streams();
        return device_lost_override(dev, E_OUTOFMEMORY);
      }
    }
  }

  const uint32_t topology = d3d9_prim_to_topology(type);
  const size_t draw_pkt_bytes = align_up(sizeof(aerogpu_cmd_draw_indexed), 4);
  const size_t min_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) + draw_pkt_bytes;
  if (!ensure_cmd_space(dev, min_bytes)) {
    (void)restore_streams();
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  if (!emit_set_topology_locked(dev, topology)) {
    (void)restore_streams();
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  std::array<uint32_t, 16> last_elem{};
  last_elem.fill(std::numeric_limits<uint32_t>::max());

  for (uint32_t inst = 0; inst < cfg.instance_count; ++inst) {
    for (uint32_t s = 0; s < 16; ++s) {
      if (!used_streams.test(s) || cfg.instanced_divisor[s] == 0u) {
        continue;
      }
      const DeviceStateStream& saved = saved_streams[s];
      const uint64_t stride = saved.stride_bytes;
      const uint32_t divisor = cfg.instanced_divisor[s];
      const uint32_t elem = inst / divisor;
      if (elem == last_elem[s]) {
        continue;
      }
      last_elem[s] = elem;

      const uint8_t* inst_data =
          instanced_src[s].data + static_cast<size_t>(elem) * static_cast<size_t>(stride);
      uint8_t* dst_all = instanced_expanded[s].data();
      for (uint32_t v = 0; v < num_vertices; ++v) {
        std::memcpy(dst_all + static_cast<size_t>(v) * static_cast<size_t>(stride),
                    inst_data,
                    static_cast<size_t>(stride));
      }

      Resource* dst_vb = dev->instancing_vertex_buffers[s];
      const uint32_t expanded_size = static_cast<uint32_t>(instanced_expanded[s].size());
      hr = emit_upload_buffer_locked(dev, dst_vb, instanced_expanded[s].data(), expanded_size);
      if (FAILED(hr)) {
        (void)restore_streams();
        return hr;
      }
    }

    if (!ensure_cmd_space(dev, draw_pkt_bytes)) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)restore_streams();
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
    if (!cmd) {
      (void)restore_streams();
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->index_count = index_count;
    cmd->instance_count = 1;
    cmd->first_index = start_index;
    cmd->base_vertex = draw_base_vertex;
    cmd->first_instance = 0;
  }

  if (!restore_streams()) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  return S_OK;
}

} // namespace

HRESULT AEROGPU_D3D9_CALL device_draw_primitive(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t start_vertex,
    uint32_t primitive_count) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawPrimitive,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type),
                      d3d9_trace_pack_u32_u32(start_vertex, primitive_count),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  if (primitive_count == 0) {
    return trace.ret(S_OK);
  }

  const HRESULT inst_hr = try_draw_instanced_primitive_locked(dev, type, start_vertex, primitive_count);
  if (inst_hr != S_FALSE) {
    return trace.ret(inst_hr);
  }

  HRESULT hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  const bool fixedfunc_vertex_active = (fvf_variant != FixedFuncVariant::NONE) && dev->vertex_decl && !dev->user_vs;
  const bool fixedfunc_xyzrhw = fixedfunc_vertex_active && fixedfunc_variant_uses_rhw(fvf_variant);

  if (fixedfunc_vertex_active) {
    const DeviceStateStream& ss0 = dev->streams[0];
    if (!ss0.vb) {
      return trace.ret(kD3DErrInvalidCall);
    }
    const HRESULT stride_hr = validate_fixedfunc_vertex_stride(dev->fvf, ss0.stride_bytes);
    if (FAILED(stride_hr)) {
      return trace.ret(stride_hr);
    }
  }

  // Fixed-function emulation path: for XYZRHW vertices we upload a transformed
  // (clip-space) copy of the referenced vertices into a scratch VB and draw
  // using a built-in shader pair.
  if (fixedfunc_xyzrhw) {
    DeviceStateStream saved = dev->streams[0];
    DeviceStateStream& ss = dev->streams[0];

      const uint32_t vertex_count = vertex_count_from_primitive(type, primitive_count);
      const uint64_t src_offset_u64 =
          static_cast<uint64_t>(ss.offset_bytes) + static_cast<uint64_t>(start_vertex) * ss.stride_bytes;
      const uint64_t size_u64 = static_cast<uint64_t>(vertex_count) * ss.stride_bytes;
      const uint64_t vb_size_u64 = ss.vb->size_bytes;
      if (src_offset_u64 > vb_size_u64 || size_u64 > vb_size_u64 - src_offset_u64) {
        return E_INVALIDARG;
      }

      const uint8_t* src_vertices = nullptr;
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      void* vb_ptr = nullptr;
      bool vb_locked = false;
#endif

      bool use_vb_storage = ss.vb->storage.size() >= static_cast<size_t>(src_offset_u64 + size_u64);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      // Guest-backed buffers may still allocate a CPU shadow buffer (e.g. shared
      // resources opened via OpenResource). On real WDDM builds the authoritative
      // bytes live in the WDDM allocation, so prefer mapping it directly.
      if (ss.vb->backing_alloc_id != 0) {
        use_vb_storage = false;
      }
#endif

      if (use_vb_storage) {
        src_vertices = ss.vb->storage.data() + static_cast<size_t>(src_offset_u64);
      } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (ss.vb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
          const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                       dev->wddm_device,
                                                       ss.vb->wddm_hAllocation,
                                                       src_offset_u64,
                                                       size_u64,
                                                       kD3DLOCK_READONLY,
                                                       &vb_ptr,
                                                       dev->wddm_context.hContext);
          if (FAILED(lock_hr) || !vb_ptr) {
            return FAILED(lock_hr) ? lock_hr : E_FAIL;
          }
          vb_locked = true;
          src_vertices = static_cast<const uint8_t*>(vb_ptr);
        } else
#endif
        {
          return E_INVALIDARG;
        }
      }

      std::vector<uint8_t> converted;
      HRESULT hr = convert_xyzrhw_to_clipspace_locked(
          dev,
          src_vertices,
          ss.stride_bytes,
          vertex_count,
          &converted);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (vb_locked) {
        const HRESULT unlock_hr =
            wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, ss.vb->wddm_hAllocation, dev->wddm_context.hContext);
        if (FAILED(unlock_hr)) {
          logf("aerogpu-d3d9: draw_primitive fixedfunc: UnlockCb failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
               static_cast<unsigned long>(unlock_hr),
               static_cast<unsigned>(ss.vb->backing_alloc_id),
               static_cast<unsigned long long>(ss.vb->wddm_hAllocation));
          return unlock_hr;
        }
      }
#endif
      if (FAILED(hr)) {
        return hr;
      }

      hr = ensure_up_vertex_buffer_locked(dev, static_cast<uint32_t>(converted.size()));
      if (FAILED(hr)) {
        return hr;
      }
      hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, converted.data(), static_cast<uint32_t>(converted.size()));
      if (FAILED(hr)) {
        return hr;
      }

    if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, ss.stride_bytes)) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    const uint32_t topology = d3d9_prim_to_topology(type);
    if (!emit_set_topology_locked(dev, topology)) {
      (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    // Ensure the command buffer has space before we track allocations; tracking
    // may force a submission split, and command-buffer splits must not occur
    // after tracking or the allocation list would be out of sync.
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw), 4))) {
      (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
    if (!cmd) {
      (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->vertex_count = vertex_count;
    cmd->instance_count = 1;
    cmd->first_vertex = 0;
    cmd->first_instance = 0;

    if (!emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes)) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    return trace.ret(S_OK);
  }

  const uint32_t vertex_count = vertex_count_from_primitive(type, primitive_count);
  const size_t draw_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) +
                            align_up(sizeof(aerogpu_cmd_draw), 4);
  if (!ensure_cmd_space(dev, draw_bytes)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  const uint32_t topology = d3d9_prim_to_topology(type);
  if (!emit_set_topology_locked(dev, topology)) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw), 4))) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  hr = track_draw_state_locked(dev);
  if (hr < 0) {
    return hr;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
  if (!cmd) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }
  cmd->vertex_count = vertex_count;
  cmd->instance_count = 1;
  cmd->first_vertex = start_vertex;
  cmd->first_instance = 0;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_draw_primitive_up(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t primitive_count,
    const void* pVertexData,
    uint32_t stride_bytes) {
  const uint64_t packed = d3d9_trace_pack_u32_u32(primitive_count, stride_bytes);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawPrimitiveUP,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type),
                      packed,
                      d3d9_trace_arg_ptr(pVertexData));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  if (primitive_count == 0) {
    return trace.ret(S_OK);
  }
  if (!pVertexData || stride_bytes == 0) {
    return trace.ret(E_INVALIDARG);
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  const uint32_t vertex_count = vertex_count_from_primitive(type, primitive_count);
  const uint64_t size_u64 = static_cast<uint64_t>(vertex_count) * stride_bytes;
  if (size_u64 == 0 || size_u64 > 0x7FFFFFFFu) {
    return trace.ret(E_INVALIDARG);
  }

  HRESULT hr = S_OK;

  // Validate instancing configuration up-front so invalid-call failures do not
  // emit shader binds or UP uploads/bindings.
  if (any_stream_source_freq_non_default_locked(dev)) {
    std::bitset<16> used_streams{};
    if (!vertex_decl_used_streams(dev->vertex_decl, &used_streams)) {
      return trace.ret(kD3DErrInvalidCall);
    }
    bool any_used_non_default = false;
    for (uint32_t s = 0; s < 16; ++s) {
      if (used_streams.test(s) && dev->stream_source_freq[s] != 1u) {
        any_used_non_default = true;
        break;
      }
    }
    if (any_used_non_default) {
      InstancingConfig cfg{};
      hr = parse_instancing_config_locked(dev, used_streams, &cfg);
      if (FAILED(hr)) {
        return trace.ret(hr);
      }
      bool has_instanced_stream = false;
      for (uint32_t s = 0; s < 16; ++s) {
        if (used_streams.test(s) && cfg.instanced_divisor[s] != 0u) {
          has_instanced_stream = true;
          break;
        }
      }
      if (!(cfg.instance_count == 1 && !has_instanced_stream) && !dev->user_vs) {
        return trace.ret(kD3DErrInvalidCall);
      }
    }
  }

  hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  const bool fixedfunc_vertex_active = (fvf_variant != FixedFuncVariant::NONE) && dev->vertex_decl && !dev->user_vs;
  const bool fixedfunc_xyzrhw = fixedfunc_vertex_active && fixedfunc_variant_uses_rhw(fvf_variant);

  DeviceStateStream saved = dev->streams[0];

  if (fixedfunc_vertex_active) {
    const HRESULT stride_hr = validate_fixedfunc_vertex_stride(dev->fvf, stride_bytes);
    if (FAILED(stride_hr)) {
      return trace.ret(stride_hr);
    }
  }

  std::vector<uint8_t> converted;
  const void* upload_data = pVertexData;
  uint32_t upload_size = static_cast<uint32_t>(size_u64);
  uint32_t upload_stride = stride_bytes;

  if (fixedfunc_xyzrhw) {
    HRESULT hr = convert_xyzrhw_to_clipspace_locked(dev, pVertexData, stride_bytes, vertex_count, &converted);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
    upload_data = converted.data();
    upload_size = static_cast<uint32_t>(converted.size());
  }

  hr = ensure_up_vertex_buffer_locked(dev, upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }
  hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, upload_data, upload_size);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, upload_stride)) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }

  const HRESULT inst_hr = try_draw_instanced_primitive_locked(dev, type, /*start_vertex=*/0, primitive_count);
  if (inst_hr != S_FALSE) {
    if (!emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes)) {
      return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
    }
    return trace.ret(inst_hr);
  }
  const uint32_t topology = d3d9_prim_to_topology(type);
  if (!emit_set_topology_locked(dev, topology)) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw), 4))) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }
  hr = track_draw_state_locked(dev);
  if (FAILED(hr)) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return trace.ret(hr);
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
  if (!cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }
  cmd->vertex_count = vertex_count;
  cmd->instance_count = 1;
  cmd->first_vertex = 0;
  cmd->first_instance = 0;

  if (!emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes)) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive2(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWINDEXEDPRIMITIVE2* pDraw);

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive_up(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t min_vertex_index,
    uint32_t num_vertices,
    uint32_t primitive_count,
    const void* pIndexData,
    D3DDDIFORMAT index_data_format,
    const void* pVertexData,
    uint32_t stride_bytes) {
  const uint64_t min_num = d3d9_trace_pack_u32_u32(min_vertex_index, num_vertices);
  const uint64_t pc_stride = d3d9_trace_pack_u32_u32(primitive_count, stride_bytes);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawIndexedPrimitiveUP,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type),
                      min_num,
                      pc_stride);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  if (primitive_count == 0) {
    return trace.ret(S_OK);
  }
  if (!pVertexData || stride_bytes == 0 || !pIndexData || num_vertices == 0) {
    return trace.ret(E_INVALIDARG);
  }
  // Only INDEX16/INDEX32 are valid for DrawIndexedPrimitiveUP.
  if (index_data_format != kD3dFmtIndex16 && index_data_format != kD3dFmtIndex32) {
    return trace.ret(E_INVALIDARG);
  }

  D3DDDIARG_DRAWINDEXEDPRIMITIVE2 draw{};
  draw.PrimitiveType = type;
  draw.PrimitiveCount = primitive_count;
  draw.MinIndex = min_vertex_index;
  draw.NumVertices = num_vertices;
  draw.pIndexData = pIndexData;
  draw.IndexDataFormat = index_data_format;
  draw.pVertexStreamZeroData = pVertexData;
  draw.VertexStreamZeroStride = stride_bytes;
  return trace.ret(device_draw_indexed_primitive2(hDevice, &draw));
}

HRESULT AEROGPU_D3D9_CALL device_draw_primitive2(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWPRIMITIVE2* pDraw) {
  if (!hDevice.pDrvPrivate || !pDraw) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return E_INVALIDARG;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }
  if (pDraw->PrimitiveCount == 0) {
    return S_OK;
  }
  if (!pDraw->pVertexStreamZeroData || pDraw->VertexStreamZeroStride == 0) {
    return E_INVALIDARG;
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }
  const uint32_t vertex_count = vertex_count_from_primitive(pDraw->PrimitiveType, pDraw->PrimitiveCount);
  const uint64_t size_u64 = static_cast<uint64_t>(vertex_count) * pDraw->VertexStreamZeroStride;
  if (size_u64 == 0 || size_u64 > 0x7FFFFFFFu) {
    return E_INVALIDARG;
  }

  HRESULT hr = S_OK;

  // Validate instancing configuration up-front so invalid-call failures do not
  // emit shader binds or UP uploads/bindings.
  if (any_stream_source_freq_non_default_locked(dev)) {
    std::bitset<16> used_streams{};
    if (!vertex_decl_used_streams(dev->vertex_decl, &used_streams)) {
      return kD3DErrInvalidCall;
    }
    bool any_used_non_default = false;
    for (uint32_t s = 0; s < 16; ++s) {
      if (used_streams.test(s) && dev->stream_source_freq[s] != 1u) {
        any_used_non_default = true;
        break;
      }
    }
    if (any_used_non_default) {
      InstancingConfig cfg{};
      hr = parse_instancing_config_locked(dev, used_streams, &cfg);
      if (FAILED(hr)) {
        return hr;
      }
      bool has_instanced_stream = false;
      for (uint32_t s = 0; s < 16; ++s) {
        if (used_streams.test(s) && cfg.instanced_divisor[s] != 0u) {
          has_instanced_stream = true;
          break;
        }
      }
      if (!(cfg.instance_count == 1 && !has_instanced_stream) && !dev->user_vs) {
        return kD3DErrInvalidCall;
      }
    }
  }

  hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return hr;
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  const bool fixedfunc_vertex_active = (fvf_variant != FixedFuncVariant::NONE) && dev->vertex_decl && !dev->user_vs;
  const bool fixedfunc_xyzrhw = fixedfunc_vertex_active && fixedfunc_variant_uses_rhw(fvf_variant);

  DeviceStateStream saved = dev->streams[0];

  if (fixedfunc_vertex_active) {
    const HRESULT stride_hr = validate_fixedfunc_vertex_stride(dev->fvf, pDraw->VertexStreamZeroStride);
    if (FAILED(stride_hr)) {
      return stride_hr;
    }
  }

  std::vector<uint8_t> converted;
  const void* upload_data = pDraw->pVertexStreamZeroData;
  uint32_t upload_size = static_cast<uint32_t>(size_u64);
  uint32_t upload_stride = pDraw->VertexStreamZeroStride;

  if (fixedfunc_xyzrhw) {
    HRESULT hr = convert_xyzrhw_to_clipspace_locked(
        dev, pDraw->pVertexStreamZeroData, pDraw->VertexStreamZeroStride, vertex_count, &converted);
    if (FAILED(hr)) {
      return hr;
    }
    upload_data = converted.data();
    upload_size = static_cast<uint32_t>(converted.size());
  }

  hr = ensure_up_vertex_buffer_locked(dev, upload_size);
  if (FAILED(hr)) {
    return hr;
  }
  hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, upload_data, upload_size);
  if (FAILED(hr)) {
    return hr;
  }

  if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, upload_stride)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  const HRESULT inst_hr =
      try_draw_instanced_primitive_locked(dev, pDraw->PrimitiveType, /*start_vertex=*/0, pDraw->PrimitiveCount);
  if (inst_hr != S_FALSE) {
    if (!emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes)) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    return inst_hr;
  }
  const uint32_t topology = d3d9_prim_to_topology(pDraw->PrimitiveType);
  if (!emit_set_topology_locked(dev, topology)) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw), 4))) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  hr = track_draw_state_locked(dev);
  if (FAILED(hr)) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return hr;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
  if (!cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes);
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  cmd->vertex_count = vertex_count;
  cmd->instance_count = 1;
  cmd->first_vertex = 0;
  cmd->first_instance = 0;

  if (!emit_set_stream_source_locked(dev, 0, saved.vb, saved.offset_bytes, saved.stride_bytes)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  return S_OK;
}

static HRESULT device_draw_indexed_primitive2_locked(Device* dev,
                                                     const D3DDDIARG_DRAWINDEXEDPRIMITIVE2* pDraw);

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive2(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWINDEXEDPRIMITIVE2* pDraw) {
  if (!hDevice.pDrvPrivate || !pDraw) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return E_INVALIDARG;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }
  if (pDraw->PrimitiveCount == 0) {
    return S_OK;
  }
  if (!pDraw->pVertexStreamZeroData || pDraw->VertexStreamZeroStride == 0 || !pDraw->pIndexData) {
    return E_INVALIDARG;
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  return device_draw_indexed_primitive2_locked(dev, pDraw);
}

// Shared implementation for indexed UP draws. Callers must hold `Device::mutex`.
static HRESULT device_draw_indexed_primitive2_locked(
    Device* dev,
    const D3DDDIARG_DRAWINDEXEDPRIMITIVE2* pDraw) {
  if (!dev || !pDraw) {
    return E_INVALIDARG;
  }
  if (device_is_lost(dev)) {
    return device_lost_hresult(dev);
  }

  const uint32_t index_count = index_count_from_primitive(pDraw->PrimitiveType, pDraw->PrimitiveCount);
  const uint32_t index_size = (pDraw->IndexDataFormat == kD3dFmtIndex32) ? 4u : 2u;
  const uint64_t ib_size_u64 = static_cast<uint64_t>(index_count) * index_size;
  if (ib_size_u64 == 0 || ib_size_u64 > 0x7FFFFFFFu) {
    return E_INVALIDARG;
  }
  const uint32_t ib_size = static_cast<uint32_t>(ib_size_u64);
  const bool single_draw_safe =
      (pDraw->PrimitiveType == D3DDDIPT_TRIANGLELIST || pDraw->PrimitiveType == D3DDDIPT_LINELIST ||
       pDraw->PrimitiveType == D3DDDIPT_POINTLIST);

  const uint64_t vertex_count_u64 = static_cast<uint64_t>(pDraw->MinIndex) + static_cast<uint64_t>(pDraw->NumVertices);
  const uint64_t vb_size_u64 = vertex_count_u64 * static_cast<uint64_t>(pDraw->VertexStreamZeroStride);
  if (vertex_count_u64 == 0 || vb_size_u64 == 0 || vb_size_u64 > 0x7FFFFFFFu) {
    return E_INVALIDARG;
  }

  HRESULT hr = S_OK;
  bool instancing_active = false;
  uint32_t instancing_instance_count = 1;

  // Validate instancing configuration up-front so invalid-call failures do not
  // emit shader binds or UP uploads/bindings.
  if (any_stream_source_freq_non_default_locked(dev)) {
    std::bitset<16> used_streams{};
    if (!vertex_decl_used_streams(dev->vertex_decl, &used_streams)) {
      return kD3DErrInvalidCall;
    }
    bool any_used_non_default = false;
    for (uint32_t s = 0; s < 16; ++s) {
      if (used_streams.test(s) && dev->stream_source_freq[s] != 1u) {
        any_used_non_default = true;
        break;
      }
    }
    if (any_used_non_default) {
      InstancingConfig cfg{};
      hr = parse_instancing_config_locked(dev, used_streams, &cfg);
      if (FAILED(hr)) {
        return hr;
      }
      bool has_instanced_stream = false;
      for (uint32_t s = 0; s < 16; ++s) {
        if (used_streams.test(s) && cfg.instanced_divisor[s] != 0u) {
          has_instanced_stream = true;
          break;
        }
      }
      const bool instancing_enabled = !(cfg.instance_count == 1 && !has_instanced_stream);
      if (instancing_enabled) {
        instancing_active = true;
        instancing_instance_count = cfg.instance_count;
      }
      if (instancing_enabled && !dev->user_vs) {
        return kD3DErrInvalidCall;
      }
    }
  }

  hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return hr;
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  const bool fixedfunc_vertex_active = (fvf_variant != FixedFuncVariant::NONE) && dev->vertex_decl && !dev->user_vs;
  const bool fixedfunc_xyzrhw = fixedfunc_vertex_active && fixedfunc_variant_uses_rhw(fvf_variant);

  DeviceStateStream saved_stream = dev->streams[0];
  Resource* saved_ib = dev->index_buffer;
  const D3DDDIFORMAT saved_fmt = dev->index_format;
  const uint32_t saved_offset = dev->index_offset_bytes;

  if (fixedfunc_vertex_active) {
    const HRESULT stride_hr = validate_fixedfunc_vertex_stride(dev->fvf, pDraw->VertexStreamZeroStride);
    if (FAILED(stride_hr)) {
      return stride_hr;
    }
  }

  std::vector<uint8_t> converted;
  const void* vb_upload_data = pDraw->pVertexStreamZeroData;
  uint32_t vb_upload_size = static_cast<uint32_t>(vb_size_u64);
  uint32_t vb_upload_stride = pDraw->VertexStreamZeroStride;

  if (fixedfunc_xyzrhw) {
    HRESULT hr = convert_xyzrhw_to_clipspace_locked(
        dev,
        pDraw->pVertexStreamZeroData,
        pDraw->VertexStreamZeroStride,
        static_cast<uint32_t>(vertex_count_u64),
        &converted);
    if (FAILED(hr)) {
      return hr;
    }
    vb_upload_data = converted.data();
    vb_upload_size = static_cast<uint32_t>(converted.size());
  }

  hr = ensure_up_vertex_buffer_locked(dev, vb_upload_size);
  if (FAILED(hr)) {
    return hr;
  }
  hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, vb_upload_data, vb_upload_size);
  if (FAILED(hr)) {
    return hr;
  }

  uint32_t ib_alloc_size = ib_size;
  if (instancing_active && single_draw_safe) {
    const uint64_t expanded_index_bytes_u64 =
        static_cast<uint64_t>(index_count) * static_cast<uint64_t>(instancing_instance_count) * 4u;
    if (expanded_index_bytes_u64 != 0 && expanded_index_bytes_u64 <= 0x7FFFFFFFu) {
      ib_alloc_size = std::max(ib_alloc_size, static_cast<uint32_t>(expanded_index_bytes_u64));
    }
  }

  hr = ensure_up_index_buffer_locked(dev, ib_alloc_size);
  if (FAILED(hr)) {
    return hr;
  }
  hr = emit_upload_buffer_locked(dev, dev->up_index_buffer, pDraw->pIndexData, ib_size);
  if (FAILED(hr)) {
    return hr;
  }

  if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, vb_upload_stride)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  dev->index_buffer = dev->up_index_buffer;
  dev->index_format = pDraw->IndexDataFormat;
  dev->index_offset_bytes = 0;

  auto* ib_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (!ib_cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  ib_cmd->buffer = dev->up_index_buffer ? dev->up_index_buffer->handle : 0;
  ib_cmd->format = d3d9_index_format_to_aerogpu(pDraw->IndexDataFormat);
  ib_cmd->offset_bytes = 0;
  ib_cmd->reserved0 = 0;

  const HRESULT inst_hr = try_draw_instanced_indexed_primitive_locked(
      dev,
      pDraw->PrimitiveType,
      /*base_vertex=*/0,
      pDraw->MinIndex,
      pDraw->NumVertices,
      /*start_index=*/0,
      pDraw->PrimitiveCount);
  if (inst_hr != S_FALSE) {
    // Restore stream source 0.
    if (!emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes)) {
      // Restore IB state (best-effort) before returning.
      dev->index_buffer = saved_ib;
      dev->index_format = saved_fmt;
      dev->index_offset_bytes = saved_offset;
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    // Restore index buffer binding.
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    auto* restore_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (!restore_cmd) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    restore_cmd->buffer = saved_ib ? saved_ib->handle : 0;
    restore_cmd->format = d3d9_index_format_to_aerogpu(saved_fmt);
    restore_cmd->offset_bytes = saved_offset;
    restore_cmd->reserved0 = 0;
    return inst_hr;
  }

  const uint32_t topology = d3d9_prim_to_topology(pDraw->PrimitiveType);
  if (!emit_set_topology_locked(dev, topology)) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    // Restore IB state.
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    auto* restore = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (restore) {
      restore->buffer = saved_ib ? saved_ib->handle : 0;
      restore->format = d3d9_index_format_to_aerogpu(saved_fmt);
      restore->offset_bytes = saved_offset;
      restore->reserved0 = 0;
    }
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw_indexed), 4))) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    // Restore IB state.
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    auto* restore = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (restore) {
      restore->buffer = saved_ib ? saved_ib->handle : 0;
      restore->format = d3d9_index_format_to_aerogpu(saved_fmt);
      restore->offset_bytes = saved_offset;
      restore->reserved0 = 0;
    }
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  hr = track_draw_state_locked(dev);
  if (FAILED(hr)) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    // Restore IB state.
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    auto* restore = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (restore) {
      restore->buffer = saved_ib ? saved_ib->handle : 0;
      restore->format = d3d9_index_format_to_aerogpu(saved_fmt);
      restore->offset_bytes = saved_offset;
      restore->reserved0 = 0;
    }
    return hr;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
  if (!cmd) {
    (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
    // Restore IB state.
    dev->index_buffer = saved_ib;
    dev->index_format = saved_fmt;
    dev->index_offset_bytes = saved_offset;
    auto* restore = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
    if (restore) {
      restore->buffer = saved_ib ? saved_ib->handle : 0;
      restore->format = d3d9_index_format_to_aerogpu(saved_fmt);
      restore->offset_bytes = saved_offset;
      restore->reserved0 = 0;
    }
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  cmd->index_count = index_count;
  cmd->instance_count = 1;
  cmd->first_index = 0;
  cmd->base_vertex = 0;
  cmd->first_instance = 0;

  // Restore stream source 0.
  if (!emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  // Restore index buffer binding.
  dev->index_buffer = saved_ib;
  dev->index_format = saved_fmt;
  dev->index_offset_bytes = saved_offset;
  auto* restore_cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
  if (!restore_cmd) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }
  restore_cmd->buffer = saved_ib ? saved_ib->handle : 0;
  restore_cmd->format = d3d9_index_format_to_aerogpu(saved_fmt);
  restore_cmd->offset_bytes = saved_offset;
  restore_cmd->reserved0 = 0;
  return S_OK;
}


HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    int32_t base_vertex,
    uint32_t min_index,
    uint32_t num_vertices,
    uint32_t start_index,
    uint32_t primitive_count) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDrawIndexedPrimitive,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(type),
                      d3d9_trace_pack_u32_u32(static_cast<uint32_t>(base_vertex), start_index),
                      static_cast<uint64_t>(primitive_count));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  if (primitive_count == 0) {
    return trace.ret(S_OK);
  }

  const HRESULT inst_hr = try_draw_instanced_indexed_primitive_locked(
      dev, type, base_vertex, min_index, num_vertices, start_index, primitive_count);
  if (inst_hr != S_FALSE) {
    return trace.ret(inst_hr);
  }

  HRESULT hr = ensure_draw_pipeline_locked(dev);
  if (FAILED(hr)) {
    return trace.ret(hr);
  }

  const FixedFuncVariant fvf_variant = fixedfunc_variant_from_fvf(dev->fvf);
  const bool fixedfunc_vertex_active = (fvf_variant != FixedFuncVariant::NONE) && dev->vertex_decl && !dev->user_vs;
  const bool fixedfunc_xyzrhw = fixedfunc_vertex_active && fixedfunc_variant_uses_rhw(fvf_variant);

  // Fixed-function emulation for indexed draws: expand indices into a temporary
  // vertex stream and issue a non-indexed draw. This is intentionally
  // conservative but is sufficient for bring-up.
  if (fixedfunc_vertex_active) {
    const DeviceStateStream& ss0 = dev->streams[0];
    if (!ss0.vb) {
      return trace.ret(kD3DErrInvalidCall);
    }
    const HRESULT stride_hr = validate_fixedfunc_vertex_stride(dev->fvf, ss0.stride_bytes);
    if (FAILED(stride_hr)) {
      return trace.ret(stride_hr);
    }
  }

  if (fixedfunc_xyzrhw) {
    DeviceStateStream saved_stream = dev->streams[0];
    DeviceStateStream& ss = dev->streams[0];

    if (!dev->index_buffer) {
      return trace.ret(kD3DErrInvalidCall);
    }

    const uint32_t index_count = index_count_from_primitive(type, primitive_count);
    const uint32_t index_size = (dev->index_format == kD3dFmtIndex32) ? 4u : 2u;
    const uint64_t index_bytes_u64 = static_cast<uint64_t>(index_count) * index_size;
    const uint64_t index_offset_u64 =
        static_cast<uint64_t>(dev->index_offset_bytes) + static_cast<uint64_t>(start_index) * index_size;

    std::vector<uint8_t> expanded;
    const uint64_t expanded_bytes_u64 = static_cast<uint64_t>(index_count) * ss.stride_bytes;
    if (expanded_bytes_u64 == 0 || expanded_bytes_u64 > 0x7FFFFFFFu) {
      return E_INVALIDARG;
    }

    const uint64_t ib_size_u64 = dev->index_buffer->size_bytes;
    if (index_offset_u64 > ib_size_u64 || index_bytes_u64 > ib_size_u64 - index_offset_u64) {
      return E_INVALIDARG;
    }

    {
      const uint8_t* index_data = nullptr;
      const uint8_t* vb_base = nullptr;
      uint32_t min_vtx = 0;
      uint32_t max_vtx = 0;
      bool have_bounds = false;

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      struct AutoUnlock {
        Device* dev = nullptr;
        WddmAllocationHandle hAllocation = 0;
        uint32_t alloc_id = 0;
        const char* tag = nullptr;
        bool locked = false;

        AutoUnlock(Device* dev, WddmAllocationHandle hAllocation, uint32_t alloc_id, const char* tag)
            : dev(dev), hAllocation(hAllocation), alloc_id(alloc_id), tag(tag) {}

        ~AutoUnlock() noexcept {
          try {
            if (locked && dev && dev->wddm_device != 0 && hAllocation != 0) {
              const HRESULT hr = wddm_unlock_allocation(dev->wddm_callbacks, dev->wddm_device, hAllocation, dev->wddm_context.hContext);
              if (FAILED(hr)) {
                logf("aerogpu-d3d9: draw_indexed_primitive fixedfunc: UnlockCb(%s) failed hr=0x%08lx alloc_id=%u hAllocation=%llu\n",
                     tag ? tag : "?",
                     static_cast<unsigned long>(hr),
                     static_cast<unsigned>(alloc_id),
                     static_cast<unsigned long long>(hAllocation));
              }
            }
          } catch (...) {
          }
        }
      };

      AutoUnlock ib_lock(dev, dev->index_buffer->wddm_hAllocation, dev->index_buffer->backing_alloc_id, "IB");
      AutoUnlock vb_lock(dev, ss.vb->wddm_hAllocation, ss.vb->backing_alloc_id, "VB");
      void* ib_ptr = nullptr;
      void* vb_ptr = nullptr;
#endif

      // Lock index buffer if we don't have a CPU shadow copy.
      bool use_ib_storage = dev->index_buffer->storage.size() >= static_cast<size_t>(index_offset_u64 + index_bytes_u64);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      // Guest-backed buffers can have a CPU shadow allocation when they are
      // shared/OpenResource'd; in WDDM builds the underlying allocation memory is
      // authoritative.
      if (dev->index_buffer->backing_alloc_id != 0) {
        use_ib_storage = false;
      }
#endif
      if (use_ib_storage) {
        index_data = dev->index_buffer->storage.data() + static_cast<size_t>(index_offset_u64);
      } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (dev->index_buffer->wddm_hAllocation != 0 && dev->wddm_device != 0) {
           const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                        dev->wddm_device,
                                                        dev->index_buffer->wddm_hAllocation,
                                                        index_offset_u64,
                                                        index_bytes_u64,
                                                        kD3DLOCK_READONLY,
                                                        &ib_ptr,
                                                        dev->wddm_context.hContext);
           if (FAILED(lock_hr) || !ib_ptr) {
             return FAILED(lock_hr) ? lock_hr : E_FAIL;
           }
          ib_lock.locked = true;
          index_data = static_cast<const uint8_t*>(ib_ptr);
        } else
#endif
        {
          return E_INVALIDARG;
        }
      }

      // First pass: compute min/max referenced vertex index so we can map a single
      // contiguous vertex range.
      for (uint32_t i = 0; i < index_count; ++i) {
        uint32_t idx = 0;
        if (index_size == 4) {
          std::memcpy(&idx, index_data + static_cast<size_t>(i) * 4, sizeof(idx));
        } else {
          uint16_t idx16 = 0;
          std::memcpy(&idx16, index_data + static_cast<size_t>(i) * 2, sizeof(idx16));
          idx = idx16;
        }

        const int64_t vtx = static_cast<int64_t>(base_vertex) + static_cast<int64_t>(idx);
        if (vtx < 0) {
          return E_INVALIDARG;
        }
        const uint32_t vtx_u32 = static_cast<uint32_t>(vtx);
        if (!have_bounds) {
          min_vtx = vtx_u32;
          max_vtx = vtx_u32;
          have_bounds = true;
        } else {
          min_vtx = std::min(min_vtx, vtx_u32);
          max_vtx = std::max(max_vtx, vtx_u32);
        }
      }
      if (!have_bounds) {
        return E_INVALIDARG;
      }

      const uint64_t vb_size_u64 = ss.vb->size_bytes;
      const uint64_t vb_range_offset =
          static_cast<uint64_t>(ss.offset_bytes) + static_cast<uint64_t>(min_vtx) * ss.stride_bytes;
      const uint64_t vb_range_size =
          (static_cast<uint64_t>(max_vtx) - static_cast<uint64_t>(min_vtx) + 1) * ss.stride_bytes;
      if (vb_range_offset > vb_size_u64 || vb_range_size > vb_size_u64 - vb_range_offset) {
        return E_INVALIDARG;
      }

      bool use_vb_storage = ss.vb->storage.size() >= static_cast<size_t>(vb_range_offset + vb_range_size);
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
      if (ss.vb->backing_alloc_id != 0) {
        use_vb_storage = false;
      }
#endif
      if (use_vb_storage) {
        vb_base = ss.vb->storage.data() + static_cast<size_t>(vb_range_offset);
      } else {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
        if (ss.vb->wddm_hAllocation != 0 && dev->wddm_device != 0) {
           const HRESULT lock_hr = wddm_lock_allocation(dev->wddm_callbacks,
                                                        dev->wddm_device,
                                                        ss.vb->wddm_hAllocation,
                                                        vb_range_offset,
                                                        vb_range_size,
                                                        kD3DLOCK_READONLY,
                                                        &vb_ptr,
                                                        dev->wddm_context.hContext);
           if (FAILED(lock_hr) || !vb_ptr) {
             return FAILED(lock_hr) ? lock_hr : E_FAIL;
           }
          vb_lock.locked = true;
          vb_base = static_cast<const uint8_t*>(vb_ptr);
        } else
#endif
        {
          return E_INVALIDARG;
        }
      }

      try {
        expanded.resize(static_cast<size_t>(expanded_bytes_u64));
      } catch (...) {
        return device_lost_override(dev, E_OUTOFMEMORY);
      }

      float vp_x = 0.0f;
      float vp_y = 0.0f;
      float vp_w = 1.0f;
      float vp_h = 1.0f;
      get_viewport_dims_locked(dev, &vp_x, &vp_y, &vp_w, &vp_h);
      // Defensive: viewport dimensions are used as divisors for XYZRHW -> NDC
      // conversion. Some runtimes may pass garbage state; ensure we keep
      // conversion math finite.
      if (!std::isfinite(vp_x)) {
        vp_x = 0.0f;
      }
      if (!std::isfinite(vp_y)) {
        vp_y = 0.0f;
      }
      if (!std::isfinite(vp_w) || vp_w == 0.0f) {
        vp_w = 1.0f;
      }
      if (!std::isfinite(vp_h) || vp_h == 0.0f) {
        vp_h = 1.0f;
      }

      for (uint32_t i = 0; i < index_count; i++) {
        uint32_t idx = 0;
        if (index_size == 4) {
          std::memcpy(&idx, index_data + static_cast<size_t>(i) * 4, sizeof(idx));
        } else {
          uint16_t idx16 = 0;
          std::memcpy(&idx16, index_data + static_cast<size_t>(i) * 2, sizeof(idx16));
          idx = idx16;
        }

        const int64_t vtx = static_cast<int64_t>(base_vertex) + static_cast<int64_t>(idx);
        if (vtx < 0) {
          return E_INVALIDARG;
        }
        const uint32_t vtx_u32 = static_cast<uint32_t>(vtx);
        const uint64_t local_off =
            (static_cast<uint64_t>(vtx_u32) - static_cast<uint64_t>(min_vtx)) * ss.stride_bytes;
        if (local_off + ss.stride_bytes > vb_range_size) {
          return E_INVALIDARG;
        }

        const uint8_t* src = vb_base + static_cast<size_t>(local_off);
        uint8_t* dst = expanded.data() + static_cast<size_t>(i) * ss.stride_bytes;
        std::memcpy(dst, src, ss.stride_bytes);

        float x = read_f32_unaligned(src + 0);
        float y = read_f32_unaligned(src + 4);
        float z = read_f32_unaligned(src + 8);
        const float rhw = read_f32_unaligned(src + 12);
        // Keep coordinates finite. If the app provides NaN/Inf XYZRHW positions,
        // default to the center of the effective viewport (0,0 in NDC).
        if (!std::isfinite(x)) {
          x = vp_x + vp_w * 0.5f - 0.5f;
        }
        if (!std::isfinite(y)) {
          y = vp_y + vp_h * 0.5f - 0.5f;
        }
        if (!std::isfinite(z)) {
          z = 0.0f;
        }

        // `rhw` is the reciprocal clip-space w. Some apps may pass non-finite
        // values; keep conversion math finite so downstream shader math does not
        // get poisoned by NaNs/Infs.
        float w = 1.0f;
        if (rhw != 0.0f && std::isfinite(rhw)) {
          w = 1.0f / rhw;
          if (!std::isfinite(w)) {
            w = 1.0f;
          }
        }
        // D3D9's viewport transform uses a -0.5 pixel center convention. Invert it
        // so typical D3D9 pre-transformed vertex coordinates line up with pixel
        // centers.
        float ndc_x = ((x + 0.5f - vp_x) / vp_w) * 2.0f - 1.0f;
        float ndc_y = 1.0f - ((y + 0.5f - vp_y) / vp_h) * 2.0f;
        float ndc_z = z;
        if (!std::isfinite(ndc_x)) {
          ndc_x = 0.0f;
        }
        if (!std::isfinite(ndc_y)) {
          ndc_y = 0.0f;
        }
        if (!std::isfinite(ndc_z)) {
          ndc_z = 0.0f;
        }

        float clip_x = ndc_x * w;
        float clip_y = ndc_y * w;
        float clip_z = ndc_z * w;
        if (!std::isfinite(clip_x)) {
          clip_x = 0.0f;
        }
        if (!std::isfinite(clip_y)) {
          clip_y = 0.0f;
        }
        if (!std::isfinite(clip_z)) {
          clip_z = 0.0f;
        }

        write_f32_unaligned(dst + 0, clip_x);
        write_f32_unaligned(dst + 4, clip_y);
        write_f32_unaligned(dst + 8, clip_z);
        write_f32_unaligned(dst + 12, w);
      }
    }

    hr = ensure_up_vertex_buffer_locked(dev, static_cast<uint32_t>(expanded.size()));
    if (FAILED(hr)) {
      return hr;
    }
    hr = emit_upload_buffer_locked(dev, dev->up_vertex_buffer, expanded.data(), static_cast<uint32_t>(expanded.size()));
    if (FAILED(hr)) {
      return hr;
    }

    if (!emit_set_stream_source_locked(dev, 0, dev->up_vertex_buffer, 0, ss.stride_bytes)) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    const uint32_t topology = d3d9_prim_to_topology(type);
    if (!emit_set_topology_locked(dev, topology)) {
      (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }

    // Ensure the command buffer has space before we track allocations; tracking
    // may force a submission split, and command-buffer splits must not occur
    // after tracking or the allocation list would be out of sync.
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw), 4))) {
      (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    hr = track_draw_state_locked(dev);
    if (FAILED(hr)) {
      (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
      return hr;
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_draw>(dev, AEROGPU_CMD_DRAW);
    if (!cmd) {
      (void)emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes);
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    cmd->vertex_count = index_count;
    cmd->instance_count = 1;
    cmd->first_vertex = 0;
    cmd->first_instance = 0;

    if (!emit_set_stream_source_locked(dev, 0, saved_stream.vb, saved_stream.offset_bytes, saved_stream.stride_bytes)) {
      return device_lost_override(dev, E_OUTOFMEMORY);
    }
    return trace.ret(S_OK);
  }

  const uint32_t index_count = index_count_from_primitive(type, primitive_count);
  const size_t draw_bytes = align_up(sizeof(aerogpu_cmd_set_primitive_topology), 4) +
                            align_up(sizeof(aerogpu_cmd_draw_indexed), 4);
  if (!ensure_cmd_space(dev, draw_bytes)) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  const uint32_t topology = d3d9_prim_to_topology(type);
  if (!emit_set_topology_locked(dev, topology)) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }

  // Ensure the command buffer has space before we track allocations; tracking
  // may force a submission split, and command-buffer splits must not occur
  // after tracking or the allocation list would be out of sync.
  if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_draw_indexed), 4))) {
    return device_lost_override(dev, E_OUTOFMEMORY);
  }

  hr = track_draw_state_locked(dev);
  if (hr < 0) {
    return hr;
  }

  auto* cmd = append_fixed_locked<aerogpu_cmd_draw_indexed>(dev, AEROGPU_CMD_DRAW_INDEXED);
  if (!cmd) {
    return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
  }
  cmd->index_count = index_count;
  cmd->instance_count = 1;
  cmd->first_index = start_index;
  cmd->base_vertex = base_vertex;
  cmd->first_instance = 0;
  return trace.ret(S_OK);
}

// Callers must hold `Device::mutex`.
static void overlay_device_cursor_locked(Device* dev, Resource* present_src) {
  if (!dev || !present_src) {
    return;
  }
  if (!dev->cursor_visible) {
    return;
  }
  // If we successfully programmed the KMD hardware cursor path, avoid also
  // drawing a software cursor into the present source.
  if (dev->cursor_hw_active) {
    return;
  }
  Resource* cursor = dev->cursor_bitmap;
  if (!cursor) {
    return;
  }
  // Avoid self-sampling hazards: only overlay cursors sourced from a distinct resource.
  if (cursor == present_src) {
    return;
  }
  if (cursor->width == 0 || cursor->height == 0) {
    return;
  }

  const long dst_w = static_cast<long>(present_src->width);
  const long dst_h = static_cast<long>(present_src->height);
  if (dst_w <= 0 || dst_h <= 0) {
    return;
  }

  RECT src_rect{};
  src_rect.left = 0;
  src_rect.top = 0;
  src_rect.right = static_cast<long>(cursor->width);
  src_rect.bottom = static_cast<long>(cursor->height);

  RECT dst_rect{};
  const long dst_left = static_cast<long>(dev->cursor_x) - static_cast<long>(dev->cursor_hot_x);
  const long dst_top = static_cast<long>(dev->cursor_y) - static_cast<long>(dev->cursor_hot_y);
  dst_rect.left = dst_left;
  dst_rect.top = dst_top;
  dst_rect.right = dst_left + src_rect.right;
  dst_rect.bottom = dst_top + src_rect.bottom;

  // Clip to destination bounds, adjusting the source rect to preserve 1:1 mapping.
  if (dst_rect.left < 0) {
    const long dx = -dst_rect.left;
    dst_rect.left = 0;
    src_rect.left += dx;
  }
  if (dst_rect.top < 0) {
    const long dy = -dst_rect.top;
    dst_rect.top = 0;
    src_rect.top += dy;
  }
  if (dst_rect.right > dst_w) {
    const long dx = dst_rect.right - dst_w;
    dst_rect.right = dst_w;
    src_rect.right -= dx;
  }
  if (dst_rect.bottom > dst_h) {
    const long dy = dst_rect.bottom - dst_h;
    dst_rect.bottom = dst_h;
    src_rect.bottom -= dy;
  }

  if (dst_rect.right <= dst_rect.left || dst_rect.bottom <= dst_rect.top) {
    return;
  }
  if (src_rect.right <= src_rect.left || src_rect.bottom <= src_rect.top) {
    return;
  }

  constexpr uint32_t kD3d9TexFilterPoint = 1u; // D3DTEXF_POINT
  (void)blit_alpha_locked(dev, present_src, &dst_rect, cursor, &src_rect, kD3d9TexFilterPoint);
}

HRESULT AEROGPU_D3D9_CALL device_present_ex(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_PRESENTEX* pPresentEx) {
  const uint64_t wnd = pPresentEx ? d3d9_trace_arg_ptr(d3d9_present_hwnd(*pPresentEx)) : 0;
  const uint64_t sync_flags = pPresentEx ? d3d9_trace_pack_u32_u32(d3d9_present_sync_interval(*pPresentEx),
                                                                   d3d9_present_flags(*pPresentEx))
                                         : 0;
  const uint64_t src = pPresentEx ? d3d9_trace_arg_ptr(d3d9_present_src(*pPresentEx).pDrvPrivate) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DevicePresentEx, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), wnd, sync_flags, src);
  if (!hDevice.pDrvPrivate || !pPresentEx) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  const D3DDDI_HRESOURCE src_handle = d3d9_present_src(*pPresentEx);
  const uint32_t sync_interval = d3d9_present_sync_interval(*pPresentEx);
  const uint32_t present_flags = d3d9_present_flags(*pPresentEx);
  uint32_t present_count = 0;
  HRESULT present_hr = S_OK;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);

    bool occluded = false;
    // Returning S_PRESENT_OCCLUDED from PresentEx helps some D3D9Ex clients avoid
    // pathological present loops when their target window is minimized/occluded.
    // Keep the check cheap and never block on it.
    HWND hwnd = d3d9_present_hwnd(*pPresentEx);
    if (!hwnd) {
      SwapChain* sc = dev->current_swapchain;
      if (!sc && !dev->swapchains.empty()) {
        sc = dev->swapchains[0];
      }
      hwnd = sc ? sc->hwnd : nullptr;
    }
    occluded = hwnd_is_occluded(hwnd);

    if (occluded) {
      // Even when occluded, Present/PresentEx act as a flush point and must
      // advance D3D9Ex present statistics (GetPresentStats/GetLastPresentCount).
      retire_completed_presents_locked(dev);
      (void)submit(dev, /*is_present=*/false);
      if (device_is_lost(dev)) {
        return trace.ret(device_lost_hresult(dev));
      }

      dev->present_count++;
      present_count = dev->present_count;
      dev->present_refresh_count = dev->present_count;
      dev->sync_refresh_count = dev->present_count;
      dev->last_present_qpc = qpc_now();

      SwapChain* sc = dev->current_swapchain;
      if (!sc && !dev->swapchains.empty()) {
        sc = dev->swapchains[0];
      }
      if (sc) {
        sc->present_count++;
      }

      present_hr = kSPresentOccluded;
    } else {
      HRESULT hr = throttle_presents_locked(dev, present_flags);
      if (hr != S_OK) {
        return trace.ret(hr);
      }

      // Submit any pending render work via the Render callback before issuing a
      // Present submission. This ensures the KMD/emulator observes distinct
      // render vs present submissions (DxgkDdiRender vs DxgkDdiPresent).
      (void)submit(dev, /*is_present=*/false);
      if (device_is_lost(dev)) {
        return trace.ret(device_lost_hresult(dev));
      }

      // Composite the device-managed cursor (if enabled) over the present
      // source surface before we emit the PRESENT_EX packet.
      overlay_device_cursor_locked(dev, as_resource(src_handle));

      // Track the present source allocation so the KMD can resolve the backing
      // `alloc_id` via the per-submit allocation table even though we keep the
      // patch-location list empty.
      //
      // Ensure command space before tracking: tracking may split/submit and must
      // not occur after command-buffer overflow handling.
      if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_present_ex), 4))) {
        return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
      }
      if (auto* src_res = as_resource(src_handle)) {
        const HRESULT track_hr = track_resource_allocation_locked(dev, src_res, /*write=*/false);
        if (track_hr < 0) {
          return trace.ret(track_hr);
        }
      }

      auto* cmd = append_fixed_locked<aerogpu_cmd_present_ex>(dev, AEROGPU_CMD_PRESENT_EX);
      if (!cmd) {
        return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
      }
      cmd->scanout_id = 0;
      bool vsync = (sync_interval != 0) && (sync_interval != kD3dPresentIntervalImmediate);
      if (vsync && dev->adapter && dev->adapter->umd_private_valid) {
        // Only request vblank-paced presents when the active device reports vblank support.
        vsync = (dev->adapter->umd_private.flags & AEROGPU_UMDPRIV_FLAG_HAS_VBLANK) != 0;
      }
      cmd->flags = vsync ? AEROGPU_PRESENT_FLAG_VSYNC : AEROGPU_PRESENT_FLAG_NONE;
      cmd->d3d9_present_flags = present_flags;
      cmd->reserved0 = 0;

      const uint64_t submit_fence = submit(dev, /*is_present=*/true);
      if (device_is_lost(dev)) {
        return trace.ret(device_lost_hresult(dev));
      }
      const uint64_t present_fence = submit_fence;
      if (present_fence) {
        try {
          dev->inflight_present_fences.push_back(present_fence);
        } catch (...) {
          // Present has already been submitted; if we cannot track the fence (OOM), drop throttling state
          // rather than crashing.
          dev->inflight_present_fences.clear();
        }
      }

      dev->present_count++;
      present_count = dev->present_count;
      dev->present_refresh_count = dev->present_count;
      dev->sync_refresh_count = dev->present_count;
      dev->last_present_qpc = qpc_now();
      SwapChain* sc = dev->current_swapchain;
      if (!sc && !dev->swapchains.empty()) {
        sc = dev->swapchains[0];
      }
      if (sc) {
        sc->present_count++;
        sc->last_present_fence = present_fence;
        if (sc->backbuffers.size() > 1 && sc->swap_effect != 0u) {
          auto is_backbuffer = [sc](const Resource* res) -> bool {
            if (!sc || !res) {
              return false;
            }
            return std::find(sc->backbuffers.begin(), sc->backbuffers.end(), res) != sc->backbuffers.end();
          };

          // Present-style backbuffer rotation swaps the underlying identities
          // (host handle + backing allocation) attached to the backbuffer Resource
          // objects. If any backbuffers are currently bound via device state (RTs,
          // textures, IA buffers), we must re-emit those binds so the host stops
          // referencing the old handles.
          size_t needed_bytes = align_up(sizeof(aerogpu_cmd_set_render_targets), 4);
          for (uint32_t stage = 0; stage < 16; ++stage) {
            if (is_backbuffer(dev->textures[stage])) {
              needed_bytes += align_up(sizeof(aerogpu_cmd_set_texture), 4);
            }
          }
          for (uint32_t stream = 0; stream < 16; ++stream) {
            if (is_backbuffer(dev->streams[stream].vb)) {
              needed_bytes += align_up(sizeof(aerogpu_cmd_set_vertex_buffers) + sizeof(aerogpu_vertex_buffer_binding), 4);
            }
          }
          if (is_backbuffer(dev->index_buffer)) {
            needed_bytes += align_up(sizeof(aerogpu_cmd_set_index_buffer), 4);
          }

          if (ensure_cmd_space(dev, needed_bytes)) {
            struct ResourceIdentity {
              aerogpu_handle_t handle = 0;
              uint32_t backing_alloc_id = 0;
              uint32_t backing_offset_bytes = 0;
              uint64_t share_token = 0;
              bool is_shared = false;
              bool is_shared_alias = false;
              bool locked = false;
              uint32_t locked_offset = 0;
              uint32_t locked_size = 0;
              uint32_t locked_flags = 0;
              WddmAllocationHandle wddm_hAllocation = 0;
              std::vector<uint8_t> storage;
              std::vector<uint8_t> shared_private_driver_data;
            };

            auto take_identity = [](Resource* res) -> ResourceIdentity {
              ResourceIdentity id{};
              id.handle = res->handle;
              id.backing_alloc_id = res->backing_alloc_id;
              id.backing_offset_bytes = res->backing_offset_bytes;
              id.share_token = res->share_token;
              id.is_shared = res->is_shared;
              id.is_shared_alias = res->is_shared_alias;
              id.locked = res->locked;
              id.locked_offset = res->locked_offset;
              id.locked_size = res->locked_size;
              id.locked_flags = res->locked_flags;
              id.wddm_hAllocation = res->wddm_hAllocation;
              id.storage = std::move(res->storage);
              id.shared_private_driver_data = std::move(res->shared_private_driver_data);
              return id;
            };

            auto put_identity = [](Resource* res, ResourceIdentity&& id) {
              res->handle = id.handle;
              res->backing_alloc_id = id.backing_alloc_id;
              res->backing_offset_bytes = id.backing_offset_bytes;
              res->share_token = id.share_token;
              res->is_shared = id.is_shared;
              res->is_shared_alias = id.is_shared_alias;
              res->locked = id.locked;
              res->locked_offset = id.locked_offset;
              res->locked_size = id.locked_size;
              res->locked_flags = id.locked_flags;
              res->wddm_hAllocation = id.wddm_hAllocation;
              res->storage = std::move(id.storage);
              res->shared_private_driver_data = std::move(id.shared_private_driver_data);
            };

            auto undo_rotation = [sc, &take_identity, &put_identity]() {
              // Undo the rotation (rotate right by one).
              ResourceIdentity undo_saved = take_identity(sc->backbuffers.back());
              for (size_t i = sc->backbuffers.size() - 1; i > 0; --i) {
                put_identity(sc->backbuffers[i], take_identity(sc->backbuffers[i - 1]));
              }
              put_identity(sc->backbuffers[0], std::move(undo_saved));
            };

            // Rotate left by one.
             ResourceIdentity saved = take_identity(sc->backbuffers[0]);
             for (size_t i = 0; i + 1 < sc->backbuffers.size(); ++i) {
               put_identity(sc->backbuffers[i], take_identity(sc->backbuffers[i + 1]));
             }
             put_identity(sc->backbuffers.back(), std::move(saved));
 
             bool ok = true;
             if (dev->wddm_context.hContext != 0 &&
                 dev->alloc_list_tracker.list_base() != nullptr &&
                 dev->alloc_list_tracker.list_capacity_effective() != 0) {
               // The rebinding commands reference multiple resources. Individual
               // allocation tracking calls can internally split the submission when
               // the allocation list is full; if that happens mid-sequence, earlier
               // tracked allocations are dropped and the submission would be missing
               // alloc-table entries for some binds. Pre-scan and split once before
               // tracking.
               std::array<UINT, 4 + 1 + 16 + 16 + 1> unique_allocs{};
               size_t unique_alloc_len = 0;
               auto add_alloc = [&unique_allocs, &unique_alloc_len](const Resource* res) {
                 if (!res) {
                   return;
                 }
                 if (res->backing_alloc_id == 0) {
                   return;
                 }
                 if (res->wddm_hAllocation == 0) {
                   return;
                 }
                 const UINT alloc_id = res->backing_alloc_id;
                 for (size_t i = 0; i < unique_alloc_len; ++i) {
                   if (unique_allocs[i] == alloc_id) {
                     return;
                   }
                 }
                 unique_allocs[unique_alloc_len++] = alloc_id;
               };
 
               for (uint32_t i = 0; i < 4; ++i) {
                 add_alloc(dev->render_targets[i]);
               }
               add_alloc(dev->depth_stencil);
               for (uint32_t stage = 0; stage < 16; ++stage) {
                 if (is_backbuffer(dev->textures[stage])) {
                   add_alloc(dev->textures[stage]);
                 }
               }
               for (uint32_t stream = 0; stream < 16; ++stream) {
                 if (is_backbuffer(dev->streams[stream].vb)) {
                   add_alloc(dev->streams[stream].vb);
                 }
               }
               if (is_backbuffer(dev->index_buffer)) {
                 add_alloc(dev->index_buffer);
               }
 
               const UINT needed_total = static_cast<UINT>(unique_alloc_len);
               if (needed_total != 0) {
                 const UINT cap = dev->alloc_list_tracker.list_capacity_effective();
                 if (needed_total > cap) {
                   ok = false;
                 } else {
                   UINT needed_new = 0;
                   for (size_t i = 0; i < unique_alloc_len; ++i) {
                     if (!dev->alloc_list_tracker.contains_alloc_id(unique_allocs[i])) {
                       needed_new++;
                     }
                   }
                   const UINT existing = dev->alloc_list_tracker.list_len();
                   if (existing > cap || needed_new > cap - existing) {
                     (void)submit(dev);
                     if (!ensure_cmd_space(dev, needed_bytes)) {
                       ok = false;
                     }
                   }
                 }
               }
             }
 
             // Track allocations referenced by the rebinding commands so the KMD can
             // resolve alloc_id -> GPA even if no draw occurs before the next
             // flush/present.
             if (ok && track_render_targets_locked(dev) < 0) {
               ok = false;
             }
             for (uint32_t stage = 0; ok && stage < 16; ++stage) {
               if (!is_backbuffer(dev->textures[stage])) {
                 continue;
               }
               if (track_resource_allocation_locked(dev, dev->textures[stage], /*write=*/false) < 0) {
                ok = false;
              }
            }
            for (uint32_t stream = 0; ok && stream < 16; ++stream) {
              if (!is_backbuffer(dev->streams[stream].vb)) {
                continue;
              }
              if (track_resource_allocation_locked(dev, dev->streams[stream].vb, /*write=*/false) < 0) {
                ok = false;
              }
            }
            if (ok && is_backbuffer(dev->index_buffer)) {
              if (track_resource_allocation_locked(dev, dev->index_buffer, /*write=*/false) < 0) {
                ok = false;
              }
            }

            ok = ok && emit_set_render_targets_locked(dev);
            for (uint32_t stage = 0; ok && stage < 16; ++stage) {
              if (!is_backbuffer(dev->textures[stage])) {
                continue;
              }
              auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE);
              if (!cmd) {
                ok = false;
                break;
              }
              cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
              cmd->slot = stage;
              cmd->texture = dev->textures[stage] ? dev->textures[stage]->handle : 0;
              cmd->reserved0 = 0;
            }

            for (uint32_t stream = 0; ok && stream < 16; ++stream) {
              if (!is_backbuffer(dev->streams[stream].vb)) {
                continue;
              }

              aerogpu_vertex_buffer_binding binding{};
              binding.buffer = dev->streams[stream].vb ? dev->streams[stream].vb->handle : 0;
              binding.stride_bytes = dev->streams[stream].stride_bytes;
              binding.offset_bytes = dev->streams[stream].offset_bytes;
              binding.reserved0 = 0;

              auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
                  dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding));
              if (!cmd) {
                ok = false;
                break;
              }
              cmd->start_slot = stream;
              cmd->buffer_count = 1;
            }

            if (ok && is_backbuffer(dev->index_buffer)) {
              auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
              if (!cmd) {
                ok = false;
              } else {
                cmd->buffer = dev->index_buffer ? dev->index_buffer->handle : 0;
                cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
                cmd->offset_bytes = dev->index_offset_bytes;
                cmd->reserved0 = 0;
              }
            }

            if (!ok) {
              // Preserve device/host state consistency: if we cannot emit the
              // rebinding commands, undo the rotation so future draws still target
              // the host's current bindings.
              undo_rotation();
              dev->cmd.reset();
              dev->alloc_list_tracker.reset();
            }
          }
        }
      }
    }
  }
  if (device_is_lost(dev)) {
    present_hr = device_lost_hresult(dev);
  }

  const HRESULT trace_hr = trace.ret(present_hr);
  trace.maybe_dump_on_present(present_count);
  return trace_hr;
}

HRESULT AEROGPU_D3D9_CALL device_present(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_PRESENT* pPresent) {
  const uint64_t sc_ptr = pPresent ? d3d9_trace_arg_ptr(pPresent->hSwapChain.pDrvPrivate) : 0;
  const uint64_t src_ptr = pPresent ? d3d9_trace_arg_ptr(d3d9_present_src(*pPresent).pDrvPrivate) : 0;
  const uint64_t sync_flags = pPresent ? d3d9_trace_pack_u32_u32(d3d9_present_sync_interval(*pPresent),
                                                                 d3d9_present_flags(*pPresent))
                                       : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DevicePresent, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), sc_ptr, src_ptr, sync_flags);
  if (!hDevice.pDrvPrivate || !pPresent) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (device_is_lost(dev)) {
    return trace.ret(device_lost_hresult(dev));
  }
  const D3DDDI_HRESOURCE src_handle = d3d9_present_src(*pPresent);
  const uint32_t sync_interval = d3d9_present_sync_interval(*pPresent);
  const uint32_t present_flags = d3d9_present_flags(*pPresent);
  const HWND wnd = d3d9_present_hwnd(*pPresent);
  uint32_t present_count = 0;
  HRESULT present_hr = S_OK;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);

    bool occluded = false;
    HWND hwnd = wnd;
    if (!hwnd) {
      SwapChain* sc = as_swapchain(pPresent->hSwapChain);
      if (sc) {
        auto it = std::find(dev->swapchains.begin(), dev->swapchains.end(), sc);
        if (it == dev->swapchains.end()) {
          sc = nullptr;
        }
      }
      if (!sc) {
        sc = dev->current_swapchain;
      }
      if (!sc && !dev->swapchains.empty()) {
        sc = dev->swapchains[0];
      }
      hwnd = sc ? sc->hwnd : nullptr;
    }
    occluded = hwnd_is_occluded(hwnd);

    if (occluded) {
      retire_completed_presents_locked(dev);
      (void)submit(dev, /*is_present=*/false);
      if (device_is_lost(dev)) {
        return trace.ret(device_lost_hresult(dev));
      }

      dev->present_count++;
      present_count = dev->present_count;
      dev->present_refresh_count = dev->present_count;
      dev->sync_refresh_count = dev->present_count;
      dev->last_present_qpc = qpc_now();

      SwapChain* sc = as_swapchain(pPresent->hSwapChain);
      if (sc) {
        auto it = std::find(dev->swapchains.begin(), dev->swapchains.end(), sc);
        if (it == dev->swapchains.end()) {
          sc = nullptr;
        }
      }
      if (!sc) {
        sc = dev->current_swapchain;
      }
      if (!sc && !dev->swapchains.empty()) {
        sc = dev->swapchains[0];
      }
      if (sc) {
        sc->present_count++;
      }

      present_hr = kSPresentOccluded;
    } else {
    HRESULT hr = throttle_presents_locked(dev, present_flags);
    if (hr != S_OK) {
      return trace.ret(hr);
    }

    // Submit any pending render work via the Render callback before issuing a
    // Present submission. This ensures the KMD/emulator observes distinct
    // render vs present submissions (DxgkDdiRender vs DxgkDdiPresent).
    (void)submit(dev, /*is_present=*/false);
    if (device_is_lost(dev)) {
      return trace.ret(device_lost_hresult(dev));
    }

    // Composite the device-managed cursor (if enabled) over the present source
    // surface before we emit the PRESENT_EX packet.
    overlay_device_cursor_locked(dev, as_resource(src_handle));

    // Track the present source allocation so the KMD can resolve it when the
    // Present callback hands the DMA buffer to the kernel.
    if (!ensure_cmd_space(dev, align_up(sizeof(aerogpu_cmd_present_ex), 4))) {
      return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
    }
    if (auto* src_res = as_resource(src_handle)) {
      const HRESULT track_hr = track_resource_allocation_locked(dev, src_res, /*write=*/false);
      if (track_hr < 0) {
        return trace.ret(track_hr);
      }
    }

    auto* cmd = append_fixed_locked<aerogpu_cmd_present_ex>(dev, AEROGPU_CMD_PRESENT_EX);
    if (!cmd) {
      return trace.ret(device_lost_override(dev, E_OUTOFMEMORY));
    }
    cmd->scanout_id = 0;
    bool vsync = (sync_interval != 0) && (sync_interval != kD3dPresentIntervalImmediate);
    if (vsync && dev->adapter && dev->adapter->umd_private_valid) {
      vsync = (dev->adapter->umd_private.flags & AEROGPU_UMDPRIV_FLAG_HAS_VBLANK) != 0;
    }
    cmd->flags = vsync ? AEROGPU_PRESENT_FLAG_VSYNC : AEROGPU_PRESENT_FLAG_NONE;
    cmd->d3d9_present_flags = present_flags;
    cmd->reserved0 = 0;

    const uint64_t submit_fence = submit(dev, /*is_present=*/true);
    if (device_is_lost(dev)) {
      return trace.ret(device_lost_hresult(dev));
    }
    const uint64_t present_fence = submit_fence;
    if (present_fence) {
      try {
        dev->inflight_present_fences.push_back(present_fence);
      } catch (...) {
        dev->inflight_present_fences.clear();
      }
    }

    dev->present_count++;
    present_count = dev->present_count;
    dev->present_refresh_count = dev->present_count;
    dev->sync_refresh_count = dev->present_count;
    dev->last_present_qpc = qpc_now();
    SwapChain* sc = as_swapchain(pPresent->hSwapChain);
    if (sc) {
      auto it = std::find(dev->swapchains.begin(), dev->swapchains.end(), sc);
      if (it == dev->swapchains.end()) {
        sc = nullptr;
      }
    }
    if (!sc) {
      sc = dev->current_swapchain;
    }
    if (!sc && (wnd || src_handle.pDrvPrivate)) {
      for (SwapChain* candidate : dev->swapchains) {
        if (!candidate) {
          continue;
        }
        if (wnd && candidate->hwnd == wnd) {
          sc = candidate;
          break;
        }
        if (src_handle.pDrvPrivate) {
          auto* src = as_resource(src_handle);
          if (src && std::find(candidate->backbuffers.begin(), candidate->backbuffers.end(), src) != candidate->backbuffers.end()) {
            sc = candidate;
            break;
          }
        }
      }
    }
    if (!sc && !dev->swapchains.empty()) {
      sc = dev->swapchains[0];
    }
    if (sc) {
      sc->present_count++;
      sc->last_present_fence = present_fence;
      if (sc->backbuffers.size() > 1 && sc->swap_effect != 0u) {
        auto is_backbuffer = [sc](const Resource* res) -> bool {
          if (!sc || !res) {
            return false;
          }
          return std::find(sc->backbuffers.begin(), sc->backbuffers.end(), res) != sc->backbuffers.end();
        };

        size_t needed_bytes = align_up(sizeof(aerogpu_cmd_set_render_targets), 4);
        for (uint32_t stage = 0; stage < 16; ++stage) {
          if (is_backbuffer(dev->textures[stage])) {
            needed_bytes += align_up(sizeof(aerogpu_cmd_set_texture), 4);
          }
        }
        for (uint32_t stream = 0; stream < 16; ++stream) {
          if (is_backbuffer(dev->streams[stream].vb)) {
            needed_bytes += align_up(sizeof(aerogpu_cmd_set_vertex_buffers) + sizeof(aerogpu_vertex_buffer_binding), 4);
          }
        }
        if (is_backbuffer(dev->index_buffer)) {
          needed_bytes += align_up(sizeof(aerogpu_cmd_set_index_buffer), 4);
        }

        if (ensure_cmd_space(dev, needed_bytes)) {
          struct ResourceIdentity {
            aerogpu_handle_t handle = 0;
            uint32_t backing_alloc_id = 0;
            uint32_t backing_offset_bytes = 0;
            uint64_t share_token = 0;
            bool is_shared = false;
            bool is_shared_alias = false;
            bool locked = false;
            uint32_t locked_offset = 0;
            uint32_t locked_size = 0;
            uint32_t locked_flags = 0;
            WddmAllocationHandle wddm_hAllocation = 0;
            std::vector<uint8_t> storage;
            std::vector<uint8_t> shared_private_driver_data;
          };

          auto take_identity = [](Resource* res) -> ResourceIdentity {
            ResourceIdentity id{};
            id.handle = res->handle;
            id.backing_alloc_id = res->backing_alloc_id;
            id.backing_offset_bytes = res->backing_offset_bytes;
            id.share_token = res->share_token;
            id.is_shared = res->is_shared;
            id.is_shared_alias = res->is_shared_alias;
            id.locked = res->locked;
            id.locked_offset = res->locked_offset;
            id.locked_size = res->locked_size;
            id.locked_flags = res->locked_flags;
            id.wddm_hAllocation = res->wddm_hAllocation;
            id.storage = std::move(res->storage);
            id.shared_private_driver_data = std::move(res->shared_private_driver_data);
            return id;
          };

          auto put_identity = [](Resource* res, ResourceIdentity&& id) {
            res->handle = id.handle;
            res->backing_alloc_id = id.backing_alloc_id;
            res->backing_offset_bytes = id.backing_offset_bytes;
            res->share_token = id.share_token;
            res->is_shared = id.is_shared;
            res->is_shared_alias = id.is_shared_alias;
            res->locked = id.locked;
            res->locked_offset = id.locked_offset;
            res->locked_size = id.locked_size;
            res->locked_flags = id.locked_flags;
            res->wddm_hAllocation = id.wddm_hAllocation;
            res->storage = std::move(id.storage);
            res->shared_private_driver_data = std::move(id.shared_private_driver_data);
          };

          auto undo_rotation = [sc, &take_identity, &put_identity]() {
            ResourceIdentity undo_saved = take_identity(sc->backbuffers.back());
            for (size_t i = sc->backbuffers.size() - 1; i > 0; --i) {
              put_identity(sc->backbuffers[i], take_identity(sc->backbuffers[i - 1]));
            }
            put_identity(sc->backbuffers[0], std::move(undo_saved));
          };

          ResourceIdentity saved = take_identity(sc->backbuffers[0]);
          for (size_t i = 0; i + 1 < sc->backbuffers.size(); ++i) {
            put_identity(sc->backbuffers[i], take_identity(sc->backbuffers[i + 1]));
          }
          put_identity(sc->backbuffers.back(), std::move(saved));

          bool ok = true;
          if (dev->wddm_context.hContext != 0 &&
              dev->alloc_list_tracker.list_base() != nullptr &&
              dev->alloc_list_tracker.list_capacity_effective() != 0) {
            // See PresentEx: pre-scan all allocations referenced by the rebinding
            // commands and split once before tracking so we don't drop earlier
            // allocations when the list is full.
            std::array<UINT, 4 + 1 + 16 + 16 + 1> unique_allocs{};
            size_t unique_alloc_len = 0;
            auto add_alloc = [&unique_allocs, &unique_alloc_len](const Resource* res) {
              if (!res) {
                return;
              }
              if (res->backing_alloc_id == 0) {
                return;
              }
              if (res->wddm_hAllocation == 0) {
                return;
              }
              const UINT alloc_id = res->backing_alloc_id;
              for (size_t i = 0; i < unique_alloc_len; ++i) {
                if (unique_allocs[i] == alloc_id) {
                  return;
                }
              }
              unique_allocs[unique_alloc_len++] = alloc_id;
            };

            for (uint32_t i = 0; i < 4; ++i) {
              add_alloc(dev->render_targets[i]);
            }
            add_alloc(dev->depth_stencil);
            for (uint32_t stage = 0; stage < 16; ++stage) {
              if (is_backbuffer(dev->textures[stage])) {
                add_alloc(dev->textures[stage]);
              }
            }
            for (uint32_t stream = 0; stream < 16; ++stream) {
              if (is_backbuffer(dev->streams[stream].vb)) {
                add_alloc(dev->streams[stream].vb);
              }
            }
            if (is_backbuffer(dev->index_buffer)) {
              add_alloc(dev->index_buffer);
            }

            const UINT needed_total = static_cast<UINT>(unique_alloc_len);
            if (needed_total != 0) {
              const UINT cap = dev->alloc_list_tracker.list_capacity_effective();
              if (needed_total > cap) {
                ok = false;
              } else {
                UINT needed_new = 0;
                for (size_t i = 0; i < unique_alloc_len; ++i) {
                  if (!dev->alloc_list_tracker.contains_alloc_id(unique_allocs[i])) {
                    needed_new++;
                  }
                }
                const UINT existing = dev->alloc_list_tracker.list_len();
                if (existing > cap || needed_new > cap - existing) {
                  (void)submit(dev);
                  if (!ensure_cmd_space(dev, needed_bytes)) {
                    ok = false;
                  }
                }
              }
            }
          }

          if (ok && track_render_targets_locked(dev) < 0) {
            ok = false;
          }
          for (uint32_t stage = 0; ok && stage < 16; ++stage) {
            if (!is_backbuffer(dev->textures[stage])) {
              continue;
            }
            if (track_resource_allocation_locked(dev, dev->textures[stage], /*write=*/false) < 0) {
              ok = false;
            }
          }
          for (uint32_t stream = 0; ok && stream < 16; ++stream) {
            if (!is_backbuffer(dev->streams[stream].vb)) {
              continue;
            }
            if (track_resource_allocation_locked(dev, dev->streams[stream].vb, /*write=*/false) < 0) {
              ok = false;
            }
          }
          if (ok && is_backbuffer(dev->index_buffer)) {
            if (track_resource_allocation_locked(dev, dev->index_buffer, /*write=*/false) < 0) {
              ok = false;
            }
          }

          ok = ok && emit_set_render_targets_locked(dev);
          for (uint32_t stage = 0; ok && stage < 16; ++stage) {
            if (!is_backbuffer(dev->textures[stage])) {
              continue;
            }
            auto* cmd = append_fixed_locked<aerogpu_cmd_set_texture>(dev, AEROGPU_CMD_SET_TEXTURE);
            if (!cmd) {
              ok = false;
              break;
            }
            cmd->shader_stage = AEROGPU_SHADER_STAGE_PIXEL;
            cmd->slot = stage;
            cmd->texture = dev->textures[stage] ? dev->textures[stage]->handle : 0;
            cmd->reserved0 = 0;
          }

          for (uint32_t stream = 0; ok && stream < 16; ++stream) {
            if (!is_backbuffer(dev->streams[stream].vb)) {
              continue;
            }

            aerogpu_vertex_buffer_binding binding{};
            binding.buffer = dev->streams[stream].vb ? dev->streams[stream].vb->handle : 0;
            binding.stride_bytes = dev->streams[stream].stride_bytes;
            binding.offset_bytes = dev->streams[stream].offset_bytes;
            binding.reserved0 = 0;

            auto* cmd = append_with_payload_locked<aerogpu_cmd_set_vertex_buffers>(
                dev, AEROGPU_CMD_SET_VERTEX_BUFFERS, &binding, sizeof(binding));
            if (!cmd) {
              ok = false;
              break;
            }
            cmd->start_slot = stream;
            cmd->buffer_count = 1;
          }

          if (ok && is_backbuffer(dev->index_buffer)) {
            auto* cmd = append_fixed_locked<aerogpu_cmd_set_index_buffer>(dev, AEROGPU_CMD_SET_INDEX_BUFFER);
            if (!cmd) {
              ok = false;
            } else {
              cmd->buffer = dev->index_buffer ? dev->index_buffer->handle : 0;
              cmd->format = d3d9_index_format_to_aerogpu(dev->index_format);
              cmd->offset_bytes = dev->index_offset_bytes;
              cmd->reserved0 = 0;
            }
          }

          if (!ok) {
            undo_rotation();
            dev->cmd.reset();
            dev->alloc_list_tracker.reset();
          }
        }
      }
    }
    }
  }

  const HRESULT trace_hr = trace.ret(present_hr);
  trace.maybe_dump_on_present(present_count);
  return trace_hr;
}

HRESULT AEROGPU_D3D9_CALL device_set_maximum_frame_latency(
    D3DDDI_HDEVICE hDevice,
    uint32_t max_frame_latency) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetMaximumFrameLatency,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(max_frame_latency),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  if (max_frame_latency == 0) {
    return trace.ret(E_INVALIDARG);
  }
  dev->max_frame_latency = std::clamp(max_frame_latency, kMaxFrameLatencyMin, kMaxFrameLatencyMax);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_maximum_frame_latency(
    D3DDDI_HDEVICE hDevice,
    uint32_t* pMaxFrameLatency) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetMaximumFrameLatency,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pMaxFrameLatency),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pMaxFrameLatency) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  *pMaxFrameLatency = dev->max_frame_latency;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_present_stats(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_PRESENTSTATS* pStats) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetPresentStats,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pStats),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pStats) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  std::memset(pStats, 0, sizeof(*pStats));
  pStats->PresentCount = dev->present_count;
  pStats->PresentRefreshCount = dev->present_refresh_count;
  pStats->SyncRefreshCount = dev->sync_refresh_count;
  pStats->SyncQPCTime = static_cast<int64_t>(dev->last_present_qpc);
  pStats->SyncGPUTime = 0;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_last_present_count(
    D3DDDI_HDEVICE hDevice,
    uint32_t* pLastPresentCount) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetLastPresentCount,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pLastPresentCount),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pLastPresentCount) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  *pLastPresentCount = dev->present_count;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_set_gpu_thread_priority(D3DDDI_HDEVICE hDevice, int32_t priority) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetGPUThreadPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(static_cast<uint32_t>(priority)),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->gpu_thread_priority = std::clamp(priority, kMinGpuThreadPriority, kMaxGpuThreadPriority);
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_gpu_thread_priority(D3DDDI_HDEVICE hDevice, int32_t* pPriority) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetGPUThreadPriority,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pPriority),
                      0,
                      0);
  if (!hDevice.pDrvPrivate || !pPriority) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  *pPriority = dev->gpu_thread_priority;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_query_resource_residency(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_QUERYRESOURCERESIDENCY* pArgs) {
  const uint32_t resource_count = pArgs ? d3d9_query_resource_residency_count(*pArgs) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceQueryResourceResidency,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(resource_count),
                      pArgs ? d3d9_trace_arg_ptr(pArgs->pResidencyStatus) : 0,
                      d3d9_trace_arg_ptr(pArgs));
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  // System-memory-only model: resources are always considered resident.

  if (pArgs && pArgs->pResidencyStatus) {
    for (uint32_t i = 0; i < resource_count; i++) {
      pArgs->pResidencyStatus[i] = 1;
    }
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_display_mode_ex(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_GETDISPLAYMODEEX* pGetModeEx) {
  const uint64_t mode_ptr = pGetModeEx ? d3d9_trace_arg_ptr(pGetModeEx->pMode) : 0;
  const uint64_t rotation_ptr = pGetModeEx ? d3d9_trace_arg_ptr(pGetModeEx->pRotation) : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetDisplayModeEx,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pGetModeEx),
                      mode_ptr,
                      rotation_ptr);
  if (!hDevice.pDrvPrivate || !pGetModeEx) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  Adapter* adapter = dev->adapter;
  if (!adapter) {
    return trace.ret(E_FAIL);
  }

  if (pGetModeEx->pMode) {
    D3DDDI_DISPLAYMODEEX mode{};
    mode.Size = sizeof(D3DDDI_DISPLAYMODEEX);
    mode.Width = adapter->primary_width;
    mode.Height = adapter->primary_height;
    mode.RefreshRate = adapter->primary_refresh_hz;
    mode.Format = adapter->primary_format;
    // D3DDDI_SCANLINEORDERING_PROGRESSIVE (Win7) - numeric value.
    mode.ScanLineOrdering = 1;
    *pGetModeEx->pMode = mode;
  }

  if (pGetModeEx->pRotation) {
    *pGetModeEx->pRotation = static_cast<D3DDDI_ROTATION>(adapter->primary_rotation);
  }

  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_compose_rects(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_COMPOSERECTS* pComposeRects) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceComposeRects,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(pComposeRects),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  // ComposeRects is used by some D3D9Ex clients (including DWM in some modes).
  // Initial bring-up: accept and no-op to keep composition alive.
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_flush(D3DDDI_HDEVICE hDevice) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceFlush, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), 0, 0, 0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  return trace.ret(flush_locked(dev));
}

HRESULT AEROGPU_D3D9_CALL device_wait_for_vblank(D3DDDI_HDEVICE hDevice, uint32_t swap_chain_index) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceWaitForVBlank,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(swap_chain_index),
                      0,
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    sleep_ms(16);
    return trace.ret(S_OK);
  }

#if defined(_WIN32)
  uint32_t period_ms = 16;
  if (dev->adapter->primary_refresh_hz != 0) {
    period_ms = std::max<uint32_t>(1, 1000u / dev->adapter->primary_refresh_hz);
  }
  // Some display stacks (particularly remote/virtualised ones) can report bizarre
  // refresh rates (e.g. 1Hz, or extremely high values that would otherwise lead
  // to near-zero sleep times). Clamp the computed period so WaitForVBlank
  // remains bounded and DWM never stalls for seconds or devolves into a busy
  // loop.
  period_ms = std::clamp<uint32_t>(period_ms, 4u, 50u);

  // Prefer a real vblank wait when possible (KMD-backed scanline polling),
  // but always keep the wait bounded so DWM cannot hang if vblank delivery is
  // broken.
  const uint32_t timeout_ms = std::min<uint32_t>(40, std::max<uint32_t>(1, period_ms * 2));
  uint32_t vid_pn_source_id = 0;
  if (dev->adapter->vid_pn_source_id_valid) {
    vid_pn_source_id = dev->adapter->vid_pn_source_id;
  }
  if (dev->adapter->kmd_query.WaitForVBlank(vid_pn_source_id, timeout_ms)) {
    return trace.ret(S_OK);
  }
  sleep_ms(std::min<uint32_t>(period_ms, timeout_ms));
#else
  sleep_ms(16);
#endif
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_check_resource_residency(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE* pResources,
    uint32_t count) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCheckResourceResidency,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(count),
                      d3d9_trace_arg_ptr(pResources),
                      0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  // System-memory-only model: resources are always considered resident.
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_create_query(
    D3DDDI_HDEVICE hDevice,
    D3D9DDIARG_CREATEQUERY* pCreateQuery) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceCreateQuery,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pCreateQuery ? static_cast<uint64_t>(d3d9_query_type(*pCreateQuery)) : 0,
                      d3d9_trace_arg_ptr(pCreateQuery),
                      0);
  if (!hDevice.pDrvPrivate || !pCreateQuery) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  Adapter* adapter = dev->adapter;
  const uint32_t query_type = d3d9_query_type(*pCreateQuery);
  bool is_event = false;
  {
    std::lock_guard<std::mutex> lock(adapter->fence_mutex);
    if (!adapter->event_query_type_known.load(std::memory_order_acquire)) {
      // Accept both the public D3DQUERYTYPE_EVENT (8) encoding and the DDI-style
      // encoding where EVENT is the first enum entry (0). Once observed, lock
      // in the value so we don't accidentally treat other query types as EVENT.
      if (query_type == 0u || query_type == kD3DQueryTypeEvent) {
        adapter->event_query_type.store(query_type, std::memory_order_relaxed);
        adapter->event_query_type_known.store(true, std::memory_order_release);
      }
    }
    const bool known = adapter->event_query_type_known.load(std::memory_order_acquire);
    const uint32_t event_type = adapter->event_query_type.load(std::memory_order_relaxed);
    is_event = known && (query_type == event_type);
  }

  if (!is_event) {
    pCreateQuery->hQuery.pDrvPrivate = nullptr;
    return trace.ret(D3DERR_NOTAVAILABLE);
  }

  auto q = make_unique_nothrow<Query>();
  if (!q) {
    pCreateQuery->hQuery.pDrvPrivate = nullptr;
    return trace.ret(E_OUTOFMEMORY);
  }
  q->type = query_type;
  pCreateQuery->hQuery.pDrvPrivate = q.release();
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_destroy_query(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HQUERY hQuery) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceDestroyQuery,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      d3d9_trace_arg_ptr(hQuery.pDrvPrivate),
                      0,
                      0);
  auto* dev = as_device(hDevice);
  auto* q = as_query(hQuery);
  if (dev && q) {
    std::lock_guard<std::mutex> lock(dev->mutex);
    auto& pending = dev->pending_event_queries;
    if (!pending.empty()) {
      pending.erase(std::remove(pending.begin(), pending.end(), q), pending.end());
    }
  }
  delete q;
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_issue_query(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_ISSUEQUERY* pIssueQuery) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceIssueQuery,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pIssueQuery ? d3d9_trace_arg_ptr(pIssueQuery->hQuery.pDrvPrivate) : 0,
                      pIssueQuery ? static_cast<uint64_t>(d3d9_present_flags(*pIssueQuery)) : 0,
                      0);
  if (!hDevice.pDrvPrivate || !pIssueQuery) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  auto* q = as_query(pIssueQuery->hQuery);
  if (!q) {
    return trace.ret(E_INVALIDARG);
  }
  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }

  std::lock_guard<std::mutex> lock(dev->mutex);

  Adapter* adapter = dev->adapter;
  const bool event_known = adapter->event_query_type_known.load(std::memory_order_acquire);
  const uint32_t event_type = adapter->event_query_type.load(std::memory_order_relaxed);
  const bool is_event =
      event_known ? (q->type == event_type) : (q->type == 0u || q->type == kD3DQueryTypeEvent);
  if (!is_event) {
    return trace.ret(D3DERR_NOTAVAILABLE);
  }

  const uint32_t flags = d3d9_present_flags(*pIssueQuery);
  // Some runtimes appear to pass 0 for END. Be permissive and treat both 0 and
  // the common END bit encodings as an END marker (0x1 in the public D3D9 API,
  // 0x2 in some DDI header vintages).
  const bool end = (flags == 0) || ((flags & kD3DIssueEnd) != 0) || ((flags & kD3DIssueEndAlt) != 0);
  if (!end) {
    return trace.ret(S_OK);
  }

  // D3D9Ex EVENT queries are polled by DWM using GetData(DONOTFLUSH). To keep
  // those polls non-blocking, we submit any recorded work here (so the query
  // latches a real per-submit fence value), but we intentionally do *not* make
  // the query visible to GetData(DONOTFLUSH) until a later explicit
  // flush/submission boundary (Flush/Present/GetData(FLUSH)).
  //
  const bool had_pending_cmds = !dev->cmd.empty();
  dev->pending_event_queries.erase(std::remove(dev->pending_event_queries.begin(),
                                               dev->pending_event_queries.end(),
                                               q),
                                   dev->pending_event_queries.end());
  q->issued.store(true, std::memory_order_release);
  q->completion_logged.store(false, std::memory_order_relaxed);

  if (!had_pending_cmds) {
    // No pending commands: associate the query with the most recent submission.
    q->fence_value.store(dev->last_submission_fence, std::memory_order_release);
    q->submitted.store(true, std::memory_order_release);
    return trace.ret(S_OK);
  }

  const uint64_t issue_fence = submit(dev);

  q->fence_value.store(issue_fence, std::memory_order_release);
  q->submitted.store(false, std::memory_order_relaxed);
  try {
    dev->pending_event_queries.push_back(q);
  } catch (...) {
    // Best-effort: if we cannot track the query in the pending list (OOM), fall back to treating it as
    // immediately submitted so GetData can still observe forward progress.
    q->submitted.store(true, std::memory_order_release);
  }
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL device_get_query_data(
    D3DDDI_HDEVICE hDevice,
    const D3D9DDIARG_GETQUERYDATA* pGetQueryData) {
  const uint64_t data_flags = pGetQueryData ? d3d9_trace_pack_u32_u32(d3d9_query_data_size(*pGetQueryData),
                                                                      d3d9_present_flags(*pGetQueryData))
                                            : 0;
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetQueryData,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      pGetQueryData ? d3d9_trace_arg_ptr(pGetQueryData->hQuery.pDrvPrivate) : 0,
                      data_flags,
                      pGetQueryData ? d3d9_trace_arg_ptr(pGetQueryData->pData) : 0);
  if (!hDevice.pDrvPrivate || !pGetQueryData) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  const uint32_t data_size = d3d9_query_data_size(*pGetQueryData);
  const uint32_t flags = d3d9_present_flags(*pGetQueryData);
  if (device_is_lost(dev)) {
    if (pGetQueryData->pData && data_size != 0) {
      std::memset(pGetQueryData->pData, 0, data_size);
    }
    return trace.ret(device_lost_hresult(dev));
  }
  auto* q = as_query(pGetQueryData->hQuery);
  if (!q) {
    return trace.ret(E_INVALIDARG);
  }

  if (!dev || !dev->adapter) {
    return trace.ret(E_FAIL);
  }
  Adapter* adapter = dev->adapter;

  const bool event_known = adapter->event_query_type_known.load(std::memory_order_acquire);
  const uint32_t event_type = adapter->event_query_type.load(std::memory_order_relaxed);
  const bool is_event =
      event_known ? (q->type == event_type) : (q->type == 0u || q->type == kD3DQueryTypeEvent);
  if (!is_event) {
    return trace.ret(D3DERR_NOTAVAILABLE);
  }

  const bool has_data_ptr = (pGetQueryData->pData != nullptr);
  const bool has_data_size = (data_size != 0);
  // Mirror IDirect3DQuery9::GetData validation: pData must be NULL iff data_size
  // is 0. Treat mismatched pointer/size as D3DERR_INVALIDCALL.
  if (has_data_ptr != has_data_size) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  // EVENT queries return a BOOL-like DWORD; validate the output buffer size even
  // when the query is not yet ready so callers observe D3DERR_INVALIDCALL.
  if (has_data_ptr && data_size < sizeof(uint32_t)) {
    return trace.ret(D3DERR_INVALIDCALL);
  }

  // If no output buffer provided, just report readiness via HRESULT.
  const bool need_data = has_data_ptr;

  if (!q->issued.load(std::memory_order_acquire)) {
    // D3D9 clients can call GetData before Issue(END). Treat it as "not ready"
    // rather than a hard error to keep polling code (DWM) robust.
    if (need_data && data_size >= sizeof(uint32_t)) {
      *reinterpret_cast<uint32_t*>(pGetQueryData->pData) = FALSE;
    }
    return trace.ret(S_FALSE);
  }

  // EVENT query has been issued but not yet associated with a submission fence.
  // This happens when Issue(END) was called but we have not hit a flush/submission
  // boundary yet.
  if (!q->submitted.load(std::memory_order_acquire)) {
    if (flags & kD3DGetDataFlush) {
      // Non-blocking GetData(FLUSH): attempt a single flush to force a submission
      // boundary, then re-check. Never wait here (DWM can call into GetData while
      // holding global locks). Also avoid blocking on the device mutex: if another
      // thread is inside the UMD we skip the flush attempt and fall back to
      // polling.
      std::unique_lock<std::mutex> dev_lock(dev->mutex, std::try_to_lock);
      if (dev_lock.owns_lock()) {
        const HRESULT flush_hr = flush_locked(dev);
        if (flush_hr == kD3dErrDeviceLost || device_is_lost(dev)) {
          if (need_data && pGetQueryData->pData && data_size != 0) {
            std::memset(pGetQueryData->pData, 0, data_size);
          }
          return trace.ret(device_lost_override(dev, flush_hr));
        }
      }
    }
    if (!q->submitted.load(std::memory_order_acquire)) {
      return trace.ret(S_FALSE);
    }
  }

  uint64_t fence_value = q->fence_value.load(std::memory_order_acquire);

  FenceWaitResult wait_res = wait_for_fence(dev, fence_value, /*timeout_ms=*/0);
  if (wait_res == FenceWaitResult::NotReady && (flags & kD3DGetDataFlush)) {
    // Non-blocking GetData(FLUSH): attempt a single flush then re-check. Never
    // wait here (DWM can call into GetData while holding global locks). Also
    // avoid blocking on the device mutex: if another thread is inside the UMD
    // we skip the flush attempt and fall back to polling.
    std::unique_lock<std::mutex> dev_lock(dev->mutex, std::try_to_lock);
    if (dev_lock.owns_lock()) {
      const HRESULT flush_hr = flush_locked(dev);
      if (flush_hr == kD3dErrDeviceLost || device_is_lost(dev)) {
        if (need_data && pGetQueryData->pData && data_size != 0) {
          std::memset(pGetQueryData->pData, 0, data_size);
        }
        return trace.ret(device_lost_override(dev, flush_hr));
      }
    }
    fence_value = q->fence_value.load(std::memory_order_acquire);
    wait_res = wait_for_fence(dev, fence_value, /*timeout_ms=*/0);
  }

  if (wait_res == FenceWaitResult::Complete) {
    if (need_data) {
      // D3DQUERYTYPE_EVENT expects a BOOL-like result.
      if (data_size < sizeof(uint32_t)) {
        return trace.ret(D3DERR_INVALIDCALL);
      }
      *reinterpret_cast<uint32_t*>(pGetQueryData->pData) = TRUE;
    }
    (void)q->completion_logged.exchange(true, std::memory_order_relaxed);
    return trace.ret(S_OK);
  }
  if (wait_res == FenceWaitResult::Failed) {
    return trace.ret(E_FAIL);
  }
  return trace.ret(S_FALSE);
}

HRESULT AEROGPU_D3D9_CALL device_wait_for_idle(D3DDDI_HDEVICE hDevice) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceWaitForIdle, d3d9_trace_arg_ptr(hDevice.pDrvPrivate), 0, 0, 0);
  if (!hDevice.pDrvPrivate) {
    return trace.ret(E_INVALIDARG);
  }
  auto* dev = as_device(hDevice);
  if (!dev) {
    return trace.ret(E_INVALIDARG);
  }

  uint64_t fence_value = 0;
  {
    std::lock_guard<std::mutex> lock(dev->mutex);
    fence_value = submit(dev);
  }
  if (fence_value == 0) {
    return trace.ret(S_OK);
  }

  // Never block indefinitely in a DDI call. Waiting for idle should be best-effort:
  // if the GPU stops making forward progress we return a non-fatal "still drawing"
  // code so callers can decide how to proceed.
  const uint64_t deadline = monotonic_ms() + 2000;
  while (monotonic_ms() < deadline) {
    const uint64_t now = monotonic_ms();
    const uint64_t remaining = (deadline > now) ? (deadline - now) : 0;
    const uint64_t slice = std::min<uint64_t>(remaining, 250);

    const FenceWaitResult wait_res = wait_for_fence(dev, fence_value, /*timeout_ms=*/slice);
    if (wait_res == FenceWaitResult::Complete) {
      return trace.ret(S_OK);
    }
    if (wait_res == FenceWaitResult::Failed) {
      return trace.ret(E_FAIL);
    }
  }

  const FenceWaitResult final_check = wait_for_fence(dev, fence_value, /*timeout_ms=*/0);
  if (final_check == FenceWaitResult::Complete) {
    return trace.ret(S_OK);
  }
  if (final_check == FenceWaitResult::Failed) {
    return trace.ret(E_FAIL);
  }
  return trace.ret(kD3dErrWasStillDrawing);
}

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
namespace {

std::atomic<uint64_t> g_raster_status_sim_line{0};

template <typename T, typename = void>
struct has_member_InVBlank : std::false_type {};

template <typename T>
struct has_member_InVBlank<T, std::void_t<decltype(std::declval<T>().InVBlank)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_InVerticalBlank : std::false_type {};

template <typename T>
struct has_member_InVerticalBlank<T, std::void_t<decltype(std::declval<T>().InVerticalBlank)>> : std::true_type {};

template <typename T, typename = void>
struct has_member_ScanLine : std::false_type {};

template <typename T>
struct has_member_ScanLine<T, std::void_t<decltype(std::declval<T>().ScanLine)>> : std::true_type {};

template <typename RasterStatusT>
void write_raster_status(RasterStatusT* out, bool in_vblank, uint32_t scan_line) {
  if (!out) {
    return;
  }
  if constexpr (has_member_InVBlank<RasterStatusT>::value) {
    out->InVBlank = in_vblank ? TRUE : FALSE;
  } else if constexpr (has_member_InVerticalBlank<RasterStatusT>::value) {
    out->InVerticalBlank = in_vblank ? TRUE : FALSE;
  }
  if constexpr (has_member_ScanLine<RasterStatusT>::value) {
    out->ScanLine = scan_line;
  }
}

template <typename DeviceHandleT, typename SwapChainT, typename RasterStatusT>
HRESULT device_get_raster_status_impl(DeviceHandleT hDevice, SwapChainT swap_chain, RasterStatusT* pRasterStatus) {
  const auto packed = d3d9_stub_trace_args(hDevice, swap_chain, pRasterStatus);
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetRasterStatus, packed[0], packed[1], packed[2], packed[3]);

  if (!pRasterStatus) {
    return trace.ret(E_INVALIDARG);
  }

  void* drv_private = nullptr;
  if constexpr (aerogpu_has_member_pDrvPrivate<DeviceHandleT>::value) {
    drv_private = hDevice.pDrvPrivate;
  }
  if (!drv_private) {
    write_raster_status(pRasterStatus, /*in_vblank=*/false, /*scan_line=*/0);
    return trace.ret(E_INVALIDARG);
  }

  auto* dev = reinterpret_cast<Device*>(drv_private);
  Adapter* adapter = dev->adapter;
  if (!adapter) {
    write_raster_status(pRasterStatus, /*in_vblank=*/false, /*scan_line=*/0);
    return trace.ret(S_OK);
  }

  bool in_vblank = false;
  uint32_t scan_line = 0;
  const uint32_t vid_pn_source_id = adapter->vid_pn_source_id_valid ? adapter->vid_pn_source_id : 0;
  const bool ok = adapter->kmd_query.GetScanLine(vid_pn_source_id, &in_vblank, &scan_line);
  if (!ok) {
    const uint32_t height = adapter->primary_height ? adapter->primary_height : 768u;
    const uint32_t vblank_lines = std::max<uint32_t>(1u, height / 20u);
    const uint32_t total_lines = height + vblank_lines;
    const uint64_t tick = g_raster_status_sim_line.fetch_add(1, std::memory_order_relaxed);
    const uint32_t pos = static_cast<uint32_t>(tick % total_lines);
    in_vblank = (pos >= height);
    scan_line = in_vblank ? 0u : pos;
  }

  write_raster_status(pRasterStatus, in_vblank, scan_line);
  return trace.ret(S_OK);
}

template <typename... Args>
HRESULT device_get_raster_status_dispatch(Args... args) {
  if constexpr (sizeof...(Args) == 3) {
    return device_get_raster_status_impl(args...);
  }
  return D3DERR_NOTAVAILABLE;
}

template <typename Fn>
struct aerogpu_d3d9_impl_pfnGetRasterStatus;

template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRasterStatus<Ret(__stdcall*)(Args...)> {
  static Ret __stdcall pfnGetRasterStatus(Args... args) {
    return static_cast<Ret>(device_get_raster_status_dispatch(args...));
  }
};

template <typename Ret, typename... Args>
struct aerogpu_d3d9_impl_pfnGetRasterStatus<Ret(*)(Args...)> {
  static Ret pfnGetRasterStatus(Args... args) {
    return static_cast<Ret>(device_get_raster_status_dispatch(args...));
  }
};

} // namespace
#endif

// -----------------------------------------------------------------------------
// Portable ABI: fixed-function transform DDIs (SetTransform/MultiplyTransform/GetTransform)
// -----------------------------------------------------------------------------
// The full Win7 D3D9 UMD exposes these entrypoints even for fixed-function apps.
// Our host-side portable ABI historically omitted them (tests didn't need them),
// but fixed-function XYZ pipelines require transform updates:
// - Untransformed `D3DFVF_XYZ*` fixed-function draws use internal WVP vertex shader
//   variants that read `WORLD0 * VIEW * PROJECTION` from a reserved high VS constant
//   range (`c240..c243`) uploaded by `ensure_fixedfunc_wvp_constants_locked()`.
// - The minimal lighting subset (`D3DFVF_XYZ | D3DFVF_NORMAL{,DIFFUSE}{,TEX1}`)
//   also consumes WORLD/VIEW for normal transform (uploaded in c208..c210).
//
// Keep these outside the WDK-only DDI block so they are available in portable
// builds (Linux/CI) and in Windows portable builds that do not use WDK headers.
static HRESULT AEROGPU_D3D9_CALL device_set_transform_portable(
    D3DDDI_HDEVICE hDevice,
    D3DTRANSFORMSTATETYPE state,
    const D3DMATRIX* pMatrix) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceSetTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(state),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t idx = static_cast<uint32_t>(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const bool affects_fixedfunc_matrix =
      (idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection);
  const bool affects_fixedfunc_lighting = (idx == kD3dTransformWorld0 || idx == kD3dTransformView);

  // Avoid spurious fixed-function constant updates when the runtime (or app)
  // redundantly calls SetTransform with identical data.
  bool changed = true;
  if (affects_fixedfunc_matrix || affects_fixedfunc_lighting) {
    changed = std::memcmp(dev->transform_matrices[idx], pMatrix, 16u * sizeof(float)) != 0;
  }
  if (changed) {
    std::memcpy(dev->transform_matrices[idx], pMatrix, 16u * sizeof(float));
    if (affects_fixedfunc_matrix) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (affects_fixedfunc_lighting) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  stateblock_record_transform_locked(dev, idx, dev->transform_matrices[idx]);

  // If the fixed-function WVP vertex shader path is currently active, eagerly
  // refresh the reserved VS constant range so the next draw does not need to
  // re-upload it (and so state queries/tests can observe the dirty bit cleared
  // immediately after transform updates).
  //
  // Important: do not touch the reserved range when a user vertex shader is
  // bound, as that would clobber app-provided constants.
  if (affects_fixedfunc_matrix && !dev->user_vs && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
  }
  return trace.ret(S_OK);
}

static HRESULT AEROGPU_D3D9_CALL device_multiply_transform_portable(
    D3DDDI_HDEVICE hDevice,
    D3DTRANSFORMSTATETYPE state,
    const D3DMATRIX* pMatrix) {
  D3d9TraceCall trace(D3d9TraceFunc::DeviceMultiplyTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(state),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t idx = static_cast<uint32_t>(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);

  float tmp[16];
  float rhs[16];
  std::memcpy(rhs, pMatrix, sizeof(rhs));
  d3d9_mul_mat4_row_major(dev->transform_matrices[idx], rhs, tmp);

  const bool affects_fixedfunc_matrix =
      (idx == kD3dTransformWorld0 || idx == kD3dTransformView || idx == kD3dTransformProjection);
  const bool affects_fixedfunc_lighting = (idx == kD3dTransformWorld0 || idx == kD3dTransformView);

  bool changed = true;
  if (affects_fixedfunc_matrix || affects_fixedfunc_lighting) {
    changed = std::memcmp(dev->transform_matrices[idx], tmp, sizeof(tmp)) != 0;
  }
  if (changed) {
    std::memcpy(dev->transform_matrices[idx], tmp, sizeof(tmp));
    if (affects_fixedfunc_matrix) {
      dev->fixedfunc_matrix_dirty = true;
    }
    if (affects_fixedfunc_lighting) {
      dev->fixedfunc_lighting_dirty = true;
    }
  }
  stateblock_record_transform_locked(dev, idx, dev->transform_matrices[idx]);

  // Mirror SetTransform: when fixed-function WVP rendering is active, eagerly
  // update the reserved VS constant range on MultiplyTransform so the first
  // subsequent draw does not redundantly re-upload the matrix constants.
  if (affects_fixedfunc_matrix && !dev->user_vs && fixedfunc_fvf_needs_matrix(dev->fvf)) {
    const HRESULT hr = ensure_fixedfunc_wvp_constants_locked(dev);
    if (FAILED(hr)) {
      return trace.ret(hr);
    }
  }
  return trace.ret(S_OK);
}

static HRESULT AEROGPU_D3D9_CALL device_get_transform_portable(
    D3DDDI_HDEVICE hDevice,
    D3DTRANSFORMSTATETYPE state,
    D3DMATRIX* pMatrix) {
  // Default to identity so callers never observe uninitialized output.
  if (pMatrix) {
    std::memset(pMatrix, 0, 16 * sizeof(float));
    float* f = reinterpret_cast<float*>(pMatrix);
    f[0] = 1.0f;
    f[5] = 1.0f;
    f[10] = 1.0f;
    f[15] = 1.0f;
  }
  D3d9TraceCall trace(D3d9TraceFunc::DeviceGetTransform,
                      d3d9_trace_arg_ptr(hDevice.pDrvPrivate),
                      static_cast<uint64_t>(state),
                      d3d9_trace_arg_ptr(pMatrix),
                      0);
  if (!hDevice.pDrvPrivate || !pMatrix) {
    return trace.ret(E_INVALIDARG);
  }

  const uint32_t idx = static_cast<uint32_t>(state);
  if (idx >= Device::kTransformCacheCount) {
    return trace.ret(kD3DErrInvalidCall);
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memcpy(pMatrix, dev->transform_matrices[idx], 16 * sizeof(float));
  return trace.ret(S_OK);
}

HRESULT AEROGPU_D3D9_CALL adapter_create_device(
    D3D9DDIARG_CREATEDEVICE* pCreateDevice,
    D3D9DDI_DEVICEFUNCS* pDeviceFuncs) {
  const uint64_t adapter_ptr = pCreateDevice ? d3d9_trace_arg_ptr(pCreateDevice->hAdapter.pDrvPrivate) : 0;
  const uint64_t flags = pCreateDevice ? static_cast<uint64_t>(pCreateDevice->Flags) : 0;
  D3d9TraceCall trace(
      D3d9TraceFunc::AdapterCreateDevice, adapter_ptr, flags, d3d9_trace_arg_ptr(pDeviceFuncs), d3d9_trace_arg_ptr(pCreateDevice));
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  if (!pCreateDevice || !pDeviceFuncs) {
    return trace.ret(E_INVALIDARG);
  }

  auto* adapter = as_adapter(pCreateDevice->hAdapter);
  if (!adapter) {
    return trace.ret(E_INVALIDARG);
  }

  auto dev = make_unique_nothrow<Device>(adapter);
  if (!dev) {
    return trace.ret(E_OUTOFMEMORY);
  }

  // Publish the device handle early so the runtime has a valid cookie for any
  // follow-up DDIs (including error paths).
  pCreateDevice->hDevice.pDrvPrivate = dev.get();

  if (!pCreateDevice->pCallbacks) {
    aerogpu::logf("aerogpu-d3d9: CreateDevice missing device callbacks\n");
    pCreateDevice->hDevice.pDrvPrivate = nullptr;
    return trace.ret(E_INVALIDARG);
  }

  dev->wddm_callbacks = *pCreateDevice->pCallbacks;

  {
    static std::once_flag wddm_cb_once;
    Device* dev_ptr = dev.get();
    std::call_once(wddm_cb_once, [dev_ptr] {
      const void* submit_cb = nullptr;
      const void* render_cb = nullptr;
      const void* present_cb = nullptr;
      bool submit_cb_can_present = false;
      bool render_cb_can_present = false;
      if constexpr (has_pfnSubmitCommandCb<WddmDeviceCallbacks>::value) {
        submit_cb = reinterpret_cast<const void*>(dev_ptr->wddm_callbacks.pfnSubmitCommandCb);
        using SubmitCbT = decltype(dev_ptr->wddm_callbacks.pfnSubmitCommandCb);
        submit_cb_can_present = submit_callback_can_signal_present<SubmitCbT>();
      }
      if constexpr (has_pfnRenderCb<WddmDeviceCallbacks>::value) {
        render_cb = reinterpret_cast<const void*>(dev_ptr->wddm_callbacks.pfnRenderCb);
        using RenderCbT = decltype(dev_ptr->wddm_callbacks.pfnRenderCb);
        render_cb_can_present = submit_callback_can_signal_present<RenderCbT>();
      }
      if constexpr (has_pfnPresentCb<WddmDeviceCallbacks>::value) {
        present_cb = reinterpret_cast<const void*>(dev_ptr->wddm_callbacks.pfnPresentCb);
      }
      aerogpu::logf("aerogpu-d3d9: WDDM callbacks SubmitCommandCb=%p RenderCb=%p PresentCb=%p\n",
                    submit_cb, render_cb, present_cb);
      if (submit_cb) {
        aerogpu::logf("aerogpu-d3d9: SubmitCommandCb can_signal_present=%u\n", submit_cb_can_present ? 1u : 0u);
      }
      if (render_cb) {
        aerogpu::logf("aerogpu-d3d9: RenderCb can_signal_present=%u\n", render_cb_can_present ? 1u : 0u);
      }
    });
  }

  HRESULT hr = wddm_create_device(dev->wddm_callbacks, adapter, &dev->wddm_device);
  if (FAILED(hr)) {
    aerogpu::logf("aerogpu-d3d9: CreateDeviceCb failed hr=0x%08x\n", static_cast<unsigned>(hr));
    pCreateDevice->hDevice.pDrvPrivate = nullptr;
    return trace.ret(hr);
  }

  hr = wddm_create_context(dev->wddm_callbacks, dev->wddm_device, &dev->wddm_context);
  if (FAILED(hr)) {
    aerogpu::logf("aerogpu-d3d9: CreateContextCb failed hr=0x%08x\n", static_cast<unsigned>(hr));
    wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
    dev->wddm_device = 0;
    pCreateDevice->hDevice.pDrvPrivate = nullptr;
    return trace.ret(hr);
  }

  // Some Win7-era header/runtime combinations may omit
  // `DmaBufferPrivateDataSize` even when providing `pDmaBufferPrivateData`. The
  // AeroGPU Win7 KMD expects the private-data blob to be present, and dxgkrnl
  // only forwards it when the size is non-zero.
  if (dev->wddm_context.pDmaBufferPrivateData && dev->wddm_context.DmaBufferPrivateDataSize == 0) {
    dev->wddm_context.DmaBufferPrivateDataSize = static_cast<uint32_t>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES);
  }

  // If the adapter wasn't opened through a path that initialized our KMD query
  // helper (e.g. missing HDC at OpenAdapter time), opportunistically initialize
  // it here. This enables fence polling when hSyncObject is absent/zero.
  if (!adapter->kmd_query_available.load(std::memory_order_acquire)) {
    bool kmd_ok = false;
    if (adapter->luid.LowPart != 0 || adapter->luid.HighPart != 0) {
      kmd_ok = adapter->kmd_query.InitFromLuid(adapter->luid);
    }
    if (!kmd_ok) {
      HDC hdc = GetDC(nullptr);
      if (hdc) {
        kmd_ok = adapter->kmd_query.InitFromHdc(hdc);
        ReleaseDC(nullptr, hdc);
      }
    }
    adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
  }

  // Populate best-effort adapter state that is normally discovered during
  // OpenAdapter* when the KMD query helper is initialized. Some runtimes can
  // reach CreateDevice without those paths having run (or without a usable HDC),
  // so refresh the values here once we have a working query channel.
  if (adapter->kmd_query_available.load(std::memory_order_acquire)) {
    if (!adapter->vid_pn_source_id_valid) {
      uint32_t vid_pn_source_id = 0;
      if (adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
        adapter->vid_pn_source_id = vid_pn_source_id;
        adapter->vid_pn_source_id_valid = true;
      }
    }

    if (!adapter->max_allocation_list_slot_id_logged.load(std::memory_order_acquire)) {
      uint32_t max_slot_id = 0;
      if (adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
        adapter->max_allocation_list_slot_id = max_slot_id;
        if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
          aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                        static_cast<unsigned>(max_slot_id));
        }
      }
    }

    if (!adapter->umd_private_valid) {
      aerogpu_umd_private_v1 priv;
      std::memset(&priv, 0, sizeof(priv));
      if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
        adapter->umd_private = priv;
        adapter->umd_private_valid = true;

        char magicStr[5] = {0, 0, 0, 0, 0};
        magicStr[0] = static_cast<char>((priv.device_mmio_magic >> 0) & 0xFF);
        magicStr[1] = static_cast<char>((priv.device_mmio_magic >> 8) & 0xFF);
        magicStr[2] = static_cast<char>((priv.device_mmio_magic >> 16) & 0xFF);
        magicStr[3] = static_cast<char>((priv.device_mmio_magic >> 24) & 0xFF);

        aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                      priv.device_mmio_magic,
                      magicStr,
                      priv.device_abi_version_u32,
                      static_cast<unsigned long long>(priv.device_features),
                      priv.flags);
      }
    }
  }

  // Determine whether CreateContext returned a usable persistent DMA buffer /
  // allocation list. If not, fall back to Allocate/GetCommandBuffer.
  const uint32_t min_cmd_buffer_size = static_cast<uint32_t>(
      sizeof(aerogpu_cmd_stream_header) + align_up(sizeof(aerogpu_cmd_set_render_targets), 4));
  const bool create_context_has_persistent_submit_buffers =
      dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= min_cmd_buffer_size &&
      dev->wddm_context.pAllocationList &&
      dev->wddm_context.AllocationListSize != 0 &&
      dev->wddm_context.pDmaBufferPrivateData &&
      dev->wddm_context.DmaBufferPrivateDataSize >= AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES;

  if (!create_context_has_persistent_submit_buffers) {
    aerogpu::logf("aerogpu-d3d9: CreateContext did not provide persistent submit buffers; "
                  "will use Allocate/GetCommandBuffer (dma=%p cmd=%p size=%u alloc=%p entries=%u patch=%p entries=%u dma_priv=%p bytes=%u)\n",
                  dev->wddm_context.pDmaBuffer,
                  dev->wddm_context.pCommandBuffer,
                  static_cast<unsigned>(dev->wddm_context.CommandBufferSize),
                  dev->wddm_context.pAllocationList,
                  static_cast<unsigned>(dev->wddm_context.AllocationListSize),
                  dev->wddm_context.pPatchLocationList,
                  static_cast<unsigned>(dev->wddm_context.PatchLocationListSize),
                  dev->wddm_context.pDmaBufferPrivateData,
                  static_cast<unsigned>(dev->wddm_context.DmaBufferPrivateDataSize));

    bool have_submit_cb = false;
    if constexpr (has_pfnSubmitCommandCb<WddmDeviceCallbacks>::value) {
      have_submit_cb = have_submit_cb || (dev->wddm_callbacks.pfnSubmitCommandCb != nullptr);
    }
    if constexpr (has_pfnRenderCb<WddmDeviceCallbacks>::value) {
      have_submit_cb = have_submit_cb || (dev->wddm_callbacks.pfnRenderCb != nullptr);
    }
    if constexpr (has_pfnPresentCb<WddmDeviceCallbacks>::value) {
      have_submit_cb = have_submit_cb || (dev->wddm_callbacks.pfnPresentCb != nullptr);
    }

    bool have_acquire_cb = false;
    if constexpr (has_pfnAllocateCb<WddmDeviceCallbacks>::value && has_pfnDeallocateCb<WddmDeviceCallbacks>::value) {
      have_acquire_cb = have_acquire_cb || (dev->wddm_callbacks.pfnAllocateCb != nullptr && dev->wddm_callbacks.pfnDeallocateCb != nullptr);
    }
    if constexpr (has_pfnGetCommandBufferCb<WddmDeviceCallbacks>::value) {
      have_acquire_cb = have_acquire_cb || (dev->wddm_callbacks.pfnGetCommandBufferCb != nullptr);
    }

    if (!have_submit_cb || !have_acquire_cb) {
      aerogpu::logf("aerogpu-d3d9: WDDM callbacks do not support submission without persistent buffers "
                    "(submit=%s acquire=%s)\n",
                    have_submit_cb ? "ok" : "missing",
                    have_acquire_cb ? "ok" : "missing");
      dev->wddm_context.destroy(dev->wddm_callbacks);
      wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
      dev->wddm_device = 0;
      pCreateDevice->hDevice.pDrvPrivate = nullptr;
      return trace.ret(E_FAIL);
    }
  }

  {
    static std::once_flag wddm_diag_once;
    const bool patch_list_present =
        dev->wddm_context.pPatchLocationList && dev->wddm_context.PatchLocationListSize != 0;

    const bool has_sync_object = (dev->wddm_context.hSyncObject != 0);
    const bool kmd_query_available = adapter->kmd_query_available.load(std::memory_order_acquire);
    AerogpuNtStatus sync_probe = kStatusNotSupported;
    if (has_sync_object) {
      sync_probe = static_cast<AerogpuNtStatus>(
          adapter->kmd_query.WaitForSyncObject(static_cast<uint32_t>(dev->wddm_context.hSyncObject),
                                               /*fence_value=*/1,
                                               /*timeout_ms=*/0));
    }
    const bool sync_object_wait_available =
        has_sync_object && (sync_probe == kStatusSuccess || sync_probe == kStatusTimeout);

    // `wait_for_fence()` uses different mechanisms depending on whether the caller
    // is doing a bounded wait (PresentEx throttling) or a non-blocking poll (EVENT
    // queries / GetData). Log both to make bring-up debugging on Win7 clearer.
    const char* bounded_wait_mode = "polling";
    if (sync_object_wait_available) {
      bounded_wait_mode = "sync_object";
    } else if (kmd_query_available) {
      bounded_wait_mode = "kmd_query";
    }

    const char* poll_wait_mode = "polling";
    if (kmd_query_available) {
      poll_wait_mode = "kmd_query";
    } else if (sync_object_wait_available) {
      poll_wait_mode = "sync_object";
    }

    std::call_once(wddm_diag_once,
                   [patch_list_present, bounded_wait_mode, poll_wait_mode, has_sync_object, kmd_query_available] {
      aerogpu::logf("aerogpu-d3d9: WDDM patch_list=%s (AeroGPU submits with NumPatchLocations=0)\n",
                    patch_list_present ? "present" : "absent");
      aerogpu::logf("aerogpu-d3d9: fence_wait bounded=%s poll=%s (hSyncObject=%s kmd_query=%s)\n",
                    bounded_wait_mode,
                    poll_wait_mode,
                    has_sync_object ? "present" : "absent",
                    kmd_query_available ? "available" : "unavailable");
    });
  }

  aerogpu::logf("aerogpu-d3d9: CreateDevice wddm_device=0x%08x hContext=0x%08x hSyncObject=0x%08x "
                "dma=%p cmd=%p bytes=%u alloc_list=%p entries=%u patch_list=%p entries=%u dma_priv=%p bytes=%u\n",
                static_cast<unsigned>(dev->wddm_device),
                static_cast<unsigned>(dev->wddm_context.hContext),
                static_cast<unsigned>(dev->wddm_context.hSyncObject),
                dev->wddm_context.pDmaBuffer,
                dev->wddm_context.pCommandBuffer,
                static_cast<unsigned>(dev->wddm_context.CommandBufferSize),
                dev->wddm_context.pAllocationList,
                static_cast<unsigned>(dev->wddm_context.AllocationListSize),
                dev->wddm_context.pPatchLocationList,
                static_cast<unsigned>(dev->wddm_context.PatchLocationListSize),
                dev->wddm_context.pDmaBufferPrivateData,
                static_cast<unsigned>(dev->wddm_context.DmaBufferPrivateDataSize));

  // Wire the command stream builder to the runtime-provided DMA buffer so all
  // command emission paths write directly into `pCommandBuffer` (no per-submit
  // std::vector allocations). This is a prerequisite for real Win7 D3D9UMDDI
  // submission plumbing.
  if (dev->wddm_context.pCommandBuffer &&
      dev->wddm_context.CommandBufferSize >= sizeof(aerogpu_cmd_stream_header)) {
    dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
  }

  // Bind the per-submit allocation list tracker to the runtime-provided list so
  // command emission paths can populate D3DDDI_ALLOCATIONLIST entries as
  // resources are referenced (no patch list).
  dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                 dev->wddm_context.AllocationListSize,
                                 adapter->max_allocation_list_slot_id);
 
  std::memset(pDeviceFuncs, 0, sizeof(*pDeviceFuncs));
 
  // Assign entrypoints through compiler-checked thunks so signature mismatches
  // are diagnosed at build time, and so C++ exceptions never escape into the
  // runtime.
  pDeviceFuncs->pfnDestroyDevice =
      aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnDestroyDevice), device_destroy>::thunk;
  pDeviceFuncs->pfnCreateResource =
      aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnCreateResource), device_create_resource>::thunk;
  if constexpr (aerogpu_has_member_pfnOpenResource<D3D9DDI_DEVICEFUNCS>::value) {
    pDeviceFuncs->pfnOpenResource =
        aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnOpenResource), device_open_resource>::thunk;
  }
  if constexpr (aerogpu_has_member_pfnOpenResource2<D3D9DDI_DEVICEFUNCS>::value) {
    pDeviceFuncs->pfnOpenResource2 =
        aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnOpenResource2), device_open_resource2>::thunk;
  }
  pDeviceFuncs->pfnDestroyResource =
      aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnDestroyResource), device_destroy_resource>::thunk;
  pDeviceFuncs->pfnLock = aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnLock), device_lock>::thunk;
  pDeviceFuncs->pfnUnlock = aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->pfnUnlock), device_unlock>::thunk;
 
  // Assign the remaining entrypoints through type-safe thunks so the compiler
  // enforces the WDK function pointer signatures.
#define AEROGPU_SET_D3D9DDI_FN(member, fn)                                                               \
  do {                                                                                                   \
    pDeviceFuncs->member = aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->member), fn>::thunk;            \
  } while (0)

  AEROGPU_SET_D3D9DDI_FN(pfnSetRenderTarget, device_set_render_target);
  AEROGPU_SET_D3D9DDI_FN(pfnSetDepthStencil, device_set_depth_stencil);
  AEROGPU_SET_D3D9DDI_FN(pfnSetViewport, device_set_viewport);
  AEROGPU_SET_D3D9DDI_FN(pfnSetScissorRect, device_set_scissor);
  AEROGPU_SET_D3D9DDI_FN(pfnSetTexture, device_set_texture);
  if constexpr (aerogpu_has_member_pfnSetTextureStageState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetTextureStageState,
        aerogpu_d3d9_impl_pfnSetTextureStageState<decltype(pDeviceFuncs->pfnSetTextureStageState)>::pfnSetTextureStageState);
  }
  AEROGPU_SET_D3D9DDI_FN(pfnSetSamplerState, device_set_sampler_state);
  AEROGPU_SET_D3D9DDI_FN(pfnSetRenderState, device_set_render_state);
  if constexpr (aerogpu_has_member_pfnSetMaterial<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetMaterial,
        aerogpu_d3d9_impl_pfnSetMaterial<decltype(pDeviceFuncs->pfnSetMaterial)>::pfnSetMaterial);
  }
  if constexpr (aerogpu_has_member_pfnSetLight<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetLight, aerogpu_d3d9_impl_pfnSetLight<decltype(pDeviceFuncs->pfnSetLight)>::pfnSetLight);
  }
  if constexpr (aerogpu_has_member_pfnLightEnable<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnLightEnable,
        aerogpu_d3d9_impl_pfnLightEnable<decltype(pDeviceFuncs->pfnLightEnable)>::pfnLightEnable);
  }
  if constexpr (aerogpu_has_member_pfnSetNPatchMode<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetNPatchMode,
        aerogpu_d3d9_impl_pfnSetNPatchMode<decltype(pDeviceFuncs->pfnSetNPatchMode)>::pfnSetNPatchMode);
  }
  if constexpr (aerogpu_has_member_pfnSetGammaRamp<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetGammaRamp,
        aerogpu_d3d9_impl_pfnSetGammaRamp<decltype(pDeviceFuncs->pfnSetGammaRamp)>::pfnSetGammaRamp);
  }
  if constexpr (aerogpu_has_member_pfnSetTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetTransform,
        aerogpu_d3d9_impl_pfnSetTransform<decltype(pDeviceFuncs->pfnSetTransform)>::pfnSetTransform);
  }
  if constexpr (aerogpu_has_member_pfnMultiplyTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnMultiplyTransform,
        aerogpu_d3d9_impl_pfnMultiplyTransform<decltype(pDeviceFuncs->pfnMultiplyTransform)>::pfnMultiplyTransform);
  }
  if constexpr (aerogpu_has_member_pfnSetClipPlane<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetClipPlane,
        aerogpu_d3d9_impl_pfnSetClipPlane<decltype(pDeviceFuncs->pfnSetClipPlane)>::pfnSetClipPlane);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnCreateVertexDecl, device_create_vertex_decl);
  AEROGPU_SET_D3D9DDI_FN(pfnSetVertexDecl, device_set_vertex_decl);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyVertexDecl, device_destroy_vertex_decl);
  if constexpr (aerogpu_has_member_pfnSetFVF<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetFVF, device_set_fvf);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnCreateShader, device_create_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShader, device_set_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyShader, device_destroy_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShaderConstF, device_set_shader_const_f);
  if constexpr (aerogpu_has_member_pfnSetShaderConstI<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetShaderConstI,
        aerogpu_d3d9_impl_pfnSetShaderConstI<decltype(pDeviceFuncs->pfnSetShaderConstI)>::pfnSetShaderConstI);
  }
  if constexpr (aerogpu_has_member_pfnSetShaderConstB<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetShaderConstB,
        aerogpu_d3d9_impl_pfnSetShaderConstB<decltype(pDeviceFuncs->pfnSetShaderConstB)>::pfnSetShaderConstB);
  }

  if constexpr (aerogpu_has_member_pfnCreateStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnCreateStateBlock,
        aerogpu_d3d9_impl_pfnCreateStateBlock<decltype(pDeviceFuncs->pfnCreateStateBlock)>::pfnCreateStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnDeleteStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnDeleteStateBlock,
        aerogpu_d3d9_impl_pfnDeleteStateBlock<decltype(pDeviceFuncs->pfnDeleteStateBlock)>::pfnDeleteStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnCaptureStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnCaptureStateBlock,
        aerogpu_d3d9_impl_pfnCaptureStateBlock<decltype(pDeviceFuncs->pfnCaptureStateBlock)>::pfnCaptureStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnApplyStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnApplyStateBlock,
        aerogpu_d3d9_impl_pfnApplyStateBlock<decltype(pDeviceFuncs->pfnApplyStateBlock)>::pfnApplyStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnValidateDevice<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnValidateDevice,
        aerogpu_d3d9_impl_pfnValidateDevice<decltype(pDeviceFuncs->pfnValidateDevice)>::pfnValidateDevice);
  }
  if constexpr (aerogpu_has_member_pfnSetSoftwareVertexProcessing<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetSoftwareVertexProcessing,
        aerogpu_d3d9_impl_pfnSetSoftwareVertexProcessing<decltype(
            pDeviceFuncs->pfnSetSoftwareVertexProcessing)>::pfnSetSoftwareVertexProcessing);
  }
  if constexpr (aerogpu_has_member_pfnSetCursorProperties<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetCursorProperties,
        aerogpu_d3d9_impl_pfnSetCursorProperties<decltype(pDeviceFuncs->pfnSetCursorProperties)>::pfnSetCursorProperties);
  }
  if constexpr (aerogpu_has_member_pfnSetCursorPosition<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetCursorPosition,
        aerogpu_d3d9_impl_pfnSetCursorPosition<decltype(pDeviceFuncs->pfnSetCursorPosition)>::pfnSetCursorPosition);
  }
  if constexpr (aerogpu_has_member_pfnShowCursor<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnShowCursor,
        aerogpu_d3d9_impl_pfnShowCursor<decltype(pDeviceFuncs->pfnShowCursor)>::pfnShowCursor);
  }
  if constexpr (aerogpu_has_member_pfnSetPaletteEntries<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetPaletteEntries,
        aerogpu_d3d9_impl_pfnSetPaletteEntries<decltype(pDeviceFuncs->pfnSetPaletteEntries)>::pfnSetPaletteEntries);
  }
  if constexpr (aerogpu_has_member_pfnSetCurrentTexturePalette<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetCurrentTexturePalette,
                           aerogpu_d3d9_impl_pfnSetCurrentTexturePalette<decltype(
                               pDeviceFuncs->pfnSetCurrentTexturePalette)>::pfnSetCurrentTexturePalette);
  }
  if constexpr (aerogpu_has_member_pfnSetClipStatus<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetClipStatus,
        aerogpu_d3d9_impl_pfnSetClipStatus<decltype(pDeviceFuncs->pfnSetClipStatus)>::pfnSetClipStatus);
  }
  if constexpr (aerogpu_has_member_pfnGetClipStatus<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetClipStatus,
        aerogpu_d3d9_impl_pfnGetClipStatus<decltype(pDeviceFuncs->pfnGetClipStatus)>::pfnGetClipStatus);
  }
  if constexpr (aerogpu_has_member_pfnGetGammaRamp<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetGammaRamp,
        aerogpu_d3d9_impl_pfnGetGammaRamp<decltype(pDeviceFuncs->pfnGetGammaRamp)>::pfnGetGammaRamp);
  }
  if constexpr (aerogpu_has_member_pfnDrawRectPatch<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawRectPatch, device_draw_rect_patch);
  }
  if constexpr (aerogpu_has_member_pfnDrawTriPatch<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawTriPatch, device_draw_tri_patch);
  }
  if constexpr (aerogpu_has_member_pfnDeletePatch<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDeletePatch, device_delete_patch);
  }
  if constexpr (aerogpu_has_member_pfnProcessVertices<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnProcessVertices, device_process_vertices);
  }
  if constexpr (aerogpu_has_member_pfnGetRasterStatus<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetRasterStatus,
        aerogpu_d3d9_impl_pfnGetRasterStatus<decltype(pDeviceFuncs->pfnGetRasterStatus)>::pfnGetRasterStatus);
  }
  if constexpr (aerogpu_has_member_pfnSetDialogBoxMode<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetDialogBoxMode,
        aerogpu_d3d9_noop_pfnSetDialogBoxMode<decltype(pDeviceFuncs->pfnSetDialogBoxMode)>::pfnSetDialogBoxMode);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnSetStreamSource, device_set_stream_source);
  if constexpr (aerogpu_has_member_pfnSetStreamSourceFreq<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetStreamSourceFreq,
                           aerogpu_d3d9_impl_pfnSetStreamSourceFreq<decltype(
                               pDeviceFuncs->pfnSetStreamSourceFreq)>::pfnSetStreamSourceFreq);
  }
  AEROGPU_SET_D3D9DDI_FN(pfnSetIndices, device_set_indices);
  if constexpr (aerogpu_has_member_pfnBeginScene<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnBeginScene, device_begin_scene);
  }
  if constexpr (aerogpu_has_member_pfnEndScene<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnEndScene, device_end_scene);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnClear, device_clear);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitive, device_draw_primitive);
  if constexpr (aerogpu_has_member_pfnDrawPrimitiveUP<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitiveUP, device_draw_primitive_up);
  }
  if constexpr (aerogpu_has_member_pfnDrawIndexedPrimitiveUP<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawIndexedPrimitiveUP, device_draw_indexed_primitive_up);
  }
  AEROGPU_SET_D3D9DDI_FN(pfnDrawIndexedPrimitive, device_draw_indexed_primitive);
  if constexpr (aerogpu_has_member_pfnDrawPrimitive2<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitive2, device_draw_primitive2);
  }
  if constexpr (aerogpu_has_member_pfnDrawIndexedPrimitive2<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnDrawIndexedPrimitive2, device_draw_indexed_primitive2);
  }

  if constexpr (aerogpu_has_member_pfnGetSoftwareVertexProcessing<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetSoftwareVertexProcessing,
                           aerogpu_d3d9_impl_pfnGetSoftwareVertexProcessing<decltype(
                               pDeviceFuncs->pfnGetSoftwareVertexProcessing)>::pfnGetSoftwareVertexProcessing);
  }
  if constexpr (aerogpu_has_member_pfnGetTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetTransform,
                           aerogpu_d3d9_impl_pfnGetTransform<decltype(pDeviceFuncs->pfnGetTransform)>::pfnGetTransform);
  }
  if constexpr (aerogpu_has_member_pfnGetClipPlane<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetClipPlane,
                           aerogpu_d3d9_impl_pfnGetClipPlane<decltype(pDeviceFuncs->pfnGetClipPlane)>::pfnGetClipPlane);
  }
  if constexpr (aerogpu_has_member_pfnGetViewport<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetViewport,
        aerogpu_d3d9_impl_pfnGetViewport<decltype(pDeviceFuncs->pfnGetViewport)>::pfnGetViewport);
  }
  if constexpr (aerogpu_has_member_pfnGetScissorRect<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetScissorRect,
        aerogpu_d3d9_impl_pfnGetScissorRect<decltype(pDeviceFuncs->pfnGetScissorRect)>::pfnGetScissorRect);
  }
  if constexpr (aerogpu_has_member_pfnBeginStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnBeginStateBlock,
        aerogpu_d3d9_impl_pfnBeginStateBlock<decltype(pDeviceFuncs->pfnBeginStateBlock)>::pfnBeginStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnEndStateBlock<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnEndStateBlock,
        aerogpu_d3d9_impl_pfnEndStateBlock<decltype(pDeviceFuncs->pfnEndStateBlock)>::pfnEndStateBlock);
  }
  if constexpr (aerogpu_has_member_pfnGetMaterial<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetMaterial,
        aerogpu_d3d9_impl_pfnGetMaterial<decltype(pDeviceFuncs->pfnGetMaterial)>::pfnGetMaterial);
  }
  if constexpr (aerogpu_has_member_pfnGetLight<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetLight, aerogpu_d3d9_impl_pfnGetLight<decltype(pDeviceFuncs->pfnGetLight)>::pfnGetLight);
  }
  if constexpr (aerogpu_has_member_pfnGetLightEnable<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetLightEnable,
                           aerogpu_d3d9_impl_pfnGetLightEnable<decltype(
                               pDeviceFuncs->pfnGetLightEnable)>::pfnGetLightEnable);
  }
  if constexpr (aerogpu_has_member_pfnGetRenderTarget<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetRenderTarget,
        aerogpu_d3d9_impl_pfnGetRenderTarget<decltype(pDeviceFuncs->pfnGetRenderTarget)>::pfnGetRenderTarget);
  }
  if constexpr (aerogpu_has_member_pfnGetDepthStencil<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetDepthStencil,
        aerogpu_d3d9_impl_pfnGetDepthStencil<decltype(pDeviceFuncs->pfnGetDepthStencil)>::pfnGetDepthStencil);
  }
  if constexpr (aerogpu_has_member_pfnGetTexture<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetTexture,
        aerogpu_d3d9_impl_pfnGetTexture<decltype(pDeviceFuncs->pfnGetTexture)>::pfnGetTexture);
  }
  if constexpr (aerogpu_has_member_pfnGetTextureStageState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetTextureStageState,
                           aerogpu_d3d9_impl_pfnGetTextureStageState<decltype(
                               pDeviceFuncs->pfnGetTextureStageState)>::pfnGetTextureStageState);
  }
  if constexpr (aerogpu_has_member_pfnGetSamplerState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetSamplerState,
        aerogpu_d3d9_impl_pfnGetSamplerState<decltype(pDeviceFuncs->pfnGetSamplerState)>::pfnGetSamplerState);
  }
  if constexpr (aerogpu_has_member_pfnGetRenderState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetRenderState,
        aerogpu_d3d9_impl_pfnGetRenderState<decltype(pDeviceFuncs->pfnGetRenderState)>::pfnGetRenderState);
  }
  if constexpr (aerogpu_has_member_pfnGetPaletteEntries<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetPaletteEntries,
                           aerogpu_d3d9_impl_pfnGetPaletteEntries<decltype(
                               pDeviceFuncs->pfnGetPaletteEntries)>::pfnGetPaletteEntries);
  }
  if constexpr (aerogpu_has_member_pfnGetCurrentTexturePalette<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetCurrentTexturePalette,
                           aerogpu_d3d9_impl_pfnGetCurrentTexturePalette<decltype(
                               pDeviceFuncs->pfnGetCurrentTexturePalette)>::pfnGetCurrentTexturePalette);
  }
  if constexpr (aerogpu_has_member_pfnGetNPatchMode<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetNPatchMode,
                           aerogpu_d3d9_impl_pfnGetNPatchMode<decltype(pDeviceFuncs->pfnGetNPatchMode)>::pfnGetNPatchMode);
  }
  if constexpr (aerogpu_has_member_pfnGetFVF<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetFVF,
        aerogpu_d3d9_impl_pfnGetFVF<decltype(pDeviceFuncs->pfnGetFVF)>::pfnGetFVF);
  }
  if constexpr (aerogpu_has_member_pfnGetVertexDecl<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetVertexDecl,
        aerogpu_d3d9_impl_pfnGetVertexDecl<decltype(pDeviceFuncs->pfnGetVertexDecl)>::pfnGetVertexDecl);
  }
  if constexpr (aerogpu_has_member_pfnGetStreamSource<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetStreamSource,
        aerogpu_d3d9_impl_pfnGetStreamSource<decltype(pDeviceFuncs->pfnGetStreamSource)>::pfnGetStreamSource);
  }
  if constexpr (aerogpu_has_member_pfnGetStreamSourceFreq<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetStreamSourceFreq,
                           aerogpu_d3d9_impl_pfnGetStreamSourceFreq<decltype(
                               pDeviceFuncs->pfnGetStreamSourceFreq)>::pfnGetStreamSourceFreq);
  }
  if constexpr (aerogpu_has_member_pfnGetIndices<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetIndices,
        aerogpu_d3d9_impl_pfnGetIndices<decltype(pDeviceFuncs->pfnGetIndices)>::pfnGetIndices);
  }
  if constexpr (aerogpu_has_member_pfnGetShader<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetShader,
        aerogpu_d3d9_impl_pfnGetShader<decltype(pDeviceFuncs->pfnGetShader)>::pfnGetShader);
  }
  if constexpr (aerogpu_has_member_pfnGetShaderConstF<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetShaderConstF,
        aerogpu_d3d9_impl_pfnGetShaderConstF<decltype(pDeviceFuncs->pfnGetShaderConstF)>::pfnGetShaderConstF);
  }
  if constexpr (aerogpu_has_member_pfnGetShaderConstI<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetShaderConstI,
                           aerogpu_d3d9_impl_pfnGetShaderConstI<decltype(
                               pDeviceFuncs->pfnGetShaderConstI)>::pfnGetShaderConstI);
  }
  if constexpr (aerogpu_has_member_pfnGetShaderConstB<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetShaderConstB,
                           aerogpu_d3d9_impl_pfnGetShaderConstB<decltype(
                               pDeviceFuncs->pfnGetShaderConstB)>::pfnGetShaderConstB);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnCreateSwapChain, device_create_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroySwapChain, device_destroy_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnGetSwapChain, device_get_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnSetSwapChain, device_set_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnReset, device_reset);
  AEROGPU_SET_D3D9DDI_FN(pfnResetEx, device_reset_ex);
  AEROGPU_SET_D3D9DDI_FN(pfnCheckDeviceState, device_check_device_state);

  if constexpr (aerogpu_has_member_pfnWaitForVBlank<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnWaitForVBlank, device_wait_for_vblank);
  }
  if constexpr (aerogpu_has_member_pfnSetGPUThreadPriority<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetGPUThreadPriority, device_set_gpu_thread_priority);
  }
  if constexpr (aerogpu_has_member_pfnGetGPUThreadPriority<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetGPUThreadPriority, device_get_gpu_thread_priority);
  }
  if constexpr (aerogpu_has_member_pfnCheckResourceResidency<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnCheckResourceResidency, device_check_resource_residency);
  }
  if constexpr (aerogpu_has_member_pfnQueryResourceResidency<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnQueryResourceResidency, device_query_resource_residency);
  }
  if constexpr (aerogpu_has_member_pfnSetPriority<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetPriority,
                           aerogpu_d3d9_impl_pfnSetPriority<decltype(pDeviceFuncs->pfnSetPriority)>::pfnSetPriority);
  }
  if constexpr (aerogpu_has_member_pfnGetPriority<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetPriority,
                           aerogpu_d3d9_impl_pfnGetPriority<decltype(pDeviceFuncs->pfnGetPriority)>::pfnGetPriority);
  }
  if constexpr (aerogpu_has_member_pfnGetDisplayModeEx<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetDisplayModeEx, device_get_display_mode_ex);
  }
  if constexpr (aerogpu_has_member_pfnComposeRects<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnComposeRects, device_compose_rects);
  }
  if constexpr (aerogpu_has_member_pfnSetConvolutionMonoKernel<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetConvolutionMonoKernel,
        aerogpu_d3d9_noop_pfnSetConvolutionMonoKernel<decltype(
            pDeviceFuncs->pfnSetConvolutionMonoKernel)>::pfnSetConvolutionMonoKernel);
  }
  if constexpr (aerogpu_has_member_pfnSetAutoGenFilterType<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnSetAutoGenFilterType,
        aerogpu_d3d9_impl_pfnSetAutoGenFilterType<decltype(
            pDeviceFuncs->pfnSetAutoGenFilterType)>::pfnSetAutoGenFilterType);
  }
  if constexpr (aerogpu_has_member_pfnGetAutoGenFilterType<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGetAutoGenFilterType,
        aerogpu_d3d9_impl_pfnGetAutoGenFilterType<decltype(
            pDeviceFuncs->pfnGetAutoGenFilterType)>::pfnGetAutoGenFilterType);
  }
  if constexpr (aerogpu_has_member_pfnGenerateMipSubLevels<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(
        pfnGenerateMipSubLevels,
        aerogpu_d3d9_impl_pfnGenerateMipSubLevels<decltype(
            pDeviceFuncs->pfnGenerateMipSubLevels)>::pfnGenerateMipSubLevels);
  }

  AEROGPU_SET_D3D9DDI_FN(pfnRotateResourceIdentities, device_rotate_resource_identities);
  AEROGPU_SET_D3D9DDI_FN(pfnPresent, device_present);
  AEROGPU_SET_D3D9DDI_FN(pfnPresentEx, device_present_ex);
  AEROGPU_SET_D3D9DDI_FN(pfnFlush, device_flush);
  AEROGPU_SET_D3D9DDI_FN(pfnSetMaximumFrameLatency, device_set_maximum_frame_latency);
  AEROGPU_SET_D3D9DDI_FN(pfnGetMaximumFrameLatency, device_get_maximum_frame_latency);
  AEROGPU_SET_D3D9DDI_FN(pfnGetPresentStats, device_get_present_stats);
  AEROGPU_SET_D3D9DDI_FN(pfnGetLastPresentCount, device_get_last_present_count);

  AEROGPU_SET_D3D9DDI_FN(pfnCreateQuery, device_create_query);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyQuery, device_destroy_query);
  AEROGPU_SET_D3D9DDI_FN(pfnIssueQuery, device_issue_query);
  AEROGPU_SET_D3D9DDI_FN(pfnGetQueryData, device_get_query_data);
  AEROGPU_SET_D3D9DDI_FN(pfnGetRenderTargetData, device_get_render_target_data);
  AEROGPU_SET_D3D9DDI_FN(pfnCopyRects, device_copy_rects);
  AEROGPU_SET_D3D9DDI_FN(pfnWaitForIdle, device_wait_for_idle);

  AEROGPU_SET_D3D9DDI_FN(pfnBlt, device_blt);
  AEROGPU_SET_D3D9DDI_FN(pfnColorFill, device_color_fill);
  AEROGPU_SET_D3D9DDI_FN(pfnUpdateSurface, device_update_surface);
  AEROGPU_SET_D3D9DDI_FN(pfnUpdateTexture, device_update_texture);

#undef AEROGPU_SET_D3D9DDI_FN

  if (!d3d9_validate_nonnull_vtable(pDeviceFuncs, "D3D9DDI_DEVICEFUNCS")) {
    // Be defensive: if we ever miss wiring a function table entry (new WDK
    // members, missed stubs), fail device creation cleanly rather than returning
    // a partially-populated vtable that would crash the runtime on first call.
    aerogpu::logf("aerogpu-d3d9: CreateDevice: device vtable contains NULL entrypoints; failing\n");
    dev->wddm_context.destroy(dev->wddm_callbacks);
    wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
    dev->wddm_device = 0;
    pCreateDevice->hDevice.pDrvPrivate = nullptr;
    return trace.ret(E_FAIL);
  }

  dev.release();
  return trace.ret(S_OK);
#else
  if (!pCreateDevice || !pDeviceFuncs) {
    return trace.ret(E_INVALIDARG);
  }
  auto* adapter = as_adapter(pCreateDevice->hAdapter);
  if (!adapter) {
    return trace.ret(E_INVALIDARG);
  }

  auto dev = make_unique_nothrow<Device>(adapter);
  if (!dev) {
    return trace.ret(E_OUTOFMEMORY);
  }
  pCreateDevice->hDevice.pDrvPrivate = dev.get();

#if defined(_WIN32)
  if (pCreateDevice->pCallbacks) {
    dev->wddm_callbacks = *pCreateDevice->pCallbacks;

    HRESULT hr = wddm_create_device(dev->wddm_callbacks, adapter, &dev->wddm_device);
    if (FAILED(hr)) {
      aerogpu::logf("aerogpu-d3d9: CreateDeviceCb failed hr=0x%08x (falling back to stub submission)\n", static_cast<unsigned>(hr));
      dev->wddm_callbacks = {};
      dev->wddm_device = 0;
    } else {
      hr = wddm_create_context(dev->wddm_callbacks, dev->wddm_device, &dev->wddm_context);
      if (FAILED(hr)) {
        aerogpu::logf("aerogpu-d3d9: CreateContextCb failed hr=0x%08x (falling back to stub submission)\n", static_cast<unsigned>(hr));
        wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
        dev->wddm_device = 0;
        dev->wddm_callbacks = {};
      } else {
        // If the adapter wasn't opened through a path that initialized our KMD query
        // helper (e.g. missing HDC at OpenAdapter time), opportunistically initialize
        // it here. This enables fence polling when hSyncObject is absent/zero.
        if (!adapter->kmd_query_available.load(std::memory_order_acquire)) {
          bool kmd_ok = false;
          if (adapter->luid.LowPart != 0 || adapter->luid.HighPart != 0) {
            kmd_ok = adapter->kmd_query.InitFromLuid(adapter->luid);
          }
          if (!kmd_ok) {
            HDC hdc = GetDC(nullptr);
            if (hdc) {
              kmd_ok = adapter->kmd_query.InitFromHdc(hdc);
              ReleaseDC(nullptr, hdc);
            }
          }
          adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
        }

        if (adapter->kmd_query_available.load(std::memory_order_acquire)) {
          if (!adapter->vid_pn_source_id_valid) {
            uint32_t vid_pn_source_id = 0;
            if (adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
              adapter->vid_pn_source_id = vid_pn_source_id;
              adapter->vid_pn_source_id_valid = true;
            }
          }

          if (!adapter->max_allocation_list_slot_id_logged.load(std::memory_order_acquire)) {
            uint32_t max_slot_id = 0;
            if (adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
              adapter->max_allocation_list_slot_id = max_slot_id;
              if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
                aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                              static_cast<unsigned>(max_slot_id));
              }
            }
          }

          if (!adapter->umd_private_valid) {
            aerogpu_umd_private_v1 priv;
            std::memset(&priv, 0, sizeof(priv));
            if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
              adapter->umd_private = priv;
              adapter->umd_private_valid = true;

              char magicStr[5] = {0, 0, 0, 0, 0};
              magicStr[0] = static_cast<char>((priv.device_mmio_magic >> 0) & 0xFF);
              magicStr[1] = static_cast<char>((priv.device_mmio_magic >> 8) & 0xFF);
              magicStr[2] = static_cast<char>((priv.device_mmio_magic >> 16) & 0xFF);
              magicStr[3] = static_cast<char>((priv.device_mmio_magic >> 24) & 0xFF);

              aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                            priv.device_mmio_magic,
                            magicStr,
                            priv.device_abi_version_u32,
                            static_cast<unsigned long long>(priv.device_features),
                            priv.flags);
            }
          }
        }

        // Validate the runtime-provided submission buffers. These must be present for
        // DMA buffer construction.
        const uint32_t min_cmd_buffer_size = static_cast<uint32_t>(
            sizeof(aerogpu_cmd_stream_header) + align_up(sizeof(aerogpu_cmd_set_render_targets), 4));
        if (!dev->wddm_context.pCommandBuffer ||
            dev->wddm_context.CommandBufferSize < min_cmd_buffer_size ||
            !dev->wddm_context.pAllocationList || dev->wddm_context.AllocationListSize == 0 ||
            !dev->wddm_context.pDmaBufferPrivateData ||
            dev->wddm_context.DmaBufferPrivateDataSize < AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES) {
          aerogpu::logf("aerogpu-d3d9: WDDM CreateContext returned invalid buffers "
                        "dma=%p cmd=%p size=%u alloc=%p size=%u patch=%p size=%u dma_priv=%p bytes=%u (need>=%u) sync=0x%08x\n",
                        dev->wddm_context.pDmaBuffer,
                        dev->wddm_context.pCommandBuffer,
                        static_cast<unsigned>(dev->wddm_context.CommandBufferSize),
                        dev->wddm_context.pAllocationList,
                        static_cast<unsigned>(dev->wddm_context.AllocationListSize),
                        dev->wddm_context.pPatchLocationList,
                        static_cast<unsigned>(dev->wddm_context.PatchLocationListSize),
                        dev->wddm_context.pDmaBufferPrivateData,
                        static_cast<unsigned>(dev->wddm_context.DmaBufferPrivateDataSize),
                        static_cast<unsigned>(AEROGPU_WIN7_DMA_BUFFER_PRIVATE_DATA_SIZE_BYTES),
                        static_cast<unsigned>(dev->wddm_context.hSyncObject));

          dev->wddm_context.destroy(dev->wddm_callbacks);
          wddm_destroy_device(dev->wddm_callbacks, dev->wddm_device);
          dev->wddm_device = 0;
          dev->wddm_callbacks = {};
        } else {
          {
            static std::once_flag wddm_diag_once;
            const bool patch_list_present =
                dev->wddm_context.pPatchLocationList && dev->wddm_context.PatchLocationListSize != 0;

            const bool has_sync_object = (dev->wddm_context.hSyncObject != 0);
            const bool kmd_query_available = adapter->kmd_query_available.load(std::memory_order_acquire);
            AerogpuNtStatus sync_probe = kStatusNotSupported;
            if (has_sync_object) {
              sync_probe = static_cast<AerogpuNtStatus>(
                  adapter->kmd_query.WaitForSyncObject(static_cast<uint32_t>(dev->wddm_context.hSyncObject),
                                                       /*fence_value=*/1,
                                                       /*timeout_ms=*/0));
            }
            const bool sync_object_wait_available =
                has_sync_object && (sync_probe == kStatusSuccess || sync_probe == kStatusTimeout);

            // `wait_for_fence()` prefers different mechanisms depending on whether
            // the caller is doing a bounded wait (PresentEx throttling) or a poll
            // (EVENT queries / GetData). Log both so bring-up can quickly confirm
            // which fallback is active on a given runtime/configuration.
            const char* bounded_wait_mode = "polling";
            if (sync_object_wait_available) {
              bounded_wait_mode = "sync_object";
            } else if (kmd_query_available) {
              bounded_wait_mode = "kmd_query";
            }

            const char* poll_wait_mode = "polling";
            if (kmd_query_available) {
              poll_wait_mode = "kmd_query";
            } else if (sync_object_wait_available) {
              poll_wait_mode = "sync_object";
            }

            std::call_once(wddm_diag_once,
                           [patch_list_present, bounded_wait_mode, poll_wait_mode, has_sync_object, kmd_query_available] {
              aerogpu::logf("aerogpu-d3d9: WDDM patch_list=%s (AeroGPU submits with NumPatchLocations=0)\n",
                            patch_list_present ? "present" : "absent");
              aerogpu::logf("aerogpu-d3d9: fence_wait bounded=%s poll=%s (hSyncObject=%s kmd_query=%s)\n",
                            bounded_wait_mode,
                            poll_wait_mode,
                            has_sync_object ? "present" : "absent",
                            kmd_query_available ? "available" : "unavailable");
            });
          }

          aerogpu::logf("aerogpu-d3d9: CreateDevice wddm_device=0x%08x hContext=0x%08x hSyncObject=0x%08x "
                        "dma=%p cmd=%p bytes=%u alloc_list=%p entries=%u patch_list=%p entries=%u dma_priv=%p bytes=%u\n",
                        static_cast<unsigned>(dev->wddm_device),
                        static_cast<unsigned>(dev->wddm_context.hContext),
                        static_cast<unsigned>(dev->wddm_context.hSyncObject),
                        dev->wddm_context.pDmaBuffer,
                        dev->wddm_context.pCommandBuffer,
                        static_cast<unsigned>(dev->wddm_context.CommandBufferSize),
                        dev->wddm_context.pAllocationList,
                        static_cast<unsigned>(dev->wddm_context.AllocationListSize),
                        dev->wddm_context.pPatchLocationList,
                        static_cast<unsigned>(dev->wddm_context.PatchLocationListSize),
                        dev->wddm_context.pDmaBufferPrivateData,
                        static_cast<unsigned>(dev->wddm_context.DmaBufferPrivateDataSize));

          // Wire the command stream builder to the runtime-provided DMA buffer so all
          // command emission paths write directly into `pCommandBuffer` (no per-submit
          // std::vector allocations). This is a prerequisite for real Win7 D3D9UMDDI
          // submission plumbing.
          if (dev->wddm_context.pCommandBuffer &&
              dev->wddm_context.CommandBufferSize >= sizeof(aerogpu_cmd_stream_header)) {
            dev->cmd.set_span(dev->wddm_context.pCommandBuffer, dev->wddm_context.CommandBufferSize);
          }

          // Bind the per-submit allocation list tracker to the runtime-provided buffers
          // so allocation tracking works immediately (e.g. shared surface CreateResource
          // can reference its backing allocation before the first submit()).
          dev->alloc_list_tracker.rebind(reinterpret_cast<D3DDDI_ALLOCATIONLIST*>(dev->wddm_context.pAllocationList),
                                         dev->wddm_context.AllocationListSize,
                                         adapter->max_allocation_list_slot_id);
        }
      }
    }
  } else {
    static std::once_flag wddm_callbacks_missing_once;
    std::call_once(wddm_callbacks_missing_once, [] {
      aerogpu::logf("aerogpu-d3d9: CreateDevice missing WDDM callbacks; submissions will be stubbed\n");
    });
  }
#endif
 
  std::memset(pDeviceFuncs, 0, sizeof(*pDeviceFuncs));
#define AEROGPU_SET_D3D9DDI_FN(member, fn)                                                             \
  do {                                                                                                 \
    pDeviceFuncs->member = aerogpu_d3d9_ddi_thunk<decltype(pDeviceFuncs->member), fn>::thunk;           \
  } while (0)

  AEROGPU_SET_D3D9DDI_FN(pfnDestroyDevice, device_destroy);
  AEROGPU_SET_D3D9DDI_FN(pfnCreateResource, device_create_resource);
  AEROGPU_SET_D3D9DDI_FN(pfnOpenResource, device_open_resource);
  AEROGPU_SET_D3D9DDI_FN(pfnOpenResource2, device_open_resource2);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyResource, device_destroy_resource);
  AEROGPU_SET_D3D9DDI_FN(pfnLock, device_lock);
  AEROGPU_SET_D3D9DDI_FN(pfnUnlock, device_unlock);
 
  AEROGPU_SET_D3D9DDI_FN(pfnSetRenderTarget, device_set_render_target);
  AEROGPU_SET_D3D9DDI_FN(pfnSetDepthStencil, device_set_depth_stencil);
  AEROGPU_SET_D3D9DDI_FN(pfnSetViewport, device_set_viewport);
  AEROGPU_SET_D3D9DDI_FN(pfnSetScissorRect, device_set_scissor);
  AEROGPU_SET_D3D9DDI_FN(pfnSetTexture, device_set_texture);
  if constexpr (aerogpu_has_member_pfnSetTextureStageState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetTextureStageState, device_set_texture_stage_state);
  }
  if constexpr (aerogpu_has_member_pfnGetTextureStageState<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetTextureStageState, device_get_texture_stage_state);
  }
  AEROGPU_SET_D3D9DDI_FN(pfnSetSamplerState, device_set_sampler_state);
  AEROGPU_SET_D3D9DDI_FN(pfnSetRenderState, device_set_render_state);
  if constexpr (aerogpu_has_member_pfnSetTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetTransform, device_set_transform_portable);
  }
  if constexpr (aerogpu_has_member_pfnMultiplyTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnMultiplyTransform, device_multiply_transform_portable);
  }
  if constexpr (aerogpu_has_member_pfnGetTransform<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnGetTransform, device_get_transform_portable);
  }
  if constexpr (aerogpu_has_member_pfnSetCursorProperties<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetCursorProperties, device_set_cursor_properties);
  }
  if constexpr (aerogpu_has_member_pfnSetCursorPosition<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnSetCursorPosition, device_set_cursor_position);
  }
  if constexpr (aerogpu_has_member_pfnShowCursor<D3D9DDI_DEVICEFUNCS>::value) {
    AEROGPU_SET_D3D9DDI_FN(pfnShowCursor, device_show_cursor);
  }
 
  AEROGPU_SET_D3D9DDI_FN(pfnCreateVertexDecl, device_create_vertex_decl);
  AEROGPU_SET_D3D9DDI_FN(pfnSetVertexDecl, device_set_vertex_decl);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyVertexDecl, device_destroy_vertex_decl);
  AEROGPU_SET_D3D9DDI_FN(pfnSetFVF, device_set_fvf);
 
  AEROGPU_SET_D3D9DDI_FN(pfnCreateShader, device_create_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShader, device_set_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyShader, device_destroy_shader);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShaderConstF, device_set_shader_const_f);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShaderConstI, device_set_shader_const_i);
  AEROGPU_SET_D3D9DDI_FN(pfnSetShaderConstB, device_set_shader_const_b);
 
  AEROGPU_SET_D3D9DDI_FN(pfnSetStreamSource, device_set_stream_source);
  AEROGPU_SET_D3D9DDI_FN(pfnSetIndices, device_set_indices);
  AEROGPU_SET_D3D9DDI_FN(pfnProcessVertices, device_process_vertices);
  AEROGPU_SET_D3D9DDI_FN(pfnBeginScene, device_begin_scene);
  AEROGPU_SET_D3D9DDI_FN(pfnEndScene, device_end_scene);
 
  AEROGPU_SET_D3D9DDI_FN(pfnClear, device_clear);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitive, device_draw_primitive);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitiveUP, device_draw_primitive_up);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawIndexedPrimitive, device_draw_indexed_primitive);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawPrimitive2, device_draw_primitive2);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawIndexedPrimitive2, device_draw_indexed_primitive2);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawRectPatch, device_draw_rect_patch);
  AEROGPU_SET_D3D9DDI_FN(pfnDrawTriPatch, device_draw_tri_patch);
  AEROGPU_SET_D3D9DDI_FN(pfnDeletePatch, device_delete_patch);
  AEROGPU_SET_D3D9DDI_FN(pfnGenerateMipSubLevels, device_generate_mip_sub_levels);
  AEROGPU_SET_D3D9DDI_FN(pfnCreateStateBlock, device_create_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnDeleteStateBlock, device_delete_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnCaptureStateBlock, device_capture_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnApplyStateBlock, device_apply_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnBeginStateBlock, device_begin_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnEndStateBlock, device_end_state_block);
  AEROGPU_SET_D3D9DDI_FN(pfnCreateSwapChain, device_create_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroySwapChain, device_destroy_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnGetSwapChain, device_get_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnSetSwapChain, device_set_swap_chain);
  AEROGPU_SET_D3D9DDI_FN(pfnReset, device_reset);
  AEROGPU_SET_D3D9DDI_FN(pfnResetEx, device_reset_ex);
  AEROGPU_SET_D3D9DDI_FN(pfnCheckDeviceState, device_check_device_state);
  AEROGPU_SET_D3D9DDI_FN(pfnWaitForVBlank, device_wait_for_vblank);
  AEROGPU_SET_D3D9DDI_FN(pfnSetGPUThreadPriority, device_set_gpu_thread_priority);
  AEROGPU_SET_D3D9DDI_FN(pfnGetGPUThreadPriority, device_get_gpu_thread_priority);
  AEROGPU_SET_D3D9DDI_FN(pfnCheckResourceResidency, device_check_resource_residency);
  AEROGPU_SET_D3D9DDI_FN(pfnQueryResourceResidency, device_query_resource_residency);
  AEROGPU_SET_D3D9DDI_FN(pfnGetDisplayModeEx, device_get_display_mode_ex);
  AEROGPU_SET_D3D9DDI_FN(pfnComposeRects, device_compose_rects);
  AEROGPU_SET_D3D9DDI_FN(pfnRotateResourceIdentities, device_rotate_resource_identities);
  AEROGPU_SET_D3D9DDI_FN(pfnPresent, device_present);
  AEROGPU_SET_D3D9DDI_FN(pfnPresentEx, device_present_ex);
  AEROGPU_SET_D3D9DDI_FN(pfnFlush, device_flush);
  AEROGPU_SET_D3D9DDI_FN(pfnSetMaximumFrameLatency, device_set_maximum_frame_latency);
  AEROGPU_SET_D3D9DDI_FN(pfnGetMaximumFrameLatency, device_get_maximum_frame_latency);
  AEROGPU_SET_D3D9DDI_FN(pfnGetPresentStats, device_get_present_stats);
  AEROGPU_SET_D3D9DDI_FN(pfnGetLastPresentCount, device_get_last_present_count);
 
  AEROGPU_SET_D3D9DDI_FN(pfnCreateQuery, device_create_query);
  AEROGPU_SET_D3D9DDI_FN(pfnDestroyQuery, device_destroy_query);
  AEROGPU_SET_D3D9DDI_FN(pfnIssueQuery, device_issue_query);
  AEROGPU_SET_D3D9DDI_FN(pfnGetQueryData, device_get_query_data);
  AEROGPU_SET_D3D9DDI_FN(pfnGetRenderTargetData, device_get_render_target_data);
  AEROGPU_SET_D3D9DDI_FN(pfnCopyRects, device_copy_rects);
  AEROGPU_SET_D3D9DDI_FN(pfnWaitForIdle, device_wait_for_idle);
 
  AEROGPU_SET_D3D9DDI_FN(pfnBlt, device_blt);
  AEROGPU_SET_D3D9DDI_FN(pfnColorFill, device_color_fill);
  AEROGPU_SET_D3D9DDI_FN(pfnUpdateSurface, device_update_surface);
  AEROGPU_SET_D3D9DDI_FN(pfnUpdateTexture, device_update_texture);

#undef AEROGPU_SET_D3D9DDI_FN
  if (!d3d9_validate_nonnull_vtable(pDeviceFuncs, "D3D9DDI_DEVICEFUNCS")) {
    aerogpu::logf("aerogpu-d3d9: CreateDevice: device vtable contains NULL entrypoints; failing\n");
    pCreateDevice->hDevice.pDrvPrivate = nullptr;
    return trace.ret(E_FAIL);
  }

  dev.release();
  return trace.ret(S_OK);
#endif
}

HRESULT OpenAdapterCommon(const char* entrypoint,
                          UINT interface_version,
                          UINT umd_version,
                          D3DDDI_ADAPTERCALLBACKS* callbacks,
                          D3DDDI_ADAPTERCALLBACKS2* callbacks2,
                          const LUID& luid,
                          D3DDDI_HADAPTER* phAdapter,
                          D3D9DDI_ADAPTERFUNCS* pAdapterFuncs) {
  if (!entrypoint || !phAdapter || !pAdapterFuncs) {
    return E_INVALIDARG;
  }

#if defined(_WIN32)
  // Emit the exact DLL path once so bring-up on Win7 x64 can quickly confirm the
  // correct UMD bitness was loaded (System32 vs SysWOW64).
  // Best-effort: avoid letting `std::call_once` failures break adapter creation.
  try {
    static std::once_flag logged_module_path_once;
    std::call_once(logged_module_path_once, [] {
      HMODULE module = NULL;
      if (GetModuleHandleExA(GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS |
                                 GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT,
                             reinterpret_cast<LPCSTR>(&OpenAdapterCommon),
                             &module)) {
        char path[MAX_PATH] = {};
        if (GetModuleFileNameA(module, path, static_cast<DWORD>(sizeof(path))) != 0) {
          aerogpu::logf("aerogpu-d3d9: module_path=%s\n", path);
        }
      }
    });
  } catch (...) {
  }
#endif

  if (interface_version == 0 || umd_version == 0) {
    aerogpu::logf("aerogpu-d3d9: %s invalid interface/version (%u/%u)\n",
                  entrypoint,
                  static_cast<unsigned>(interface_version),
                  static_cast<unsigned>(umd_version));
    return E_INVALIDARG;
  }

#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  // The D3D runtime passes a D3D_UMD_INTERFACE_VERSION in the OpenAdapter args.
  // Be defensive: if the runtime asks for a newer interface than the headers we
  // are compiled against, fail cleanly rather than returning a vtable that does
  // not match what the runtime expects.
  if (interface_version > D3D_UMD_INTERFACE_VERSION) {
    aerogpu::logf("aerogpu-d3d9: %s unsupported interface_version=%u (compiled=%u)\n",
                  entrypoint,
                  static_cast<unsigned>(interface_version),
                  static_cast<unsigned>(D3D_UMD_INTERFACE_VERSION));
    return E_INVALIDARG;
  }
#endif

  Adapter* adapter = acquire_adapter(luid, interface_version, umd_version, callbacks, callbacks2);
  if (!adapter) {
    return E_OUTOFMEMORY;
  }

  phAdapter->pDrvPrivate = adapter;
 
  std::memset(pAdapterFuncs, 0, sizeof(*pAdapterFuncs));
#define AEROGPU_SET_D3D9DDI_ADAPTER_FN(member, fn)                                                    \
  do {                                                                                                 \
    pAdapterFuncs->member = aerogpu_d3d9_ddi_thunk<decltype(pAdapterFuncs->member), fn>::thunk;        \
  } while (0)

  AEROGPU_SET_D3D9DDI_ADAPTER_FN(pfnCloseAdapter, adapter_close);
  AEROGPU_SET_D3D9DDI_ADAPTER_FN(pfnGetCaps, adapter_get_caps);
  AEROGPU_SET_D3D9DDI_ADAPTER_FN(pfnCreateDevice, adapter_create_device);
  AEROGPU_SET_D3D9DDI_ADAPTER_FN(pfnQueryAdapterInfo, adapter_query_adapter_info);

#undef AEROGPU_SET_D3D9DDI_ADAPTER_FN
 
  if (!d3d9_validate_nonnull_vtable(pAdapterFuncs, "D3D9DDI_ADAPTERFUNCS")) {
    aerogpu::logf("aerogpu-d3d9: %s: adapter vtable contains NULL entrypoints; failing\n", entrypoint);
    phAdapter->pDrvPrivate = nullptr;
    release_adapter(adapter);
    return E_FAIL;
  }

  aerogpu::logf("aerogpu-d3d9: %s Interface=%u Version=%u LUID=%08x:%08x\n",
                entrypoint,
                static_cast<unsigned>(interface_version),
                static_cast<unsigned>(umd_version),
                static_cast<unsigned>(luid.HighPart),
                static_cast<unsigned>(luid.LowPart));
  return S_OK;
}

} // namespace

bool ensure_cmd_space_locked(Device* dev, size_t bytes_needed) {
  return ensure_cmd_space(dev, bytes_needed);
}

uint64_t submit_locked(Device* dev, bool is_present) {
  return submit(dev, is_present);
}

aerogpu_handle_t allocate_global_handle(Adapter* adapter) {
  if (!adapter) {
    return 0;
  }

#if defined(_WIN32)
  // Protocol object handles live in a single global namespace on the host (Win7
  // KMD currently submits context_id=0), so they must be unique across the
  // entire guest (multi-process, cross-API). Allocate them from a single
  // cross-process counter shared by all UMDs (D3D9 + D3D10/11).
  static std::mutex g_mutex;
  static HANDLE g_mapping = nullptr;
  static void* g_view = nullptr;

  std::lock_guard<std::mutex> lock(g_mutex);

  if (!g_view) {
    const wchar_t* name = L"Local\\AeroGPU.GlobalHandleCounter";

    // Use a permissive DACL so other processes in the session can open and
    // update the counter (e.g. DWM, sandboxed apps, different integrity levels).
    HANDLE mapping =
        win32::CreateFileMappingWBestEffortLowIntegrity(
            INVALID_HANDLE_VALUE, PAGE_READWRITE, 0, sizeof(uint64_t), name);
    if (mapping) {
      void* view = MapViewOfFile(mapping, FILE_MAP_ALL_ACCESS, 0, 0, sizeof(uint64_t));
      if (view) {
        g_mapping = mapping;
        g_view = view;
      } else {
        CloseHandle(mapping);
      }
    }
  }

  if (g_view) {
    auto* counter = reinterpret_cast<volatile LONG64*>(g_view);
    LONG64 token = InterlockedIncrement64(counter);
    if ((static_cast<uint64_t>(token) & 0x7FFFFFFFULL) == 0) {
      token = InterlockedIncrement64(counter);
    }
    aerogpu_handle_t handle = static_cast<aerogpu_handle_t>(static_cast<uint64_t>(token) & 0xFFFFFFFFu);
    if (handle == 0) {
      token = InterlockedIncrement64(counter);
      handle = static_cast<aerogpu_handle_t>(static_cast<uint64_t>(token) & 0xFFFFFFFFu);
    }
    return handle;
  }

  // If we fail to set up the shared counter mapping, fall back to a random
  // high-bit handle range so collisions with the shared counter (which starts
  // at 1) are vanishingly unlikely.
  static std::once_flag warn_once;
  std::call_once(warn_once, [] {
    logf("aerogpu-d3d9: global handle allocator: shared mapping unavailable; using RNG fallback\n");
  });

  for (;;) {
    const uint64_t token = adapter->share_token_allocator.allocate_share_token();
    const uint32_t low31 = static_cast<uint32_t>(token & 0x7FFFFFFFu);
    if (low31 != 0) {
      return static_cast<aerogpu_handle_t>(0x80000000u | low31);
    }
  }
#else
  aerogpu_handle_t handle = adapter->next_handle.fetch_add(1, std::memory_order_relaxed);
  if (handle == 0) {
    handle = adapter->next_handle.fetch_add(1, std::memory_order_relaxed);
  }
  return handle;
#endif
}

// -----------------------------------------------------------------------------
// Host-side test entrypoints
// -----------------------------------------------------------------------------
//
// Most D3D9 DDI entrypoints are kept in an anonymous namespace (internal
// linkage) since they are only referenced via function tables populated by
// OpenAdapter/CreateDevice on Windows. Host-side portable unit tests, however,
// instantiate `Device` directly (no runtime) and need to call a small subset of
// those DDIs directly.
//
// Provide thin wrappers with external linkage so tests can invoke key DDIs
// directly (draw calls + minimal pipeline setup) and validate command stream
// emission behavior.
namespace {

auto* const kDeviceSetFvfImpl = device_set_fvf;
auto* const kDeviceSetStreamSourceImpl = device_set_stream_source;
auto* const kDeviceSetIndicesImpl = device_set_indices;
auto* const kDeviceSetViewportImpl = device_set_viewport;
auto* const kDeviceCreateVertexDeclImpl = device_create_vertex_decl;
auto* const kDeviceSetVertexDeclImpl = device_set_vertex_decl;
auto* const kDeviceDestroyVertexDeclImpl = device_destroy_vertex_decl;
auto* const kDeviceCreateShaderImpl = device_create_shader;
auto* const kDeviceSetShaderImpl = device_set_shader;
auto* const kDeviceDestroyShaderImpl = device_destroy_shader;
auto* const kDeviceDrawPrimitiveImpl = device_draw_primitive;
auto* const kDeviceDrawIndexedPrimitiveImpl = device_draw_indexed_primitive;
auto* const kDeviceDrawPrimitiveUpImpl = device_draw_primitive_up;
auto* const kDeviceDrawIndexedPrimitiveUpImpl = device_draw_indexed_primitive_up;
auto* const kDeviceDrawPrimitive2Impl = device_draw_primitive2;
auto* const kDeviceDrawIndexedPrimitive2Impl = device_draw_indexed_primitive2;
auto* const kDeviceProcessVerticesImpl = device_process_vertices;

} // namespace

HRESULT AEROGPU_D3D9_CALL device_set_fvf(D3DDDI_HDEVICE hDevice, uint32_t fvf) {
  return kDeviceSetFvfImpl(hDevice, fvf);
}

HRESULT AEROGPU_D3D9_CALL device_create_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    const void* pDecl,
    uint32_t decl_size,
    D3D9DDI_HVERTEXDECL* phDecl) {
  return kDeviceCreateVertexDeclImpl(hDevice, pDecl, decl_size, phDecl);
}

HRESULT AEROGPU_D3D9_CALL device_set_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HVERTEXDECL hDecl) {
  return kDeviceSetVertexDeclImpl(hDevice, hDecl);
}

HRESULT AEROGPU_D3D9_CALL device_destroy_vertex_decl(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HVERTEXDECL hDecl) {
  return kDeviceDestroyVertexDeclImpl(hDevice, hDecl);
}

HRESULT AEROGPU_D3D9_CALL device_create_shader(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    const void* pBytecode,
    uint32_t bytecode_size,
    D3D9DDI_HSHADER* phShader) {
  return kDeviceCreateShaderImpl(hDevice, stage, pBytecode, bytecode_size, phShader);
}

HRESULT AEROGPU_D3D9_CALL device_set_shader(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    D3D9DDI_HSHADER hShader) {
  return kDeviceSetShaderImpl(hDevice, stage, hShader);
}

HRESULT AEROGPU_D3D9_CALL device_destroy_shader(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HSHADER hShader) {
  return kDeviceDestroyShaderImpl(hDevice, hShader);
}

HRESULT AEROGPU_D3D9_CALL device_set_stream_source(
    D3DDDI_HDEVICE hDevice,
    uint32_t stream,
    D3DDDI_HRESOURCE hVb,
    uint32_t offset_bytes,
    uint32_t stride_bytes) {
  return kDeviceSetStreamSourceImpl(hDevice, stream, hVb, offset_bytes, stride_bytes);
}

HRESULT AEROGPU_D3D9_CALL device_set_indices(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hIb,
    D3DDDIFORMAT fmt,
    uint32_t offset_bytes) {
  return kDeviceSetIndicesImpl(hDevice, hIb, fmt, offset_bytes);
}

HRESULT AEROGPU_D3D9_CALL device_set_viewport(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIVIEWPORTINFO* pViewport) {
  return kDeviceSetViewportImpl(hDevice, pViewport);
}

HRESULT AEROGPU_D3D9_CALL device_set_stream_source_freq(D3DDDI_HDEVICE hDevice, uint32_t stream, uint32_t value) {
  // Prefer the real DDI implementation when building the full WDK-backed UMD on
  // Windows so state-block recording and DDI validation match the runtime path.
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_set_stream_source_freq_dispatch(hDevice, stream, value);
#else
  // Host-side portable tests build a minimal D3D9DDI_DEVICEFUNCS table that does
  // not include SetStreamSourceFreq. Provide a small direct-call helper so tests
  // can still exercise instancing behavior without mutating `Device` caches
  // directly.
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  if (stream >= 16) {
    return kD3DErrInvalidCall;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->stream_source_freq[stream] = value;
  stateblock_record_stream_source_freq_locked(dev, stream, value);
  return S_OK;
#endif
}

HRESULT AEROGPU_D3D9_CALL device_set_transform(
    D3DDDI_HDEVICE hDevice,
    D3DTRANSFORMSTATETYPE state,
    const D3DMATRIX* pMatrix) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_set_transform_dispatch(hDevice, state, pMatrix);
#else
  return device_set_transform_portable(hDevice, state, pMatrix);
#endif
}

HRESULT AEROGPU_D3D9_CALL device_set_texture_stage_state(
    D3DDDI_HDEVICE hDevice,
    uint32_t stage,
    uint32_t state,
    uint32_t value) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_set_texture_stage_state_dispatch(hDevice, stage, state, value);
#else
  // Portable host-side tests compile a minimal D3D9DDI_DEVICEFUNCS table which
  // does not include SetTextureStageState. Provide a small direct-call helper so
  // tests can still exercise fixed-function PS selection behavior.
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  if (stage >= 16 || state >= 256) {
    return kD3DErrInvalidCall;
  }

  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->texture_stage_states[stage][state] = value;
  stateblock_record_texture_stage_state_locked(dev, stage, state, value);

  // Mirror the fixed-function PS update hook from the DDI paths.
  if (stage < kFixedfuncMaxTextureStages &&
      (state == kD3dTssColorOp || state == kD3dTssColorArg1 || state == kD3dTssColorArg2 ||
       state == kD3dTssAlphaOp || state == kD3dTssAlphaArg1 || state == kD3dTssAlphaArg2) &&
      !dev->user_ps) {
    Shader** ps_slot = nullptr;
    if (dev->user_vs) {
      ps_slot = &dev->fixedfunc_ps_interop;
    } else if (fixedfunc_supported_fvf(dev->fvf)) {
      const FixedFuncVariant variant = fixedfunc_variant_from_fvf(dev->fvf);
      if (variant != FixedFuncVariant::NONE) {
        ps_slot = &dev->fixedfunc_pipelines[static_cast<size_t>(variant)].ps;
      }
    }
    if (ps_slot) {
      // Guard fixed-function PS selection (same as the DDI paths): unsupported stage-state
      // combinations must not make SetTextureStageState fail.
      const FixedfuncPixelShaderKey ps_key = fixedfunc_ps_key_locked(dev);
      if (ps_key.supported) {
        const HRESULT ps_hr = ensure_fixedfunc_pixel_shader_locked(dev, ps_slot);
        if (FAILED(ps_hr)) {
          return ps_hr;
        }
      }
    }
  }
  return S_OK;
#endif
}

HRESULT AEROGPU_D3D9_CALL device_set_material(D3DDDI_HDEVICE hDevice, const D3DMATERIAL9* pMaterial) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_set_material_dispatch(hDevice, pMaterial);
#else
  if (!hDevice.pDrvPrivate || !pMaterial) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->material = *pMaterial;
  dev->material_valid = true;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_material_locked(dev, dev->material, dev->material_valid);
  return S_OK;
#endif
}

HRESULT AEROGPU_D3D9_CALL device_set_light(D3DDDI_HDEVICE hDevice, uint32_t index, const D3DLIGHT9* pLight) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_set_light_dispatch(hDevice, index, pLight);
#else
  if (!hDevice.pDrvPrivate || !pLight) {
    return E_INVALIDARG;
  }
  if (index >= Device::kMaxLights) {
    return kD3DErrInvalidCall;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->lights[index] = *pLight;
  dev->light_valid[index] = true;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_light_locked(dev, index, dev->lights[index], dev->light_valid[index]);
  return S_OK;
#endif
}

HRESULT AEROGPU_D3D9_CALL device_light_enable(D3DDDI_HDEVICE hDevice, uint32_t index, BOOL enabled) {
#if defined(_WIN32) && defined(AEROGPU_D3D9_USE_WDK_DDI) && AEROGPU_D3D9_USE_WDK_DDI
  return device_light_enable_dispatch(hDevice, index, enabled);
#else
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  if (index >= Device::kMaxLights) {
    return kD3DErrInvalidCall;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->light_enabled[index] = enabled ? TRUE : FALSE;
  dev->fixedfunc_lighting_dirty = true;
  stateblock_record_light_enable_locked(dev, index, dev->light_enabled[index]);
  return S_OK;
#endif
}

HRESULT AEROGPU_D3D9_CALL device_test_set_cursor_hw_active(D3DDDI_HDEVICE hDevice, BOOL active) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->cursor_hw_active = active ? true : false;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_set_unmaterialized_user_shaders(
    D3DDDI_HDEVICE hDevice,
    D3D9DDI_HSHADER user_vs,
    D3D9DDI_HSHADER user_ps) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->user_vs = as_shader(user_vs);
  dev->user_ps = as_shader(user_ps);
  dev->vs = nullptr;
  dev->ps = nullptr;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_alias_fixedfunc_stage0_ps_variant(
    D3DDDI_HDEVICE hDevice,
    uint32_t src_index,
    uint32_t dst_index) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  const uint32_t capacity = static_cast<uint32_t>(
      sizeof(dev->fixedfunc_ps_variants) / sizeof(dev->fixedfunc_ps_variants[0]));
  if (src_index >= capacity || dst_index >= capacity || src_index == dst_index) {
    return kD3DErrInvalidCall;
  }
  Shader* shared = dev->fixedfunc_ps_variants[src_index];
  if (!shared) {
    return kD3DErrInvalidCall;
  }
  if (dev->fixedfunc_ps_variants[dst_index]) {
    return kD3DErrInvalidCall;
  }
  dev->fixedfunc_ps_variants[dst_index] = shared;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_enable_wddm_context(D3DDDI_HDEVICE hDevice) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  // Allocation tracking is guarded on `hContext != 0`. Portable host tests don't
  // run against a real WDDM runtime, so CreateDevice leaves `hContext` as 0 and
  // tests opt-in by setting a dummy non-zero value.
  if (dev->wddm_context.hContext == 0) {
    dev->wddm_context.hContext = 1;
  }
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_rebind_alloc_list_tracker(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_ALLOCATIONLIST* pAllocationList,
    uint32_t allocation_list_capacity,
    uint32_t max_allocation_list_slot_id) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  if (allocation_list_capacity != 0 && !pAllocationList) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->alloc_list_tracker.rebind(
      pAllocationList,
      static_cast<UINT>(allocation_list_capacity),
      static_cast<UINT>(max_allocation_list_slot_id));
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_reset_alloc_list_tracker(D3DDDI_HDEVICE hDevice) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  dev->alloc_list_tracker.reset();
  return S_OK;
}

AllocRef AEROGPU_D3D9_CALL device_test_track_buffer_read(
    D3DDDI_HDEVICE hDevice,
    WddmAllocationHandle hAllocation,
    uint32_t alloc_id,
    uint64_t share_token) {
  if (!hDevice.pDrvPrivate) {
    AllocRef out{};
    out.status = AllocRefStatus::kInvalidArgument;
    return out;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  return dev->alloc_list_tracker.track_buffer_read(hAllocation, static_cast<UINT>(alloc_id), share_token);
}

AllocRef AEROGPU_D3D9_CALL device_test_track_texture_read(
    D3DDDI_HDEVICE hDevice,
    WddmAllocationHandle hAllocation,
    uint32_t alloc_id,
    uint64_t share_token) {
  if (!hDevice.pDrvPrivate) {
    AllocRef out{};
    out.status = AllocRefStatus::kInvalidArgument;
    return out;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  return dev->alloc_list_tracker.track_texture_read(hAllocation, static_cast<UINT>(alloc_id), share_token);
}

AllocRef AEROGPU_D3D9_CALL device_test_track_render_target_write(
    D3DDDI_HDEVICE hDevice,
    WddmAllocationHandle hAllocation,
    uint32_t alloc_id,
    uint64_t share_token) {
  if (!hDevice.pDrvPrivate) {
    AllocRef out{};
    out.status = AllocRefStatus::kInvalidArgument;
    return out;
  }
  auto* dev = as_device(hDevice);
  std::lock_guard<std::mutex> lock(dev->mutex);
  return dev->alloc_list_tracker.track_render_target_write(hAllocation, static_cast<UINT>(alloc_id), share_token);
}

HRESULT AEROGPU_D3D9_CALL device_test_force_umd_private_features(D3DDDI_HDEVICE hDevice, uint64_t device_features) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  if (!dev || !dev->adapter) {
    return E_FAIL;
  }
  std::lock_guard<std::mutex> lock(dev->mutex);
  std::memset(&dev->adapter->umd_private, 0, sizeof(dev->adapter->umd_private));
  dev->adapter->umd_private.size_bytes = sizeof(dev->adapter->umd_private);
  dev->adapter->umd_private.struct_version = AEROGPU_UMDPRIV_STRUCT_VERSION_V1;
  dev->adapter->umd_private.device_abi_version_u32 = AEROGPU_ABI_VERSION_U32;
  dev->adapter->umd_private.device_features = device_features;
  dev->adapter->umd_private_valid = true;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL adapter_test_set_completed_fence(D3DDDI_HADAPTER hAdapter, uint64_t completed_fence) {
  if (!hAdapter.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* adapter = as_adapter(hAdapter);
  std::lock_guard<std::mutex> lock(adapter->fence_mutex);
  adapter->completed_fence = completed_fence;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_set_resource_backing(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hResource,
    uint32_t backing_alloc_id,
    uint32_t backing_offset_bytes,
    WddmAllocationHandle wddm_hAllocation) {
  if (!hDevice.pDrvPrivate || !hResource.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  auto* res = as_resource(hResource);
  std::lock_guard<std::mutex> lock(dev->mutex);
  res->backing_alloc_id = backing_alloc_id;
  res->backing_offset_bytes = backing_offset_bytes;
  res->wddm_hAllocation = wddm_hAllocation;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_set_resource_share_token(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hResource,
    uint64_t share_token) {
  if (!hDevice.pDrvPrivate || !hResource.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  auto* res = as_resource(hResource);
  std::lock_guard<std::mutex> lock(dev->mutex);
  res->share_token = share_token;
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_set_resource_shared_private_driver_data(
    D3DDDI_HDEVICE hDevice,
    D3DDDI_HRESOURCE hResource,
    const void* data,
    uint32_t data_size) {
  if (!hDevice.pDrvPrivate || !hResource.pDrvPrivate) {
    return E_INVALIDARG;
  }
  if (data_size != 0 && !data) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  auto* res = as_resource(hResource);
  std::lock_guard<std::mutex> lock(dev->mutex);
  res->shared_private_driver_data.assign(static_cast<const uint8_t*>(data),
                                         static_cast<const uint8_t*>(data) + data_size);
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_test_force_device_lost(D3DDDI_HDEVICE hDevice, HRESULT hr) {
  if (!hDevice.pDrvPrivate) {
    return E_INVALIDARG;
  }
  auto* dev = as_device(hDevice);
  dev->device_lost.store(true, std::memory_order_release);
  dev->device_lost_hr.store(static_cast<int32_t>(hr), std::memory_order_release);
  dev->device_lost_reason.store(static_cast<uint32_t>(DeviceLostReason::WddmSubmitRender), std::memory_order_release);
  return S_OK;
}

HRESULT AEROGPU_D3D9_CALL device_draw_primitive(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t start_vertex,
    uint32_t primitive_count) {
  return kDeviceDrawPrimitiveImpl(hDevice, type, start_vertex, primitive_count);
}

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    int32_t base_vertex,
    uint32_t min_index,
    uint32_t num_vertices,
    uint32_t start_index,
    uint32_t primitive_count) {
  return kDeviceDrawIndexedPrimitiveImpl(hDevice, type, base_vertex, min_index, num_vertices, start_index, primitive_count);
}

HRESULT AEROGPU_D3D9_CALL device_draw_primitive_up(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t primitive_count,
    const void* pVertexData,
    uint32_t stride_bytes) {
  return kDeviceDrawPrimitiveUpImpl(hDevice, type, primitive_count, pVertexData, stride_bytes);
}

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive_up(
    D3DDDI_HDEVICE hDevice,
    D3DDDIPRIMITIVETYPE type,
    uint32_t min_vertex_index,
    uint32_t num_vertices,
    uint32_t primitive_count,
    const void* pIndexData,
    D3DDDIFORMAT index_data_format,
    const void* pVertexData,
    uint32_t stride_bytes) {
  return kDeviceDrawIndexedPrimitiveUpImpl(hDevice,
                                          type,
                                          min_vertex_index,
                                          num_vertices,
                                          primitive_count,
                                          pIndexData,
                                          index_data_format,
                                          pVertexData,
                                          stride_bytes);
}

HRESULT AEROGPU_D3D9_CALL device_draw_primitive2(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWPRIMITIVE2* pDraw) {
  return kDeviceDrawPrimitive2Impl(hDevice, pDraw);
}

HRESULT AEROGPU_D3D9_CALL device_draw_indexed_primitive2(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_DRAWINDEXEDPRIMITIVE2* pDraw) {
  return kDeviceDrawIndexedPrimitive2Impl(hDevice, pDraw);
}

HRESULT AEROGPU_D3D9_CALL device_process_vertices(
    D3DDDI_HDEVICE hDevice,
    const D3DDDIARG_PROCESSVERTICES* pProcessVertices) {
  return kDeviceProcessVerticesImpl(hDevice, pProcessVertices);
}
} // namespace aerogpu

// -----------------------------------------------------------------------------
// Public entrypoints
// -----------------------------------------------------------------------------

HRESULT AEROGPU_D3D9_CALL OpenAdapter(
    D3DDDIARG_OPENADAPTER* pOpenAdapter) {
  const uint64_t iface_version =
      pOpenAdapter ? aerogpu::d3d9_trace_pack_u32_u32(get_interface_version(pOpenAdapter), pOpenAdapter->Version) : 0;
  aerogpu::D3d9TraceCall trace(aerogpu::D3d9TraceFunc::OpenAdapter,
                               iface_version,
                               aerogpu::d3d9_trace_arg_ptr(pOpenAdapter),
                               pOpenAdapter ? aerogpu::d3d9_trace_arg_ptr(pOpenAdapter->pAdapterFuncs) : 0,
                               0);
  HDC hdc = nullptr;
  try {
    if (!pOpenAdapter) {
      return trace.ret(E_INVALIDARG);
    }
 
    LUID luid = aerogpu::default_luid();
#if defined(_WIN32)
    // Some runtimes may call OpenAdapter/OpenAdapter2 without providing an HDC or
    // explicit LUID. Resolve a stable LUID from the primary display so the adapter
    // cache and KMD query helpers can be shared with OpenAdapterFromHdc/Luid.
    hdc = GetDC(nullptr);
    if (hdc) {
      if (!aerogpu::get_luid_from_hdc(hdc, &luid)) {
        aerogpu::logf("aerogpu-d3d9: OpenAdapter failed to resolve adapter LUID from primary HDC\n");
      }
    }
#endif
    auto* adapter_funcs = reinterpret_cast<D3D9DDI_ADAPTERFUNCS*>(pOpenAdapter->pAdapterFuncs);
    if (!adapter_funcs) {
#if defined(_WIN32)
      if (hdc) {
        ReleaseDC(nullptr, hdc);
      }
#endif
      return trace.ret(E_INVALIDARG);
    }
 
    const HRESULT hr = aerogpu::OpenAdapterCommon("OpenAdapter",
                                                  get_interface_version(pOpenAdapter),
                                                  pOpenAdapter->Version,
                                                  pOpenAdapter->pAdapterCallbacks,
                                                  get_adapter_callbacks2(pOpenAdapter),
                                                  luid,
                                                  &pOpenAdapter->hAdapter,
                                                  adapter_funcs);
#if defined(_WIN32)
    if (SUCCEEDED(hr) && hdc) {
      auto* adapter = aerogpu::as_adapter(pOpenAdapter->hAdapter);
      if (adapter) {
        const int w = GetDeviceCaps(hdc, HORZRES);
        const int h = GetDeviceCaps(hdc, VERTRES);
        const int refresh = GetDeviceCaps(hdc, VREFRESH);
        if (w > 0) {
          adapter->primary_width = static_cast<uint32_t>(w);
        }
        if (h > 0) {
          adapter->primary_height = static_cast<uint32_t>(h);
        }
        if (refresh > 0) {
          adapter->primary_refresh_hz = static_cast<uint32_t>(refresh);
        }
      }
 
      const bool kmd_ok = adapter && adapter->kmd_query.InitFromHdc(hdc);
      if (adapter) {
        adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
        uint32_t vid_pn_source_id = 0;
        if (kmd_ok && adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
          adapter->vid_pn_source_id = vid_pn_source_id;
          adapter->vid_pn_source_id_valid = true;
        } else {
          adapter->vid_pn_source_id = 0;
          adapter->vid_pn_source_id_valid = false;
        }
        set_vid_pn_source_id(pOpenAdapter, adapter->vid_pn_source_id_valid ? adapter->vid_pn_source_id : 0);
      }
 
      if (kmd_ok) {
        uint32_t max_slot_id = 0;
        if (adapter && adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
          adapter->max_allocation_list_slot_id = max_slot_id;
          if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
            aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                          static_cast<unsigned>(max_slot_id));
          }
        }
 
        uint64_t submitted = 0;
        uint64_t completed = 0;
        if (adapter->kmd_query.QueryFence(&submitted, &completed)) {
          aerogpu::logf("aerogpu-d3d9: KMD fence submitted=%llu completed=%llu\n",
                        static_cast<unsigned long long>(submitted),
                        static_cast<unsigned long long>(completed));
        }
 
        aerogpu_umd_private_v1 priv;
        std::memset(&priv, 0, sizeof(priv));
        if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
          adapter->umd_private = priv;
          adapter->umd_private_valid = true;
 
          char magicStr[5] = {0, 0, 0, 0, 0};
          magicStr[0] = (char)((priv.device_mmio_magic >> 0) & 0xFF);
          magicStr[1] = (char)((priv.device_mmio_magic >> 8) & 0xFF);
          magicStr[2] = (char)((priv.device_mmio_magic >> 16) & 0xFF);
          magicStr[3] = (char)((priv.device_mmio_magic >> 24) & 0xFF);
 
          aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                        priv.device_mmio_magic,
                        magicStr,
                        priv.device_abi_version_u32,
                        static_cast<unsigned long long>(priv.device_features),
                        priv.flags);
        }
      }
    }
  }
  if (hdc) {
    ReleaseDC(nullptr, hdc);
  }
#endif
    return trace.ret(hr);
  } catch (const std::bad_alloc&) {
#if defined(_WIN32)
    if (hdc) {
      ReleaseDC(nullptr, hdc);
    }
#endif
    return trace.ret(E_OUTOFMEMORY);
  } catch (...) {
#if defined(_WIN32)
    if (hdc) {
      ReleaseDC(nullptr, hdc);
    }
#endif
    return trace.ret(E_FAIL);
  }
}

HRESULT AEROGPU_D3D9_CALL OpenAdapter2(
    D3DDDIARG_OPENADAPTER2* pOpenAdapter) {
  const uint64_t iface_version =
      pOpenAdapter ? aerogpu::d3d9_trace_pack_u32_u32(get_interface_version(pOpenAdapter), pOpenAdapter->Version) : 0;
  aerogpu::D3d9TraceCall trace(aerogpu::D3d9TraceFunc::OpenAdapter2,
                               iface_version,
                               aerogpu::d3d9_trace_arg_ptr(pOpenAdapter),
                               pOpenAdapter ? aerogpu::d3d9_trace_arg_ptr(pOpenAdapter->pAdapterFuncs) : 0,
                               0);
  HDC hdc = nullptr;
  try {
    if (!pOpenAdapter) {
      return trace.ret(E_INVALIDARG);
    }

    LUID luid = aerogpu::default_luid();
#if defined(_WIN32)
    // Some runtimes may call OpenAdapter/OpenAdapter2 without providing an HDC or
    // explicit LUID. Resolve a stable LUID from the primary display so the adapter
    // cache and KMD query helpers can be shared with OpenAdapterFromHdc/Luid.
    hdc = GetDC(nullptr);
    if (hdc) {
      if (!aerogpu::get_luid_from_hdc(hdc, &luid)) {
        aerogpu::logf("aerogpu-d3d9: OpenAdapter2 failed to resolve adapter LUID from primary HDC\n");
      }
    }
#endif
    auto* adapter_funcs = reinterpret_cast<D3D9DDI_ADAPTERFUNCS*>(pOpenAdapter->pAdapterFuncs);
    if (!adapter_funcs) {
#if defined(_WIN32)
      if (hdc) {
        ReleaseDC(nullptr, hdc);
      }
#endif
      return trace.ret(E_INVALIDARG);
    }

    const HRESULT hr = aerogpu::OpenAdapterCommon("OpenAdapter2",
                                                  get_interface_version(pOpenAdapter),
                                                  pOpenAdapter->Version,
                                                  pOpenAdapter->pAdapterCallbacks,
                                                  get_adapter_callbacks2(pOpenAdapter),
                                                  luid,
                                                  &pOpenAdapter->hAdapter,
                                                  adapter_funcs);
#if defined(_WIN32)
    if (SUCCEEDED(hr) && hdc) {
      auto* adapter = aerogpu::as_adapter(pOpenAdapter->hAdapter);
      if (adapter) {
        const int w = GetDeviceCaps(hdc, HORZRES);
        const int h = GetDeviceCaps(hdc, VERTRES);
        const int refresh = GetDeviceCaps(hdc, VREFRESH);
        if (w > 0) {
          adapter->primary_width = static_cast<uint32_t>(w);
        }
        if (h > 0) {
          adapter->primary_height = static_cast<uint32_t>(h);
        }
        if (refresh > 0) {
          adapter->primary_refresh_hz = static_cast<uint32_t>(refresh);
        }
      }

      const bool kmd_ok = adapter && adapter->kmd_query.InitFromHdc(hdc);
      if (adapter) {
        adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
        uint32_t vid_pn_source_id = 0;
        if (kmd_ok && adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
          adapter->vid_pn_source_id = vid_pn_source_id;
          adapter->vid_pn_source_id_valid = true;
        } else {
          adapter->vid_pn_source_id = 0;
          adapter->vid_pn_source_id_valid = false;
        }
        set_vid_pn_source_id(pOpenAdapter, adapter->vid_pn_source_id_valid ? adapter->vid_pn_source_id : 0);
      }
      if (kmd_ok) {
        uint32_t max_slot_id = 0;
        if (adapter && adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
          adapter->max_allocation_list_slot_id = max_slot_id;
          if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
            aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                          static_cast<unsigned>(max_slot_id));
          }
        }

        uint64_t submitted = 0;
        uint64_t completed = 0;
        if (adapter->kmd_query.QueryFence(&submitted, &completed)) {
          aerogpu::logf("aerogpu-d3d9: KMD fence submitted=%llu completed=%llu\n",
                        static_cast<unsigned long long>(submitted),
                        static_cast<unsigned long long>(completed));
        }

        aerogpu_umd_private_v1 priv;
        std::memset(&priv, 0, sizeof(priv));
        if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
          adapter->umd_private = priv;
          adapter->umd_private_valid = true;

          char magicStr[5] = {0, 0, 0, 0, 0};
          magicStr[0] = (char)((priv.device_mmio_magic >> 0) & 0xFF);
          magicStr[1] = (char)((priv.device_mmio_magic >> 8) & 0xFF);
          magicStr[2] = (char)((priv.device_mmio_magic >> 16) & 0xFF);
          magicStr[3] = (char)((priv.device_mmio_magic >> 24) & 0xFF);

          aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                        priv.device_mmio_magic,
                        magicStr,
                        priv.device_abi_version_u32,
                        static_cast<unsigned long long>(priv.device_features),
                        priv.flags);
        }
      }
    }
    if (hdc) {
      ReleaseDC(nullptr, hdc);
    }
#endif
    return trace.ret(hr);
  } catch (const std::bad_alloc&) {
#if defined(_WIN32)
    if (hdc) {
      ReleaseDC(nullptr, hdc);
    }
#endif
    return trace.ret(E_OUTOFMEMORY);
  } catch (...) {
#if defined(_WIN32)
    if (hdc) {
      ReleaseDC(nullptr, hdc);
    }
#endif
    return trace.ret(E_FAIL);
  }
}

HRESULT AEROGPU_D3D9_CALL OpenAdapterFromHdc(
    D3DDDIARG_OPENADAPTERFROMHDC* pOpenAdapter) {
  const uint64_t iface_version =
      pOpenAdapter ? aerogpu::d3d9_trace_pack_u32_u32(get_interface_version(pOpenAdapter), pOpenAdapter->Version) : 0;
  aerogpu::D3d9TraceCall trace(aerogpu::D3d9TraceFunc::OpenAdapterFromHdc,
                               iface_version,
                               pOpenAdapter ? aerogpu::d3d9_trace_arg_ptr(pOpenAdapter->hDc) : 0,
                               aerogpu::d3d9_trace_arg_ptr(pOpenAdapter),
                               pOpenAdapter ? aerogpu::d3d9_trace_arg_ptr(pOpenAdapter->pAdapterFuncs) : 0);
  try {
    if (!pOpenAdapter) {
      return trace.ret(E_INVALIDARG);
    }

    LUID luid = aerogpu::default_luid();
#if defined(_WIN32)
    if (pOpenAdapter->hDc && !aerogpu::get_luid_from_hdc(pOpenAdapter->hDc, &luid)) {
      aerogpu::logf("aerogpu-d3d9: OpenAdapterFromHdc failed to resolve adapter LUID from HDC\n");
    }
#endif
    pOpenAdapter->AdapterLuid = luid;

    aerogpu::logf("aerogpu-d3d9: OpenAdapterFromHdc hdc=%p LUID=%08x:%08x\n",
                  pOpenAdapter->hDc,
                  static_cast<unsigned>(luid.HighPart),
                  static_cast<unsigned>(luid.LowPart));
    auto* adapter_funcs = reinterpret_cast<D3D9DDI_ADAPTERFUNCS*>(pOpenAdapter->pAdapterFuncs);
    if (!adapter_funcs) {
      return trace.ret(E_INVALIDARG);
    }

    const HRESULT hr = aerogpu::OpenAdapterCommon("OpenAdapterFromHdc",
                                                  get_interface_version(pOpenAdapter),
                                                  pOpenAdapter->Version,
                                                  pOpenAdapter->pAdapterCallbacks,
                                                  get_adapter_callbacks2(pOpenAdapter),
                                                  luid,
                                                  &pOpenAdapter->hAdapter,
                                                  adapter_funcs);

#if defined(_WIN32)
    if (SUCCEEDED(hr) && pOpenAdapter->hDc) {
      auto* adapter = aerogpu::as_adapter(pOpenAdapter->hAdapter);
      if (adapter) {
        const int w = GetDeviceCaps(pOpenAdapter->hDc, HORZRES);
        const int h = GetDeviceCaps(pOpenAdapter->hDc, VERTRES);
        const int refresh = GetDeviceCaps(pOpenAdapter->hDc, VREFRESH);
        if (w > 0) {
          adapter->primary_width = static_cast<uint32_t>(w);
        }
        if (h > 0) {
          adapter->primary_height = static_cast<uint32_t>(h);
        }
        if (refresh > 0) {
          adapter->primary_refresh_hz = static_cast<uint32_t>(refresh);
        }
      }
      const bool kmd_ok = adapter && adapter->kmd_query.InitFromHdc(pOpenAdapter->hDc);
      if (adapter) {
        adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
        uint32_t vid_pn_source_id = 0;
        if (kmd_ok && adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
          adapter->vid_pn_source_id = vid_pn_source_id;
          adapter->vid_pn_source_id_valid = true;
        } else {
          adapter->vid_pn_source_id = 0;
          adapter->vid_pn_source_id_valid = false;
        }
        set_vid_pn_source_id(pOpenAdapter, adapter->vid_pn_source_id_valid ? adapter->vid_pn_source_id : 0);
      }
      if (kmd_ok) {
        uint32_t max_slot_id = 0;
        if (adapter && adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
          adapter->max_allocation_list_slot_id = max_slot_id;
          if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
            aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                          static_cast<unsigned>(max_slot_id));
          }
        }

        uint64_t submitted = 0;
        uint64_t completed = 0;
        if (adapter->kmd_query.QueryFence(&submitted, &completed)) {
          aerogpu::logf("aerogpu-d3d9: KMD fence submitted=%llu completed=%llu\n",
                        static_cast<unsigned long long>(submitted),
                        static_cast<unsigned long long>(completed));
        }

        aerogpu_umd_private_v1 priv;
        std::memset(&priv, 0, sizeof(priv));
        if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
          adapter->umd_private = priv;
          adapter->umd_private_valid = true;

          char magicStr[5] = {0, 0, 0, 0, 0};
          magicStr[0] = (char)((priv.device_mmio_magic >> 0) & 0xFF);
          magicStr[1] = (char)((priv.device_mmio_magic >> 8) & 0xFF);
          magicStr[2] = (char)((priv.device_mmio_magic >> 16) & 0xFF);
          magicStr[3] = (char)((priv.device_mmio_magic >> 24) & 0xFF);

          aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                        priv.device_mmio_magic,
                        magicStr,
                        priv.device_abi_version_u32,
                        static_cast<unsigned long long>(priv.device_features),
                        priv.flags);
        }
      }
    }
#endif

    return trace.ret(hr);
  } catch (const std::bad_alloc&) {
    return trace.ret(E_OUTOFMEMORY);
  } catch (...) {
    return trace.ret(E_FAIL);
  }
}

HRESULT AEROGPU_D3D9_CALL OpenAdapterFromLuid(
    D3DDDIARG_OPENADAPTERFROMLUID* pOpenAdapter) {
  const uint64_t iface_version =
      pOpenAdapter ? aerogpu::d3d9_trace_pack_u32_u32(get_interface_version(pOpenAdapter), pOpenAdapter->Version) : 0;
  const uint64_t luid_packed = pOpenAdapter
                                  ? aerogpu::d3d9_trace_pack_u32_u32(pOpenAdapter->AdapterLuid.LowPart,
                                                                     static_cast<uint32_t>(pOpenAdapter->AdapterLuid.HighPart))
                                  : 0;
  aerogpu::D3d9TraceCall trace(aerogpu::D3d9TraceFunc::OpenAdapterFromLuid,
                               iface_version,
                               luid_packed,
                               aerogpu::d3d9_trace_arg_ptr(pOpenAdapter),
                               pOpenAdapter ? aerogpu::d3d9_trace_arg_ptr(pOpenAdapter->pAdapterFuncs) : 0);
  try {
    if (!pOpenAdapter) {
      return trace.ret(E_INVALIDARG);
    }

    const LUID luid = pOpenAdapter->AdapterLuid;
    auto* adapter_funcs = reinterpret_cast<D3D9DDI_ADAPTERFUNCS*>(pOpenAdapter->pAdapterFuncs);
    if (!adapter_funcs) {
      return trace.ret(E_INVALIDARG);
    }

    const HRESULT hr = aerogpu::OpenAdapterCommon("OpenAdapterFromLuid",
                                                  get_interface_version(pOpenAdapter),
                                                  pOpenAdapter->Version,
                                                  pOpenAdapter->pAdapterCallbacks,
                                                  get_adapter_callbacks2(pOpenAdapter),
                                                  luid,
                                                  &pOpenAdapter->hAdapter,
                                                  adapter_funcs);

#if defined(_WIN32)
    if (SUCCEEDED(hr)) {
      auto* adapter = aerogpu::as_adapter(pOpenAdapter->hAdapter);
      const bool kmd_ok = adapter && adapter->kmd_query.InitFromLuid(luid);
      if (adapter) {
        adapter->kmd_query_available.store(kmd_ok, std::memory_order_release);
        uint32_t vid_pn_source_id = 0;
        if (kmd_ok && adapter->kmd_query.GetVidPnSourceId(&vid_pn_source_id)) {
          adapter->vid_pn_source_id = vid_pn_source_id;
          adapter->vid_pn_source_id_valid = true;
        } else {
          adapter->vid_pn_source_id = 0;
          adapter->vid_pn_source_id_valid = false;
        }
        set_vid_pn_source_id(pOpenAdapter, adapter->vid_pn_source_id_valid ? adapter->vid_pn_source_id : 0);
      }
      if (kmd_ok) {
        uint32_t max_slot_id = 0;
        if (adapter && adapter->kmd_query.QueryMaxAllocationListSlotId(&max_slot_id)) {
          adapter->max_allocation_list_slot_id = max_slot_id;
          if (!adapter->max_allocation_list_slot_id_logged.exchange(true)) {
            aerogpu::logf("aerogpu-d3d9: KMD MaxAllocationListSlotId=%u\n",
                          static_cast<unsigned>(max_slot_id));
          }
        }

        uint64_t submitted = 0;
        uint64_t completed = 0;
        if (adapter->kmd_query.QueryFence(&submitted, &completed)) {
          aerogpu::logf("aerogpu-d3d9: KMD fence submitted=%llu completed=%llu\n",
                        static_cast<unsigned long long>(submitted),
                        static_cast<unsigned long long>(completed));
        }

        aerogpu_umd_private_v1 priv;
        std::memset(&priv, 0, sizeof(priv));
        if (adapter->kmd_query.QueryUmdPrivate(&priv)) {
          adapter->umd_private = priv;
          adapter->umd_private_valid = true;

          char magicStr[5] = {0, 0, 0, 0, 0};
          magicStr[0] = (char)((priv.device_mmio_magic >> 0) & 0xFF);
          magicStr[1] = (char)((priv.device_mmio_magic >> 8) & 0xFF);
          magicStr[2] = (char)((priv.device_mmio_magic >> 16) & 0xFF);
          magicStr[3] = (char)((priv.device_mmio_magic >> 24) & 0xFF);

          aerogpu::logf("aerogpu-d3d9: UMDRIVERPRIVATE magic=0x%08x (%s) abi=0x%08x features=0x%llx flags=0x%08x\n",
                        priv.device_mmio_magic,
                        magicStr,
                        priv.device_abi_version_u32,
                        static_cast<unsigned long long>(priv.device_features),
                        priv.flags);
        }
      }
    }
#endif

    return trace.ret(hr);
  } catch (const std::bad_alloc&) {
    return trace.ret(E_OUTOFMEMORY);
  } catch (...) {
    return trace.ret(E_FAIL);
  }
}
