# Testing (local + CI)

This document is the practical companion to [`12-testing-strategy.md`](./12-testing-strategy.md). It focuses on **how to run Aero’s test stack locally**, how that maps to CI, and common browser-specific failure modes.

> **Policy note (fixtures):** The repository must not include proprietary Windows images/ISOs, BIOS ROMs, or other copyrighted firmware blobs. Tests and CI should run using **open fixtures** (synthetic images, open-source OS images, generated data). See [`FIXTURES.md`](./FIXTURES.md) and [`13-legal-considerations.md`](./13-legal-considerations.md). CI also enforces this via `scripts/ci/check-repo-policy.sh`.

---

## Node.js version

CI uses the exact Node.js version declared in the repo root [`.nvmrc`](../.nvmrc). Local tooling requires
**at least** that version and will warn if your local version differs (to help debug flaky toolchain issues).

From the repo root:

```bash
nvm install && nvm use   # if you use nvm
node scripts/check-node-version.mjs
# or: npm run check:node
```

If you need to run tooling in an environment where you can't change the Node version (unsupported), you can bypass the hard error:

```bash
  AERO_ALLOW_UNSUPPORTED_NODE=1 node scripts/check-node-version.mjs
```

---

## Manual testing checklists

Some end-to-end behaviors (especially audio) are hard to validate automatically. The repo keeps a small set of
**manual, reproducible smoke-test checklists** under `docs/testing/`.

- Windows 7 audio (in-box HDA driver): [`docs/testing/audio-windows7.md`](./testing/audio-windows7.md)

## Disk streaming endpoint conformance (HTTP Range / chunked)

If you are debugging a disk image delivery endpoint (local, staging, prod) and want to validate that it matches
the browser-side streaming client expectations, use the dependency-free conformance tool:

- Range mode (single object + HTTP `Range`): `python3 tools/disk-streaming-conformance/conformance.py --base-url ...`
- Chunked mode (`manifest.json` + `chunks/*.bin`, no `Range` header): `python3 tools/disk-streaming-conformance/conformance.py --mode chunked --manifest-url ...`

For quick local sanity checks against the repo’s dev servers:

- `python3 tools/disk-streaming-conformance/selftest_range_server.py`
- `python3 tools/disk-streaming-conformance/selftest_chunk_server.py`

Use `--strict` to fail on warnings (recommended when validating CDN/edge behavior).

## Device/driver end-to-end test plans

Some subsystems also have “single document” end-to-end test plans (device model ↔ guest drivers ↔ web runtime).

- virtio-input (device model + Win7 driver + web runtime): [`docs/test-plans/virtio-input.md`](./test-plans/virtio-input.md)

## Test fixtures (boot sectors + tiny disk images)

Boot/system tests use **tiny deterministic fixtures** under `tests/fixtures/` (e.g. a 512-byte boot sector that writes a known pattern to VGA/serial).

Regenerate them with:

```bash
cargo xtask fixtures
```

To verify they are up-to-date without modifying your working tree:

```bash
cargo xtask fixtures --check
```

CI runs the `--check` form (also the first step of `cargo xtask test-all`) and fails if any fixture generated by
`cargo xtask fixtures` is missing or out-of-date. Use `cargo xtask test-all --skip-fixtures` to skip this check
while iterating.

`cargo xtask test-all` runs the same fixture checks automatically (unless you pass `--skip-fixtures`).

## Quick start: run the full test suite

### Unified runner (recommended)

From the repo root:

```bash
cargo xtask test-all
```

Note: in this repo, `cargo xtask` runs Cargo with `--locked` (matching CI). If you hit a lockfile error, update
the lockfile (e.g. `cargo generate-lockfile`, or `cargo generate-lockfile --manifest-path path/to/tool/Cargo.toml`
for standalone tools) and include the `Cargo.lock` diff in your PR.

`bash ./scripts/test-all.sh` is kept as a thin wrapper around `cargo xtask test-all`
for a transition period, but `cargo xtask` is the canonical implementation (and
  works on Windows without bash).

The unified runner executes (in order):

1. `cargo xtask fixtures --check` (validate committed deterministic fixtures)
2. `cargo xtask bios-rom --check` (validate `assets/bios.bin`)
3. `cargo fmt --all -- --check`
4. `cargo clippy --locked --workspace --all-targets --all-features -- -D warnings`
5. `cargo test --locked --workspace --all-features`
6. `wasm-pack test --node` (in the WASM crate)
7. `cargo check --target wasm32-unknown-unknown -p aero-devices-storage -p aero-machine` (WASM build sanity)
8. `npm run test:unit`
9. `npm run test:e2e`

By default it sets `AERO_REQUIRE_WEBGPU=0` (matching CI) unless you explicitly enable it.

Common options:

```bash
# Skip deterministic fixture checks while iterating
cargo xtask test-all --skip-fixtures

# Skip the slowest step
cargo xtask test-all --skip-e2e

# Require WebGPU for tests that gate on it
cargo xtask test-all --webgpu

# Select Playwright projects (repeatable)
cargo xtask test-all --pw-project chromium --pw-project firefox

# Forward additional Playwright CLI args (everything after --)
cargo xtask test-all --pw-project chromium -- --grep smoke
```

### Minimal Rust sanity checks (no Node / no Playwright)

If you want a fast, Rust-only smoke check (useful while iterating, or when debugging CI failures), run:

```bash
bash ./scripts/safe-run.sh cargo test -p aero-platform --locked
bash ./scripts/safe-run.sh cargo check -p aero-machine --locked
bash ./scripts/safe-run.sh cargo check -p aero-wasm --target wasm32-unknown-unknown --locked
```

### Focused test runners

If you're working on a specific subsystem, `xtask` also provides smaller suites, and some subsystems
have dedicated `scripts/ci/*` helpers:

```bash
# USB + input (Rust + focused web unit tests; optional Playwright subset)
cargo xtask input
cargo xtask input --rust-only
cargo xtask input --usb-all
cargo xtask input --machine
cargo xtask input --with-wasm
cargo xtask input --rust-only --with-wasm
cargo xtask input --wasm --rust-only
cargo xtask input --e2e

# If your Node workspace entrypoint is `web/` (rather than repo root), use:
cargo xtask input --node-dir web

# Boot display (VGA/VBE/INT10) regression suite (device model + BIOS INT10 + machine wiring)
bash ./scripts/ci/run-vga-vbe-tests.sh
```

### CPU instruction conformance / differential tests (x86_64 unix)

The conformance harness compares Aero instruction semantics against native host execution on `x86_64` unix.

```bash
# Run a small corpus
cargo xtask conformance --cases 512

# (Optional) same via `just`
just test-conformance --cases 512

# Reproduce a failure (seed is decimal or 0x-hex; `_` separators allowed)
cargo xtask conformance \
  --cases 5000 \
  --seed 0x52c671d9a4f231b9 \
  --filter key:add \
  --report target/conformance.json \
  -- --nocapture
```

If your repo layout differs from the defaults, override directories:

- `AERO_NODE_DIR` / `--node-dir`: the directory containing `package.json`
- `AERO_WASM_CRATE_DIR` / `--wasm-crate-dir`: the crate directory containing the WASM `Cargo.toml`

To see what directory CI/local tooling will select by default, run:

```bash
node scripts/ci/detect-node-dir.mjs
```

---

## Node workspaces: install once, run per-package scripts

The repo uses **npm workspaces** (see [`docs/adr/0006-node-monorepo-tooling.md`](./adr/0006-node-monorepo-tooling.md)).

Install all Node dependencies from the repo root:

```bash
npm ci
```

Run package-scoped scripts using `npm -w <path>`:

```bash
# Frontend dev server
npm run dev

# Gateway unit tests
npm -w backend/aero-gateway test
```

## Security header regression tests (COOP/COEP/CSP)

Cross-origin isolation and Aero’s WASM/JIT features depend on a consistent set of security headers
(COOP/COEP/CORP/OAC + CSP with `wasm-unsafe-eval`). To prevent template/server drift, the repo includes:

```bash
# Fast static validation (templates + Vite configs + proxies)
npm run check:security-headers

# Runtime validation (Playwright, preview server)
npm run test:security-headers
```

### Manual (equivalent) commands

From the repo root:

```bash
# Rust format/lint/test (host)
cargo fmt --all -- --check
cargo clippy --locked --workspace --all-targets --all-features -- -D warnings
cargo test --locked --workspace --all-features

# Rust → WASM tests (run from the WASM crate directory; see below)
# wasm-pack test --node

# TypeScript / JS unit tests
npm ci
npm run test:unit

# Playwright E2E
npx playwright install --with-deps
npm run test:e2e
```

Notes:

- The `wasm-pack` step is usually run from the specific crate that produces WASM (often under `crates/`).
- Playwright browser downloads are large; locally you typically only need to run `npx playwright install --with-deps` once per machine.
  - CI uses `.github/actions/setup-playwright` to cache the Playwright browser binaries directory and re-install only if the cache is missing.
  - CI also sets `PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=1` for `npm ci` so browser downloads happen via the explicit install step (and can be cached).

---

## Rust unit tests (host)

Run all Rust tests in the workspace:

```bash
cargo test --locked --workspace
```

Run tests for a single crate:

```bash
cargo test --locked -p <crate-name>
```

Run a single test (by name filter):

```bash
cargo test --locked -p <crate-name> <test_name_substring>
```

Useful flags:

```bash
# Show stdout/stderr for passing tests
cargo test --locked -p <crate-name> -- --nocapture

# Run ignored tests (if any are marked #[ignore])
cargo test --locked -p <crate-name> -- --ignored
```

---

## QEMU boot tests (serial + framebuffer)

Some integration tests boot guest media **headlessly under QEMU** to validate early boot flows and
enable deterministic framebuffer/serial checkpoints.

These tests are defined under the workspace root `tests/` directory (e.g. `tests/boot_sector.rs`,
`tests/freedos_boot.rs`, `tests/windows7_boot.rs`, and `tests/boot/basic_boot.rs`).

The canonical way to run them is via the dedicated `aero-boot-tests` crate: it registers them as
explicit `[[test]]` targets in `crates/aero-boot-tests/Cargo.toml` (paths like
`../../tests/boot_sector.rs`), so you can run them with `cargo test -p aero-boot-tests --test ...`.

Note: this is a **test harness / dependency hygiene** choice (see the note below on `-p aero`), and
does *not* imply `crates/emulator` is the canonical VM wiring layer. Canonical machine wiring lives
in `crates/aero-machine` (`aero_machine::Machine`); see [`docs/vm-crate-map.md`](./vm-crate-map.md)
and [`docs/21-emulator-crate-migration.md`](./21-emulator-crate-migration.md).

Avoid running them via the workspace root package (`-p aero`): `aero` intentionally does not carry
the `firmware`/`memory` dev-deps needed by `boot_basic`, and it also pulls in heavyweight GPU
dev-dependencies that significantly slow compilation.

### Prerequisites

- `qemu-system-i386`
- `mtools` (for patching floppy images in `scripts/prepare-freedos.sh`)
- `unzip` + `curl` (for downloading FreeDOS)

### Boot sector + FreeDOS (open-source, CI-safe)

```bash
# Generate the synthetic boot fixtures used by the tests (committed output is checked in CI).
cargo xtask fixtures

# Verify the committed fixture outputs are up-to-date (this is what CI runs).
cargo xtask fixtures --check

# Download + patch FreeDOS 1.4 floppy image (written under gitignored test-images/).
bash ./scripts/prepare-freedos.sh

# Run the QEMU boot tests.
cargo test -p aero-boot-tests --test boot_sector --test freedos_boot --locked

# In constrained/contended sandboxes (agents/CI-like), prefer safe-run.sh (timeout + memory limit).
# The first run can take >10 minutes to compile on a cold checkout.
AERO_TIMEOUT=1200 bash ./scripts/safe-run.sh cargo test -p aero-boot-tests --test boot_sector --test freedos_boot --locked
```

### Windows 7 (local only)

The Windows boot test is intentionally gated and requires user-supplied media:

```bash
bash ./scripts/prepare-windows7.sh
cargo test -p aero-boot-tests --test windows7_boot --locked -- --ignored
```

### Useful environment variables

- `AERO_QEMU=/path/to/qemu-system-i386` to override the QEMU binary
- `AERO_ARTIFACT_DIR=...` to control where failing screenshots/diffs are written
- `AERO_REQUIRE_TEST_IMAGES=1` to turn missing fixture files into hard errors (CI uses this)

---

## WASM tests (Rust compiled to WebAssembly)

For crates that use `wasm-bindgen-test`, run tests in a Node environment:

```bash
# From the WASM crate directory (where Cargo.toml for the WASM crate lives)
wasm-pack test --node
```

Notes:

- `cargo xtask test-all` resolves the wasm-pack crate via `scripts/ci/detect-wasm-crate.*` (shared with CI tooling).
  - Override with `AERO_WASM_CRATE_DIR` / `--wasm-crate-dir` if needed.
  - Otherwise it prefers the canonical crate (`crates/aero-wasm`) when present.
  - If it must fall back to `cargo metadata`, it fails if multiple `cdylib` crates exist (set an override).

Common pitfalls:

- **Wrong directory:** `wasm-pack` operates on a *single crate*. Run it from the crate that builds to WASM.
- **Missing target:** ensure the WASM target is installed:
  ```bash
  rustup target add wasm32-unknown-unknown
  ```
- **Threaded/shared-memory builds:** the web app’s threaded WASM build uses `-Z build-std` and requires the **pinned**
  nightly toolchain declared in `scripts/toolchains.json` (`rust.nightlyWasm`) plus `rust-src`. Run `just setup` to
  install the correct nightly toolchain (or see README for the manual commands).
- **Node vs browser environment:** `--node` does **not** provide DOM APIs (`document`, `window`, etc.). Keep `--node` tests focused on pure logic/WASM exports. If a test needs browser APIs, it should use a browser runner (e.g. `wasm-pack test --headless --chrome`) or be covered by Playwright.
- **WASM threads:** if a test requires `SharedArrayBuffer` / WASM threads, Node support may differ from browsers. Prefer testing thread-dependent behavior in a real browser (Playwright) where COOP/COEP can be enforced.

---

## TypeScript unit tests

Install dependencies:

```bash
npm ci
```

Run unit tests:

```bash
npm run test:unit
```

### Test topology

`npm run test:unit` runs two harnesses:

- **Node-only tests (`node:test`)**: Node integration/unit suites (e.g. `web/test/**/*.test.ts`).
- **Unit tests (Vitest)**: colocated unit tests under `web/src/**/*.test.ts`, plus any dedicated Vitest suites under
  `web/test/**/*.vitest.ts` (configured via `web/vite.config.ts`).

Run with coverage (most runners accept `--coverage` via argument passthrough):

```bash
npm run test:unit -- --coverage
```

Typical output locations:

- Terminal summary (pass/fail)
- `coverage/` directory (HTML + LCOV), depending on the runner configuration

---

## Playwright E2E tests

Playwright specs live under `tests/e2e/**/*.spec.ts` (configured by the repo-root `playwright.config.ts`).

Run headless E2E tests:

```bash
npm run test:e2e
```

Notes:

- Playwright starts the **repo-root Vite app** (`npm run dev:harness`, same as `npm run dev`) on `127.0.0.1:5173` and a COI preview server on `127.0.0.1:4173`.
  If you already have another server on those ports (for example the legacy `web/` Vite app: `npm run dev:web` / `npm -w web run dev`), stop it before running Playwright.
- To reuse an already-running harness server while iterating locally:
  `AERO_PLAYWRIGHT_REUSE_SERVER=1 npm run test:e2e`

Run a specific browser project:

```bash
npm run test:e2e -- --project=chromium
npm run test:e2e -- --project=firefox
npm run test:e2e -- --project=webkit
```

Open Playwright UI mode (interactive runner):

```bash
npm run test:e2e -- --ui
```

Update snapshots (for screenshot/visual regression tests):

```bash
npm run test:e2e -- --update-snapshots
```

Debugging tips:

```bash
# Run a single test file
npm run test:e2e -- path/to/test.spec.ts

# Keep the browser open on failure (Playwright convention)
PWDEBUG=1 npm run test:e2e
```

If E2E tests fail early with errors about `SharedArrayBuffer` or `crossOriginIsolated`, see the COOP/COEP section below.

---

## GPU golden-image tests (Playwright)

The repo includes small deterministic graphics microtests that compare rendered output against committed PNG
goldens under `tests/golden/`.

Regenerate/update the CPU-generated goldens:

```bash
npm ci
npm run generate:goldens
```

Or run the same “regenerate + fail on drift” check that CI uses:

```bash
npm ci
npm run check:goldens
```

Run the GPU golden tests (Playwright):

```bash
npm run test:gpu
```

CI enforces that `npm run generate:goldens` does **not** produce changes under `tests/golden/`. If CI fails with a
golden diff, rerun the generator locally and commit the updated PNGs.

---

## COOP/COEP + `crossOriginIsolated` (SharedArrayBuffer / WASM threads)

### Why this matters

Aero relies on **WASM threads** and shared memory for performance (e.g. CPU emulation in Web Workers with `Atomics`). Browsers only expose `SharedArrayBuffer` in a **cross-origin isolated** context, which requires COOP/COEP headers.

See also:

- [Deployment & Hosting (COOP/COEP)](./deployment.md)
- [ADR 0002: Cross-origin isolation](./adr/0002-cross-origin-isolation.md)
- [ADR 0004: WASM build variants (threaded vs single)](./adr/0004-wasm-build-variants.md)

If your page is not cross-origin isolated:

- `window.crossOriginIsolated` will be `false`
- `SharedArrayBuffer` may be `undefined`
- `WebAssembly.Memory({ shared: true, ... })` will fail
- any thread-dependent code will fail or silently fall back to single-thread behavior (depending on implementation)

### Required headers

Your dev server / test server must send:

- `Cross-Origin-Opener-Policy: same-origin`
- `Cross-Origin-Embedder-Policy: require-corp` (or `credentialless` if supported and appropriate)

### How to verify (DevTools)

In the browser console:

```js
crossOriginIsolated
typeof SharedArrayBuffer
```

Expected:

- `crossOriginIsolated === true`
- `typeof SharedArrayBuffer === "function"`

Chrome also shows cross-origin isolation status in **DevTools → Security**.

### Common causes of failure

- **Serving from `file://`**: COOP/COEP isolation requires a proper origin; open the app via a dev server (and usually a secure context). `http://localhost` is treated as secure, but arbitrary `http://` origins are not.
- **Missing headers in your server/proxy**: ensure the *final* server (including any reverse proxy) sets COOP/COEP headers on HTML and subresources as needed.
- **Blocked cross-origin subresources under COEP**: with `Cross-Origin-Embedder-Policy: require-corp`, the browser will block cross-origin scripts/fonts/images that do not explicitly opt in via CORS or `Cross-Origin-Resource-Policy` headers. Symptoms show up as red errors in the console/network panel.
  - Fix by self-hosting assets, adding proper CORS, or using resources that send the correct headers.

---

## WebGPU testing policy

### Why WebGPU tests are gated in CI

WebGPU availability varies across:

- runners (most CI VMs do not have stable GPU access)
- operating systems / driver stacks
- headless browser configurations

To keep CI reliable, tests that **require** WebGPU are typically **skipped** unless explicitly requested. Tests should either:

- run with a non-WebGPU fallback (e.g. WebGL2) in default CI, or
- be conditionally enabled only when WebGPU is available and required

In Playwright, WebGPU-required tests are tagged with `@webgpu` and isolated in a
dedicated Playwright project, `chromium-webgpu` (see `playwright.config.ts`).
Default projects (`chromium`, `firefox`, `webkit`) exclude `@webgpu` tests to
keep PR CI stable on runners where WebGPU is missing or unreliable.

### Forcing WebGPU-required tests

Set `AERO_REQUIRE_WEBGPU=1` to make WebGPU a hard requirement:

```bash
# E2E (Playwright, WebGPU-only project)
AERO_REQUIRE_WEBGPU=1 npm run test:webgpu

# Or run the project directly
AERO_REQUIRE_WEBGPU=1 npx playwright test --project=chromium-webgpu

# Unit tests that exercise WebGPU-dependent paths (if applicable)
AERO_REQUIRE_WEBGPU=1 npm run test:unit
```

Expected behavior when `AERO_REQUIRE_WEBGPU=1` is set:

- tests **fail** (rather than skip/fallback) if `navigator.gpu` is missing or cannot create a device
- CI jobs that do not provide WebGPU will fail, by design

### CI workflows

- `.github/workflows/ci.yml` (PR CI) sets `AERO_REQUIRE_WEBGPU=0` and runs the
  non-WebGPU Playwright projects.
- `.github/workflows/webgpu.yml` (schedule + workflow_dispatch) sets
  `AERO_REQUIRE_WEBGPU=1` and runs the `chromium-webgpu` project; it uploads
  Playwright reports/artifacts so WebGPU regressions are actionable.

---

## CI behavior (what runs where)

CI should be reproducible locally with the same top-level commands:

- Full stack (recommended): `cargo xtask test-all` (or `bash ./scripts/test-all.sh` wrapper)
- Rust: `cargo test --locked --workspace`
- WASM tests (Node): `wasm-pack test --node`
- WASM builds (single + threaded): `npm -w web run wasm:build`
  - Threaded builds require nightly + `rust-src` (see ADR 0004).
- TypeScript unit tests: `npm run test:unit` (often with coverage enabled)
- Browser E2E: `npm run test:e2e`

### Cross-browser Playwright policy (PR vs scheduled)

Playwright E2E coverage is split into two workflows:

- **PR CI (fast):** `.github/workflows/ci.yml` runs Playwright in **Chromium only**
  (`--project=chromium`) to keep pull request feedback fast.
- **Cross-browser E2E (high confidence):** `.github/workflows/e2e-matrix.yml` runs a
  matrix over **Chromium + Firefox + WebKit** on a nightly schedule and via
  `workflow_dispatch`. This workflow is intended to catch browser-specific
  regressions without blocking PR merges.
  - When the scheduled run fails, the workflow opens/updates a single tracking
    issue so failures are visible outside of Actions.

Environment variables commonly affect CI behavior:

- `AERO_REQUIRE_WEBGPU=1`: require WebGPU (see above)

When debugging a CI failure locally, prefer matching the CI environment as closely as possible:

- use `npm ci` (not `npm install`) for deterministic dependency resolution
- run Playwright in headless mode (default) unless you specifically need `--ui`

---

## Win7 virtio portable tests (C, hardware-free)

The Win7 virtio driver stack includes a small **portable C99** module that parses the PCI
capability list for Virtio 1.0 "modern" devices. It is unit-tested using synthetic PCI
config-space images (runs on Linux CI; no hardware required).

From the repo root:

```bash
bash ./drivers/win7/virtio/tests/build_and_run.sh
```

Optionally select a compiler:

```bash
CC=clang bash ./drivers/win7/virtio/tests/build_and_run.sh
```

---

## Rust microbenchmarks (Criterion)

We use [Criterion.rs](https://github.com/bheisler/criterion.rs) to measure a
small set of emulator-critical hot paths with stable statistics.

Current benchmarks:

- `decoder_throughput`: legacy string-op decoder throughput (`aero_cpu_core::interp::decode`, `legacy-interp`)
- `x86_decode_throughput`: instruction decode throughput via the `aero_x86` wrapper (iced-x86)
- `interpreter_hot_loop`: legacy interpreter dispatch loop (`aero_cpu_core::interp::exec`, `legacy-interp`)
- `memory/bulk_copy_1mib`: legacy `RamBus` bulk copy throughput (1 MiB, `legacy-interp`)

Note: the canonical interpreter is `aero_cpu_core::interp::tier0`; the current Criterion bench target is kept behind `--features legacy-interp`.

### Run the emulator-critical microbenchmarks

```bash
# Full (slower, more stable)
cargo bench --locked -p aero-cpu-core --bench emulator_critical --features legacy-interp -- --noplot
```

Criterion writes results to `target/criterion/`.

In CI we move `target/criterion` into `target/bench-*/criterion` so the base/head
runs don't overwrite each other.

### CI / PR profile (fast)

In CI we run a shorter benchmark configuration to keep PR runtime low:

```bash
AERO_BENCH_PROFILE=ci cargo bench --locked -p aero-cpu-core --bench emulator_critical --features legacy-interp -- --noplot
```

### Run the JIT bookkeeping microbenchmarks

`jit_bookkeeping` benchmarks core JIT runtime bookkeeping overhead (CodeCache LRU maintenance,
HotnessProfile under capacity pressure, and PageVersionTracker snapshotting).

```bash
# Default profile (moderate; good for local runs)
cargo bench --locked -p aero-cpu-core --bench jit_bookkeeping -- --noplot

# CI/PR profile (fast)
AERO_BENCH_PROFILE=ci cargo bench --locked -p aero-cpu-core --bench jit_bookkeeping -- --noplot

# Full profile (slower, more stable)
AERO_BENCH_PROFILE=full cargo bench --locked -p aero-cpu-core --bench jit_bookkeeping -- --noplot
```

## Benchmark regression CI

The workflow `.github/workflows/bench.yml` runs these microbenchmarks and fails
on regressions:

- `aero-cpu-core/benches/emulator_critical` (legacy-interp)
- `aero-cpu-core/benches/jit_bookkeeping`

- **pull_request**: benchmarks the PR base commit and the PR head commit (same
  runner), then compares results. The workflow fails if any benchmark slows down
  by more than the configured threshold (see `bench/perf_thresholds.json` → `profiles.pr-smoke.criterion.maxRegressionPct`).
- **schedule / workflow_dispatch**: runs the suite on `main`, compares against
  the previous successful `main` run artifact, and uploads the current results
  as the new baseline artifact (`criterion`).

Regression detection uses Criterion's 95% confidence intervals to avoid flakey
failures on noisy CI runners (it only fails when the slowdown is both above the
threshold and statistically significant).

Artifacts:

- `bench-pr` contains both PR base + head Criterion results plus the comparison report.
- `criterion` is the moving baseline artifact for `main` (only updated on successful runs).
- `criterion-run-<run_id>` is uploaded for every `main` run to aid debugging.

### Manual comparison

You can compare two Criterion output directories locally:

```bash
python3 scripts/bench_compare.py \
  --base path/to/base/criterion \
  --new path/to/new/criterion \
  --thresholds-file bench/perf_thresholds.json \
  --profile pr-smoke
```
