#!/usr/bin/env node

import fs from "node:fs/promises";
import path from "node:path";
import process from "node:process";
import { fileURLToPath } from "node:url";

import { buildCompareResult, exitCodeForStatus, renderCompareMarkdown, statsFromSamples } from "../tools/perf/lib/compare_core.mjs";
import { DEFAULT_PROFILE, loadThresholdPolicy, pickThresholdProfile, getSuiteThresholds } from "../tools/perf/lib/thresholds.mjs";
import { formatOneLineError, truncateUtf8 } from "../src/text.js";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

function parseArgs(argv) {
  const options = {
    baseline: path.join(__dirname, 'baseline.json'),
    current: path.join(__dirname, 'results.json'),
    thresholdsFile: path.join(__dirname, 'perf_thresholds.json'),
    profile: DEFAULT_PROFILE,
    suite: 'node',
    outputMd: path.join(__dirname, 'compare.md'),
    outputJson: null,
    failOnRegression: false,
  };

  function requireValue(flag, value) {
    if (value === undefined) {
      throw new Error(`${flag} requires a value`);
    }
    return value;
  }

  const args = [...argv];
  while (args.length > 0) {
    const arg = args.shift();
    if (arg === '--baseline') {
      options.baseline = requireValue('--baseline', args.shift());
      continue;
    }
    if (arg === '--current') {
      options.current = requireValue('--current', args.shift());
      continue;
    }
    if (arg === '--thresholds') {
      options.thresholdsFile = requireValue('--thresholds', args.shift());
      continue;
    }
    if (arg === '--thresholds-file') {
      options.thresholdsFile = requireValue('--thresholds-file', args.shift());
      continue;
    }
    if (arg === '--profile') {
      options.profile = requireValue('--profile', args.shift());
      continue;
    }
    if (arg === '--suite') {
      options.suite = requireValue('--suite', args.shift());
      continue;
    }
    if (arg === '--output-md') {
      options.outputMd = requireValue('--output-md', args.shift());
      continue;
    }
    if (arg === '--output-json') {
      options.outputJson = requireValue('--output-json', args.shift());
      continue;
    }
    if (arg === '--json') {
      options.outputJson = path.join(__dirname, 'compare.json');
      continue;
    }
    if (arg === '--fail-on-regression') {
      options.failOnRegression = true;
      continue;
    }
    if (arg === '--help' || arg === '-h') {
      return { options, help: true };
    }
    throw new Error(`Unknown argument: ${arg}`);
  }

  return { options, help: false };
}

function printHelp() {
  process.stdout.write(`bench/compare

Compares benchmark results to a baseline using the shared perf threshold policy.

Usage:
  node bench/compare [options]

Options:
  --baseline <path>      Baseline JSON (default: bench/baseline.json)
  --current <path>       Current results JSON (default: bench/results.json)
  --thresholds-file <path>   Threshold policy file (default: bench/perf_thresholds.json)
  --thresholds <path>        Alias for --thresholds-file (legacy)
  --profile <name>           Threshold profile (default: ${DEFAULT_PROFILE})
  --suite <name>             Threshold suite (default: node)
  --output-md <path>     Markdown output path (default: bench/compare.md)
  --output-json <path>   JSON output path (optional)
  --json                 Write JSON output to bench/compare.json
  --fail-on-regression   Exit non-zero if regressions/instability are detected (exit codes: 1=regression, 2=unstable)
`);
}

async function readJsonFile(filePath) {
  return JSON.parse(await fs.readFile(filePath, 'utf8'));
}

function coerceScalarString(value) {
  if (value == null) return "";
  switch (typeof value) {
    case "string":
      return value;
    case "number":
    case "boolean":
    case "bigint":
      return String(value);
    default:
      return "";
  }
}

function findMetric(result, metricName) {
  const scenarios = result?.scenarios && typeof result.scenarios === 'object' ? result.scenarios : {};
  const matches = [];
  for (const [scenarioName, scenario] of Object.entries(scenarios)) {
    const metrics = scenario?.metrics && typeof scenario.metrics === 'object' ? scenario.metrics : {};
    const metric = metrics[metricName];
    if (metric && typeof metric === 'object') {
      matches.push({ scenarioName, metric });
    }
  }
  if (matches.length === 0) return null;
  if (matches.length === 1) return matches[0];
  throw new Error(
    `Metric "${metricName}" is present in multiple scenarios: ${matches.map((m) => m.scenarioName).join(', ')}`,
  );
}

async function main() {
  const { options, help } = parseArgs(process.argv.slice(2));
  if (help) {
    printHelp();
    return;
  }

  const baseline = await readJsonFile(options.baseline);
  const current = await readJsonFile(options.current);

  const thresholdsPolicy = await loadThresholdPolicy(options.thresholdsFile);
  const { name: profileName, profile } = pickThresholdProfile(thresholdsPolicy, options.profile);
  const suiteThresholds = getSuiteThresholds(profile, options.suite);

  const cases = [];
  for (const [metricName, threshold] of Object.entries(suiteThresholds.metrics ?? {})) {
    const better = threshold?.better;
    if (better !== 'lower' && better !== 'higher') {
      throw new Error(`thresholds: ${options.suite}.metrics.${metricName}.better must be "lower" or "higher"`);
    }

    const b = findMetric(baseline, metricName);
    const c = findMetric(current, metricName);
    const scenario = c?.scenarioName ?? b?.scenarioName ?? options.suite;
    const unit = coerceScalarString(b?.metric?.unit ?? c?.metric?.unit).slice(0, 64);

    const baselineStats = b?.metric?.samples ? statsFromSamples(b.metric.samples) : null;
    const currentStats = c?.metric?.samples ? statsFromSamples(c.metric.samples) : null;

    cases.push({
      scenario,
      metric: metricName,
      unit,
      better,
      threshold,
      baseline: baselineStats,
      candidate: currentStats,
    });
  }

  const result = buildCompareResult({
    suite: options.suite,
    profile: profileName,
    thresholdsFile: options.thresholdsFile,
    baselineMeta: baseline.meta ?? null,
    candidateMeta: current.meta ?? null,
    cases,
  });

  const markdown = renderCompareMarkdown(result, { title: 'Benchmark regression report' });
  await fs.mkdir(path.dirname(options.outputMd), { recursive: true });
  await fs.writeFile(options.outputMd, markdown, 'utf8');

  if (options.outputJson) {
    await fs.mkdir(path.dirname(options.outputJson), { recursive: true });
    await fs.writeFile(options.outputJson, JSON.stringify(result, null, 2) + '\n', 'utf8');
  }

  process.stdout.write(`Wrote ${options.outputMd}\n`);
  if (options.outputJson) process.stdout.write(`Wrote ${options.outputJson}\n`);

  if (options.failOnRegression) {
    process.exitCode = exitCodeForStatus(result.status);
  }
}

main().catch((err) => {
  let stack = null;
  if (err && typeof err === "object") {
    try {
      const raw = err.stack;
      if (typeof raw === "string" && raw) stack = raw;
    } catch {
      // ignore getters throwing
    }
  }
  console.error(stack ? truncateUtf8(stack, 8 * 1024) : formatOneLineError(err, 512));
  process.exitCode = 1;
});
