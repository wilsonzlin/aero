//! Paging / MMU helpers.

pub mod fault;
pub mod long;
pub mod mode32;
pub mod pae;

use crate::bus::MemoryBus;

pub use fault::PageFault;

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum AccessType {
    Read,
    Write,
    Execute,
}

impl AccessType {
    pub(crate) fn is_write(self) -> bool {
        matches!(self, Self::Write)
    }

    pub(crate) fn is_execute(self) -> bool {
        matches!(self, Self::Execute)
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PageSize {
    Size4K,
    Size2M,
    Size1G,
    Size4M,
}

impl PageSize {
    pub fn bytes(self) -> u64 {
        match self {
            PageSize::Size4K => 4 * 1024,
            PageSize::Size2M => 2 * 1024 * 1024,
            PageSize::Size1G => 1024 * 1024 * 1024,
            PageSize::Size4M => 4 * 1024 * 1024,
        }
    }
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct TranslateCtx {
    pub cr0: u64,
    pub cr3: u64,
    pub cr4: u64,
    pub efer: u64,
    pub cpl: u8,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
pub struct PhysAddr(pub u64);

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TranslateError {
    /// x86-64 #GP generated by non-canonical virtual addresses in 4-level paging.
    GeneralProtection { vaddr: u64 },
    /// x86 #PF with CR2 and the encoded page-fault error code bits.
    PageFault(PageFault),
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub struct TranslateResult {
    pub paddr: u64,
    pub page_size: PageSize,
}

/// Page fault error code bit: protection violation (P).
pub const PFEC_P: u32 = 1 << 0;
/// Page fault error code bit: caused by a write access (W/R).
pub const PFEC_WR: u32 = 1 << 1;
/// Page fault error code bit: access came from CPL=3 (U/S).
pub const PFEC_US: u32 = 1 << 2;
/// Page fault error code bit: reserved-bit violation (RSVD).
pub const PFEC_RSVD: u32 = 1 << 3;
/// Page fault error code bit: instruction fetch (I/D).
pub const PFEC_ID: u32 = 1 << 4;

pub const CR0_PG: u64 = 1 << 31;
pub const CR0_WP: u64 = 1 << 16;
pub const CR4_PSE: u64 = 1 << 4;
pub const CR4_PAE: u64 = 1 << 5;
pub const CR4_PGE: u64 = 1 << 7;

pub const EFER_LME: u64 = 1 << 8;
pub const EFER_LMA: u64 = 1 << 10;
pub const EFER_NXE: u64 = 1 << 11;

/// Minimal MMU state tracked by the CPU core.
///
/// This stores the architectural paging control registers and provides a single
/// entrypoint (`translate`) that selects the correct page-walk algorithm.
#[derive(Debug, Clone, Copy, Default)]
pub struct Mmu {
    pub cr0: u64,
    pub cr3: u64,
    pub cr4: u64,
    pub efer: u64,
    pub cpl: u8,
}

impl Mmu {
    pub fn new() -> Self {
        Self::default()
    }

    pub fn set_cr0(&mut self, value: u64) {
        self.cr0 = value;
    }

    pub fn set_cr3(&mut self, value: u64) {
        self.cr3 = value;
    }

    pub fn set_cr4(&mut self, value: u64) {
        self.cr4 = value;
    }

    pub fn set_efer(&mut self, value: u64) {
        self.efer = value;
    }

    pub fn set_cpl(&mut self, value: u8) {
        self.cpl = value;
    }

    pub fn translate(
        &mut self,
        mem: &mut dyn MemoryBus,
        vaddr: u64,
        access: AccessType,
        cpl: u8,
    ) -> Result<u64, TranslateError> {
        self.cpl = cpl;
        translate_impl(
            mem,
            vaddr,
            access,
            &TranslateCtx {
                cr0: self.cr0,
                cr3: self.cr3,
                cr4: self.cr4,
                efer: self.efer,
                cpl,
            },
        )
    }
}

/// Translate a linear address to a physical address.
///
/// Supported translation modes:
/// - Paging disabled (`CR0.PG=0`): identity mapping (address masked to 32 bits)
/// - 32-bit non-PAE paging (`CR0.PG=1`, `CR4.PAE=0`)
/// - IA-32 PAE paging (3-level) (`CR0.PG=1`, `CR4.PAE=1`, `EFER.LME=0`)
/// - 4-level IA-32e paging (long mode) (`CR0.PG=1`, `CR4.PAE=1`, `EFER.LME=1`)
#[allow(clippy::too_many_arguments)]
pub fn translate(
    bus: &mut dyn MemoryBus,
    linear: u64,
    access: AccessType,
    cpl: u8,
    cr0: u64,
    cr3: u64,
    cr4: u64,
    efer: u64,
) -> Result<u64, TranslateError> {
    translate_impl(
        bus,
        linear,
        access,
        &TranslateCtx {
            cr0,
            cr3,
            cr4,
            efer,
            cpl,
        },
    )
}

fn translate_impl(
    bus: &mut dyn MemoryBus,
    linear: u64,
    access: AccessType,
    ctx: &TranslateCtx,
) -> Result<u64, TranslateError> {
    if (ctx.cr0 & CR0_PG) == 0 {
        return Ok(linear & 0xFFFF_FFFF);
    }

    if (ctx.cr4 & CR4_PAE) == 0 {
        return mode32::translate(bus, linear, access, ctx.cpl, ctx.cr0, ctx.cr3, ctx.cr4);
    }

    if (ctx.efer & EFER_LME) == 0 {
        return pae::translate(bus, linear, access, ctx.cpl, ctx.cr0, ctx.cr3, ctx.efer);
    }

    // IA-32e: the CPU should have set LMA when paging was enabled with LME=1.
    // We dispatch based on LME because some callers may not model LMA yet.
    let _ = ctx.efer & EFER_LMA;
    long::translate_4level(bus, linear, access, ctx.cr3, ctx.cr0, ctx.efer, ctx.cpl)
        .map(|r| r.paddr)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[derive(Default)]
    struct RecordingBus {
        reads: Vec<(u64, usize)>,
    }

    impl MemoryBus for RecordingBus {
        fn read_physical(&mut self, paddr: u64, buf: &mut [u8]) {
            self.reads.push((paddr, buf.len()));
            buf.fill(0);
        }

        fn write_physical(&mut self, _paddr: u64, _buf: &[u8]) {}
    }

    fn assert_first_read(bus: &RecordingBus, expected_paddr: u64, expected_len: usize) {
        assert!(
            !bus.reads.is_empty(),
            "expected at least one memory read, got none"
        );
        assert_eq!(bus.reads[0], (expected_paddr, expected_len));
    }

    #[test]
    fn selects_paging_disabled_when_pg_clear() {
        let mut mmu = Mmu::new();
        mmu.set_cr0(0);
        mmu.set_cr3(0xDEAD_BEEF);
        mmu.set_cr4(CR4_PAE);
        mmu.set_efer(EFER_LME);

        let mut bus = RecordingBus::default();
        let paddr = mmu.translate(&mut bus, 0x1_0000_1234, AccessType::Read, 0).unwrap();
        assert_eq!(paddr, 0x0000_1234);
        assert!(bus.reads.is_empty());
    }

    #[test]
    fn dispatches_to_legacy_32_walker_when_pae_clear() {
        let mut mmu = Mmu::new();
        mmu.set_cr0(CR0_PG);
        mmu.set_cr3(0x0010_0000);
        mmu.set_cr4(0);
        mmu.set_efer(0);

        let vaddr = 0x0040_1000u64;
        let vaddr32 = (vaddr & 0xFFFF_FFFF) as u32;
        let pde_index = ((vaddr32 >> 22) & 0x3FF) as u64;
        let expected_pde_addr = (mmu.cr3 & 0xFFFF_F000) + pde_index * 4;

        let mut bus = RecordingBus::default();
        let _ = mmu.translate(&mut bus, vaddr, AccessType::Read, 0).unwrap_err();
        assert_first_read(&bus, expected_pde_addr, 4);
    }

    #[test]
    fn dispatches_to_pae_walker_when_pae_set_and_lme_clear() {
        let mut mmu = Mmu::new();
        mmu.set_cr0(CR0_PG);
        mmu.set_cr3(0x0020_0000);
        mmu.set_cr4(CR4_PAE);
        mmu.set_efer(0);

        let vaddr = 0xC000_0000u64;
        let vaddr32 = (vaddr & 0xFFFF_FFFF) as u32;
        let pdpt_index = ((vaddr32 >> 30) & 0x3) as u64;
        let expected_pdpte_addr = (mmu.cr3 & 0xFFFF_FFE0) + pdpt_index * 8;

        let mut bus = RecordingBus::default();
        let _ = mmu.translate(&mut bus, vaddr, AccessType::Read, 0).unwrap_err();
        assert_first_read(&bus, expected_pdpte_addr, 8);
    }

    #[test]
    fn dispatches_to_long_mode_walker_when_lme_set() {
        let mut mmu = Mmu::new();
        mmu.set_cr0(CR0_PG);
        mmu.set_cr3(0x0030_0000);
        mmu.set_cr4(CR4_PAE);
        mmu.set_efer(EFER_LME);

        let vaddr = 1u64 << 39; // pml4_index = 1
        let pml4_index = (vaddr >> 39) & 0x1FF;
        let expected_pml4e_addr = (mmu.cr3 & 0x000F_FFFF_FFFF_F000) + pml4_index * 8;

        let mut bus = RecordingBus::default();
        let _ = mmu.translate(&mut bus, vaddr, AccessType::Read, 0).unwrap_err();
        assert_first_read(&bus, expected_pml4e_addr, 8);
    }
}
